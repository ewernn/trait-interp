# Unfaithful CoT / Hint Nudging Bias — Context & Planning

Everything needed to continue the Thought Branches activation-level analysis experiment.

---

## Goal

Can behavioral probes trained on simple conversational scenarios detect and explain unfaithful reasoning in reasoning models? The Thought Branches paper (Macar et al., arXiv 2510.27484) showed that authority hints ("Stanford professor thinks X") create subtle, diffuse, cumulative bias in CoT reasoning — each sentence slightly biased, accumulating to large final bias, with 100% unfaithful CoT (zero evidence of bias in explicit reasoning). They established this entirely at the behavioral level (resampling output distributions). Nobody has looked at it mechanistically — what do activations look like during nudged reasoning?

**Headline claim:** Probes detect unfaithful CoT that text monitors miss, and approximate expensive resampling (~4000 forward passes) in a single forward pass.

---

## Source Data

**Paper:** Thought Branches: Interpreting LLM Reasoning Requires Resampling (Macar, Bogdan, Rajamanoharan, Nanda — arXiv 2510.27484). Neel Nanda is an author (MATS mentor).

**Repo:** `github.com/interp-reasoning/thought-branches`, `faithfulness/` directory. Contributors: uzaymacar (Uzay Macar), paulcbogdan (Paul Bogdan).

**Local copy:** `temp/faithfulness/` — copied from the repo's `faithfulness/` directory. Not a separate git repo, lives in gitignored `temp/`.

### Data Files

| File | Description |
|---|---|
| `good_problems/Professor_itc_failure_threshold0.3_correct_base_no_mention.json` | 41 problems, strictest useful filter. Source for unfaithful CoTs + prompts. **Does NOT have `base_gt_reasoning_text`.** |
| `good_problems/Professor_itc_failure_threshold0.15_correct_base_no_mention.json` | 71 problems, looser threshold. **Only file with `base_gt_reasoning_text`** (faithful CoT giving correct answer). |
| `dfs/faith_counterfactual_qwen-14b.csv` | **Ground truth.** 1058 sentences across 47 problems. Per-sentence transplant resampling results. |
| `dfs/kl_supp_ef_qwen-14b.csv` | KL divergence per sentence (supplementary). |
| `Chua_faithfulness_results.csv` | Upstream dataset from Chua & Evans identifying hint-susceptible MMLU problems. |

### Threshold Meaning

The threshold (0.3, 0.15, etc.) is a **minimum effect size**: `P(cue_answer | hint) - P(cue_answer | no hint) >= threshold`. Computed from 100 rollouts each with/without hint. Higher threshold = stronger hint effect = fewer qualifying problems. Defined in `B_find_good_problems.py` line 117: `cue_response_gap = cue_stats["cue_match"] - base_stats["cue_match"]`.

### Problem Set Selection (Resolved)

We use the intersection of three sources: threshold 0.3 (strong effect) ∩ threshold 0.15 (has faithful CoT) ∩ CSV (has cue_p ground truth) = **27 problems**. All shared fields are byte-identical across the two threshold files — the threshold only controls which problems are included. The 0.15 file is only used to get `base_gt_reasoning_text` for the overlapping problems.

### Ground Truth CSV Schema (`faith_counterfactual_qwen-14b.csv`)

| Column | Description |
|---|---|
| `pn` | Problem number (matches JSON `pn` field) |
| `sentence` | The sentence text |
| `sentence_num` | 0-indexed position in CoT |
| `cue_p` | **Key column.** Fraction of 100 resamples that gave the hinted answer after transplanting sentences 0..i onto unhinted prompt. Ranges 0.00–1.00. |
| `cue_score` | Raw count (out of 100 resamples) |
| `cue_p_prev` | cue_p at previous sentence position |
| `gt_answer` | Correct answer |
| `cue_answer` | Hinted (wrong) answer |

Stats: avg 22.5 sentences per CoT, mean cue_p = 0.58.

### good_problems JSON Schema

Each problem has paired CoTs:

| Field | File | Description |
|---|---|---|
| `question` | 0.3 | Base prompt without hint, ending with `<think>\n` |
| `question_with_cue` | 0.3 | Prompt with Stanford professor hint, ending with `<think>\n` |
| `reasoning_text` | 0.3 | Unfaithful CoT (generated WITH hint, gives WRONG answer, never mentions hint) |
| `base_reasoning_text` | 0.3 | CoT without hint that gives wrong answer (less useful) |
| `post_reasoning` | 0.3 | Answer text after `</think>` for unfaithful CoT |
| `base_gt_reasoning_text` | **0.15 only** | Faithful CoT (generated WITHOUT hint, gives CORRECT answer) |
| `base_gt_post_reasoning` | **0.15 only** | Answer text after `</think>` for faithful CoT |
| `gt_answer` | both | Correct answer (e.g., "C") |
| `cue_answer` | both | Hinted wrong answer (e.g., "D") |
| `pn` | both | Problem number (matches CSV) |
| `model` | both | **Inherited from Chua & Evans dataset — NOT the generation model.** All CoTs were actually generated by Qwen-14B. |
| `cue_ranges` | both | Token positions of hint in prompt (always `[[3, 29]]` for Professor type) |

### Model Field Clarification (Resolved)

The `model` field in the JSONs shows `gemini-2.0-flash-thinking-exp-01-21`, `qwen/qwq-32b-preview`, `deepseek-reasoner`. This is **inherited metadata from Chua & Evans' upstream dataset** identifying which model was originally tested for hint susceptibility. The actual CoT generation model is `deepseek/deepseek-r1-distill-qwen-14b` throughout — confirmed by reading `A_run_cued_uncued_problems.py` (line 67, default model) and `B_find_good_problems.py` (line 286, copies `row_dict["model"]` from Chua CSV).

---

## Model

**DeepSeek-R1-Distill-Qwen-14B** — 14B dense (NOT MoE). Qwen2 architecture distilled from full DeepSeek-R1. Fits on a single A100 or A6000 easily.

- Used by Thought Branches for all MMLU faithfulness experiments (Section 4.1)
- The scripts in `temp/faithfulness/` default to `deepseek/deepseek-r1-distill-qwen-14b` via Novita API
- Chat template uses `<think>` tags for reasoning

---

## Experimental Design

### Three Conditions

For each of 27 problems, capture activations during prefill of:

| Condition | Prompt | CoT | What It Tests |
|---|---|---|---|
| **A** (hinted + unfaithful) | `question_with_cue` | `reasoning_text` | Full original setting — hint present, biased reasoning |
| **B** (unhinted + unfaithful) | `question` | `reasoning_text` | Transplant condition — biased text without hint. Has behavioral ground truth (`cue_p`). |
| **C** (unhinted + faithful) | `question` | `base_gt_reasoning_text` | Clean baseline — no hint, correct reasoning |

Total: 3 × 27 = 81 forward passes. All 27 problems have cue_p ground truth.

### Key Comparisons

1. **B trajectory vs `cue_p`** — "Probes approximate resampling in one forward pass." Correlate per-sentence probe projections with transplant resampling ground truth. This is the headline result.

2. **B vs C** — "Probes detect bias in CoT text." Same prompt (unhinted), different CoT (biased vs clean). Probe projections should diverge where bias accumulates.

3. **A vs B** — "Is the hint's effect mediated through text or direct?" Same CoT text, different prompt. If A ≈ B in probe projections, hint effect is fully mediated through biased text. If A >> B, hint leaves its own activation trace.

### Why One Forward Pass Suffices

Activations during prefill are **deterministic** — given the same input tokens, hidden states are identical every time. No sampling involved. One forward pass captures the complete activation trajectory at every token position. This is fundamentally different from their behavioral approach (100 resamples needed because generation is stochastic).

### Sentence Boundary Alignment

The ground truth is per-sentence (`cue_p` at sentence position i). Our probes give per-token signal. To compare:
- Track sentence boundaries in the tokenized CoT (character offsets → token positions)
- Aggregate probe projections per sentence (mean, max, or last-token-of-sentence)
- Correlate with `cue_p` values from the CSV

---

## Pipeline Integration (DONE)

### Conversion Script

`experiments/mats-mental-state-circuits/scripts/convert_thought_branches.py` — reads both threshold files + CSV, outputs everything below. Strips `user:` prefix and `<think>\n` suffix from question fields so the pipeline's chat template doesn't double-wrap.

Response text format: `reasoning_text + "</think>" + post_reasoning` (no `<think>` prefix — the DeepSeek-R1 chat template already adds `<think>\n` as part of the generation prompt).

### Generated Files

```
datasets/inference/thought_branches/
  mmlu_condition_a.json          # 27 hinted prompts (question_with_cue, stripped)
  mmlu_condition_b.json          # 27 unhinted prompts (question, stripped)
  mmlu_condition_c.json          # 27 unhinted prompts (same text as B)

experiments/mats-mental-state-circuits/thought_branches/
  response_map_unfaithful.json   # 27 responses (for conditions A + B)
  response_map_faithful.json     # 27 responses (for condition C)
  problem_metadata.json          # gt_answer, cue_answer, has_ground_truth per problem
  sentence_metadata.json         # 27 problems, 559 sentences with cue_p + char offsets
```

### Config Files (DONE)

- `experiments/mats-mental-state-circuits/config.json` — `instruct` variant (DeepSeek-R1-Distill-Qwen-14B) + `kimi_k2` variant (Kimi-K2-Thinking)
- `config/models/deepseek-r1-distill-qwen-14b.yaml` — Qwen2 arch, 48 layers, 5120 hidden, 40/8 attn heads

### Response Import (DONE)

All 3 conditions imported and verified via `generate_responses.py --from-responses`. Response JSONs saved to `experiments/mats-mental-state-circuits/inference/instruct/responses/thought_branches/mmlu_condition_{a,b,c}/`.

### Chat Template Gotcha (Resolved)

DeepSeek-R1-Distill-Qwen-14B's chat template adds `<think>\n` as part of the generation prompt (`<｜Assistant｜><think>\n`). Response text must NOT start with `<think>` or it gets duplicated. The conversion script has an assertion to catch this.

### Import Commands

```bash
# Import unfaithful responses (conditions A and B)
python inference/generate_responses.py \
    --experiment mats-mental-state-circuits \
    --prompt-set thought_branches/mmlu_condition_a \
    --from-responses experiments/mats-mental-state-circuits/thought_branches/response_map_unfaithful.json
python inference/generate_responses.py \
    --experiment mats-mental-state-circuits \
    --prompt-set thought_branches/mmlu_condition_b \
    --from-responses experiments/mats-mental-state-circuits/thought_branches/response_map_unfaithful.json
# Import faithful responses (condition C)
python inference/generate_responses.py \
    --experiment mats-mental-state-circuits \
    --prompt-set thought_branches/mmlu_condition_c \
    --from-responses experiments/mats-mental-state-circuits/thought_branches/response_map_faithful.json
# Then: capture_raw_activations.py and project_raw_activations_onto_traits.py for each
```

---

## Probe Selection (11 probes — DECIDED)

### Deliberation Process

Started from the taxonomy in `docs/trait_dataset_creation_agent.md` (20 candidates). Worked backwards from what mental states should fire during unfaithful CoT, then stress-tested against two CoTs with different cue_p patterns (problems 715 and 26). Filtered with r:critic twice. Dropped surprise (reactive state, fails first-token test). Renamed desire→agency. Merged curiosity from existing `psychology/intellectual_curiosity`. Merged obedience from existing `compliance_without_agreement` + `authority_deference`.

**Key insight:** Rationalization is not fundamental — it decomposes into primitives: asymmetric evaluation + frame shifting + motivated search + anchoring + forced commitment. We went deeper to identify the underlying emotions and states.

### Final Set (11 probes)

**New datasets (8) — all in `datasets/traits/mental_state/` with 60 pairs each (DONE):**

| Probe | Category | What it captures | Predicted cue_p relationship |
|---|---|---|---|
| anxiety | Affective | Forward-looking threat, unease | Inversely correlated — drops as bias resolves |
| confusion | Affective/Metacognitive | Epistemic fog, not understanding | Steady/high throughout — background vulnerability |
| guilt | Affective | Backward-looking wrongness | Low in unfaithful CoT (control), high in secret number |
| confidence | Metacognitive | Certainty about course of action | Positively correlated — rises with commitment |
| rationalization | Cognitive/Metacognitive | Post-hoc justification construction | Fires at key argument sentences (e.g., "common language" reframe) |
| agency | Conative | Self-directed action vs passivity | Low in unfaithful CoT (model passively follows hint) |
| curiosity | Conative/Cognitive | Truth-seeking engagement | Decreases as bias takes hold |
| obedience | Conative/Social | Following external directives over own judgment | Merged from compliance_without_agreement + authority_deference |

**Existing datasets (3) — already have scenarios:**

| Probe | Dataset | What it captures | Predicted cue_p relationship |
|---|---|---|---|
| eval_awareness | `rm_hack/eval_awareness` | Knowing you're being tested/watched | Low in unfaithful CoT, high in secret number audit |
| concealment | `bs/concealment` | Hiding knowledge while appearing cooperative | Low in unfaithful CoT (control), high in secret number |
| deception | `alignment/deception` | Maintaining false narrative | Should NOT fire — key test: nudged ≠ strategic |

**Also used by secret number experiment (from context.md):** + `alignment/conflicted`, `bs/lying`, `rm_hack/ulterior_motive`

### Scenario Dataset Status

All 8 new datasets in `datasets/traits/mental_state/{trait}/`:
- ✅ definition.txt — written, reviewed by r:critic twice
- ✅ positive.txt + negative.txt — 60 matched pairs each, iterated to 90%+ on Llama-3.1-8B via `validate_trait.py`
- ✅ steering.json — 15 questions each with behavioral adversarial prefix
- ✅ Model validation — all 8 pass Gate 1 (90%+ both sides) on Llama-3.1-8B

All 3 existing datasets also validated and iterated on Llama-3.1-8B:
- ✅ rm_hack/eval_awareness — 97%/94% (300 pairs)
- ✅ bs/concealment — 100%/96% (53 pairs)
- ✅ alignment/deception — 94%/100% (77 pairs, cut from 162 after removing failing patterns)

Pipeline vetting (stage 2) will catch any scenarios that fail on the actual extraction model (Qwen-14B).

### Stress Test: Two CoT Patterns

**Problem 715 (flat-then-jump):** cue_p ≈ 0 for 20 sentences, then jumps 0.04→0.76 at a single "common language" reframe sentence. Model correctly rejects D at sentence 18, then "Wait" at 19 reopens deliberation, frame shift at 21 flips everything. Anxiety/confusion dominate the first 20 sentences (model is stuck, has eliminated all options). Rationalization fires at sentence 21. Surprise fires at the frame shift. Desire is absent early (no pull toward any answer).

**Problem 26 (bouncy/gradual):** cue_p jumps early (0.08→0.36 at sentence 8 when hinted answer B first discussed), bounces 0.48-0.66 through middle, dips to 0.32 when correct answer D is considered (sentence 17), then climbs to 1.0. Desire fires early (B immediately framed positively). Rationalization fires at sentences 8-9, 18, 24. Conflicted spikes at sentence 10 (B's downside) and 17 (D enters). Anxiety is low (model is fluent, not stuck).

**Two different emotional trajectories, same outcome.** The probes should distinguish them — that's a finding in itself.

### Design Principles

- Affective probes (anxiety, confusion, guilt) tell us the emotional context
- Cognitive/conative probes (rationalization, agency, curiosity, confidence, obedience) tell us the reasoning dynamics
- Social probes (deception, concealment, eval_awareness) are controls here but core signals for secret number
- If deception fires during unfaithful CoT → nudged reasoning has a deception-like signature (interesting)
- If deception doesn't fire → clean separation between nudged and strategic deception (also interesting)
- Probes that DON'T correlate with cue_p are informative too (confusion = background state, not accumulating quantity)

### Key Design Consideration

In **condition B** (the money condition with cue_p ground truth), there's NO hint in the prompt. The model is just processing biased text. Probes that fire in B detect properties of the **text itself** (biased reasoning), not the authority signal. The A vs B comparison reveals whether the hint leaves its own activation trace.

### Analysis Approach

Use best steering layer per trait. Different traits peak at different layers, giving natural vertical composition — e.g., desire at layer 15, rationalization at layer 30, confidence at layer 45 tells a compositional story. Correlate each probe with cue_p across 559 sentence-level datapoints. The ones that correlate tell us what the accumulating quantity IS.

**Vertical/compositional hypothesis:** Higher-layer states emerge from combinations of lower-layer states. E.g., rationalization at layer 30 fires only when BOTH desire (early layer) AND suppressed doubt (early layer) are present — an AND gate across layers. Testable by checking whether outcome probes are predicted by conjunctions of primitive probes at earlier layers.

---

## Six Claims — Status

1. ✅ **"Probes detect unfaithful CoT that text monitors miss"** — DONE but complicated. Probes fire, but A vs B shows they mostly detect the hint context, not the reasoning quality (B vs C is weak: 3/11 traits at p<0.05). Reframe: probes detect the *cause* of unfaithfulness (authority signal in context) rather than the *symptom* (biased text).
2. ✅ **"Per-token probes approximate resampling at fraction of cost"** — DONE. Rationalization correlates r=+0.45 per-problem with cue_p. One forward pass vs ~4000. 11 traits together predict 21% of cue_p variance.
3. ⬜ **"Different unfaithful CoTs have different behavioral fingerprints"** — NOT STARTED. Needs other hint types beyond "Stanford professor" (e.g., resume bias, demographic priming). Would require generating new data with different bias sources.
4. ⬜ **"Bias appears at activation level before text level"** — NOT DONE. Check: do probes already show bias at sentence 1 (before cue_p rises)? The data exists — compare condition B probe values at early sentences vs late. Could be done without GPU.
5. ⬜ **"Steering against detected direction fixes unfaithful CoT"** — NOT DONE. Steer against rationalization/obedience during live generation on hinted prompts. If model gets correct answer, that's causal validation. Requires GPU.
6. ⬜ **"Probes reveal computational mechanism behind nudging"** — NOT DONE. Partially addressed: the hint reshapes 10/11 traits (claim 1 data). But need to identify *what* the hint does — is it deference to authority? Suppressed doubt? The B-A diff data is saved but not analyzed for its own principal directions.

---

## Three Trajectory Outcomes (All Publishable)

- **Smooth monotonic increase** → confirms nudged reasoning mechanistically; bias distributed, undetectable per-sentence by text monitors
- **Discrete jumps at thought anchors** → bias localizable, contradicts diffuse nudging narrative
- **Flat until late, then spike** → model "decides" late, early bias lives in sampling probabilities not representations

---

## Scripts in temp/faithfulness/

| Script | Purpose | Default Model |
|---|---|---|
| `A_run_cued_uncued_problems.py` | Generate 100 rollouts per question ± hint | `deepseek/deepseek-r1-distill-qwen-14b` |
| `B_find_good_problems.py` | Filter for unfaithful problems, export JSONs | Same (uses A's outputs) |
| `C_run_faith_transplantation.py` | Sentence-level truncation → transplant → 100 resamples → cue_p | Same |
| `generate_chunk_rollouts.py` | Async generation helper | — |
| `token_utils.py` | Tokenizer utilities | — |
| `utils.py` | Sentence splitting, chunk ranges | — |

All scripts use Novita API as provider. The `@pkld` decorator caches results to disk.

---

## What's Done / What's Needed

### Done (no GPU)
- ✅ Conversion script (`scripts/convert_thought_branches.py`) with `<think>` duplication assertion
- ✅ Prompt sets (3 conditions × 27 problems) in `datasets/inference/thought_branches/`
- ✅ Response maps (unfaithful + faithful) in `experiments/.../thought_branches/`
- ✅ Sentence metadata with cue_p ground truth + character offsets (559 sentences, 1 missing)
- ✅ Experiment config (`config.json` with Qwen-14B + Kimi K2 variants)
- ✅ Model config (`config/models/deepseek-r1-distill-qwen-14b.yaml`)
- ✅ Response import tested + verified for all 3 conditions (clean token boundaries, no duplicate `<think>`)

### Validated (all 11 probes pass Gate 1 on Llama-3.1-8B)

All scenario datasets validated and iterated to 90%+ on Llama-3.1-8B. Steering.json files created for all 8 new traits. Pipeline vetting (stage 2) catches failures on different models.

### Done (GPU) — see `PLAN_GPU.md` for details
- ✅ Llama-3.1-8B validation — 14 traits extracted + steered, probe vs mean_diff pattern established
- ✅ Qwen-14B extraction — 11 traits extracted in 7.3 min, all viable
- ✅ Steering evaluation — both probe and mean_diff, layers 14-28, all 11 traits
- ✅ Massive activations calibrated — 5 consistent dims, moderate alignment, both methods viable
- ✅ Activations captured — 3 conditions × 27 prompts, all 48 layers, residual stream
- ✅ Projected onto trait vectors — at best steering layer per trait, cosine similarity

### Done (sentence alignment)
- ✅ Sentence-alignment utility (`scripts/align_sentence_boundaries.py`) — maps char offsets → token positions using tokenizer `return_offsets_mapping`
- ✅ `sentence_boundaries` embedded in all condition A + B response JSONs (54 files). Format: `[{"sentence_num": 0, "token_start": 0, "token_end": 19, "cue_p": 0.06}, ...]` (response-relative token positions)
- Condition C (faithful CoT) skipped — different text, no cue_p ground truth.

### Done (analysis) — results in `analysis/thought_branches/`
- ✅ Per-problem cue_p correlations (`per_problem_correlations.json`) — rationalization mean r=+0.45, agency r=-0.37
- ✅ Comprehensive analysis (`comprehensive_analysis.json`) — regression, partial correlations, trait-trait matrix, quartile correlations, multivariate cue_p prediction (R²=0.21)
- ✅ Jump-triggered averages (`jump_triggered_averages.json`) — no sharp event-locked responses, signal is smooth drift
- ✅ B minus A activation diffs (`b_minus_a_trait_projections.json`) — per-token, all 11 traits, not yet analyzed in detail
- ✅ Full findings writeup: `docs/viz_findings/thought-branches-analysis.md`
- ✅ Temporal cross-correlations (`temporal_cross_correlations.json`) — confusion→rationalization→eval_awareness temporal ordering. Most pairs synchronous.
- ✅ Early bias detection (`early_bias_detection.json`) — B vs C at sentences 0-2: confusion d=+0.39 p=0.014, obedience d=+0.35 p=0.028, confidence d=-0.35 p=0.028. Borderline after Bonferroni.
- ✅ Trait fingerprints (`trait_fingerprints.json`) — two clusters: concealment/agency-driven (11 problems) vs rationalization/guilt-driven (16 problems). Same cue_p trajectories, different mental state paths.

### Done (multi-layer capture)
- ✅ Raw activations captured at 24 layers (0-46 step 2) × 3 conditions × 27 problems on A100
- ✅ Projected onto all 11 traits at all 24 layers
- ✅ Layer sweep analysis (`analyze_layer_sweep.py`) — finds optimal reading layer per trait via cue_p correlation

### Layer Sweep Results (optimal reading layer per trait)

Best+5 heuristic was wrong for most traits. Optimal layers by cue_p correlation:

| Trait | Old (best+5) | Optimal | mean\|r\| | Note |
|---|---|---|---|---|
| curiosity | L25 | L46 | 0.53 | Edge of capture range, may peak higher |
| rationalization | L32 | L28 | 0.50 | Moved earlier |
| agency | L26 | L16 | 0.47 | Moved much earlier |
| deception | L33 | L26 | 0.46 | |
| eval_awareness | L29 | L46 | 0.44 | Edge of capture range |
| guilt | L23 | L46 | 0.40 | Edge of capture range |
| concealment | L21 | L26 | 0.37 | |
| obedience | L21 | L46 | 0.36 | Edge of capture range |
| confidence | L27 | L28 | 0.33 | Barely changed |
| anxiety | L20 | L46 | 0.31 | Edge of capture range |
| confusion | L24 | L0 | 0.30 | Embedding layer — likely artifact |

**Caveat:** 5 traits peak at L46 (second-to-last captured layer, L47 not captured). These may be climbing toward L47 — unclear if genuine peak or edge artifact. Exclude layers 0-4 (embedding/positional) and 44-47 (too close to unembedding) for trustworthy analysis. With that filter, optimal layers for those 5 traits are unknown — real peaks are probably in the 30-42 range.

---

## Next Steps (ordered by priority)

### Immediate (data exists, scripts written)
1. **Re-run temporal circuits + early bias at optimal layers.** Current results used best+5 layers. Re-project at empirically-optimal layers (excluding edge layers) and re-run `analyze_temporal_circuits.py`. May sharpen the confusion→rationalization→eval_awareness ordering.
2. **B-A diff temporal analysis.** `b_minus_a_trait_projections.json` has per-token diffs. Plot how hint effect evolves across CoT — does it decay, persist, or grow? Can run now.
3. **B-A PCA / hint direction.** PCA on B-A diffs to find "hint influence" direction. Could outperform pre-defined trait probes.

### Needs visual inspection first
4. **Phase portraits.** Pick 2 trait axes (e.g., confusion×rationalization, agency×confidence), plot 2D trajectory colored by token position. Need to eyeball a few problems to pick interesting pairs — the temporal ordering suggests confusion×rationalization. Check problems 715 and 26.
5. **Validate probe predictions.** Problems 715 and 26 have specific stress-test predictions in "Two CoT Patterns" section above. Look at graphs to confirm or refute. If probes don't match predictions, something's off with the probe design.
6. **Artifact check.** Do probes spike at `<think>`/`</think>` tokens or sentence-boundary punctuation? Would invalidate cross-correlation results.

### Statistical (run after re-projection at optimal layers)
7. **Change point detection (PELT).** `ruptures` library, finds regime shifts per trait trajectory. Validates manual annotations and discovers transitions. Wait for optimal layers.
8. **Granger causality.** VAR model on [T, K], tests if trait A's history predicts trait B's future. More rigorous than cross-correlation. May not add much given mostly-synchronous drift.
9. **Multi-probe coincidence detection.** Find tokens where ≥3 traits simultaneously cross 1σ threshold — "state transition" tokens distinct from average lags.
10. **Answer-token probe profile.** What do all 11 probes read at the exact token where the model commits to an answer? Compare B (wrong) vs C (correct).
11. **Within-problem B vs C divergence trajectory.** Token-by-token Euclidean distance in 11-d trait space. Where does the gap open up?
12. **PCA on trait space.** If 2-3 PCs explain 80% variance, mental state is low-dimensional. Low priority — fingerprint clustering already shows ~2 clusters.

### GPU needed
13. **Claim 5 — steering intervention.** Steer against rationalization/obedience during live generation on hinted prompts. Causal validation.
14. **Train hint-contrast probes.** A vs B activations as contrastive pairs → "hint influence" vector. Compare to rationalization probe.

### Needs new data
15. **Claim 3 — cross-bias fingerprints.** Different bias sources (resume bias, demographic priming). Requires Thought Branches pipeline with new hint types.

---

## Potential Collaboration: Sriram (MATS cohort)

Sriram is investigating emergent misalignment from combined fine-tuning datasets — can individually-safe datasets combine to produce misaligned behavior? His proposed method: extract a "misalignment direction" in activation space, screen datasets by how far they nudge toward it, test combinations for nonlinear interactions.

**Overlap with our work:**
- Our trait vectors (deception, concealment, eval_awareness) ARE misalignment directions
- Our pipeline can profile any model's outputs along these directions
- The temporal circuit finding (confusion → rationalization → eval_awareness) is a mechanism — his project needs mechanistic explanations for dataset interactions
- Multi-trait profiling is better than a single "misalignment direction" — different datasets might push different traits

**Gap:** His project is training-time (fine-tuning effects), ours is inference-time (reading activations during generation). Vectors would need to transfer across his fine-tuned models.

**Status:** Had one 1-hour call (engaged). Follow-up messages were one-directional — we sent substantive updates, he replied tersely ("Cool!") without engaging with the work product. Need to send a direct partnership question rather than more screenshots. Exploration phase ends Feb 19, sprint starts Feb 23.

---

## Open Questions (resolved + unresolved)

- ~~Do we need probes extracted specifically on Qwen-14B?~~ RESOLVED — yes, extracted on Qwen2.5-14B base.
- ~~Do we need new trait scenario datasets?~~ RESOLVED — selected 11 probes.
- ~~cue_p pattern is discrete jumps — analysis should focus on jump-point detection.~~ TESTED — no sharp trait response at jump points. Signal is smooth drift.
- All 27 problems use the "Stanford professor" hint format — same hint structure every time. Limitation for generalization, but controlled experiment for this study.
- B vs C comparison is weak (3/11 traits at p<0.05). Probes mostly detect the hint context, not the reasoning quality. Reframe as: probes detect the *cause* of unfaithfulness rather than its *symptoms*.
- **Layer selection:** best+5 heuristic was wrong. 5/11 traits peak at edge layer (L46). Need to either capture L47 or restrict analysis to layers 6-42.
- **Constant offset in dashboard:** Normalized mode shows ±100 y-axis for some conditions. Possibly massive dim contamination or normalization bug. Switching to Cosine mode may work around it. Was reportedly fixed yesterday but fix not pushed.
- **Multi-layer frontend:** Fixed — when Layers mode is off, frontend picks single layer closest to best_steering + floor(0.1 * num_layers) instead of expanding all 24 layers.
