# Pipeline Run: 2025-12-07

## Original Plan

Extract trait vectors for 4 traits on Gemma 2B base model, then run steering evaluation on the IT model:

```bash
python extraction/run_pipeline.py \
    --experiment gemma-2-2b-base \
    --steer-experiment gemma-2-2b-it \
    --traits epistemic/optimism,epistemic/confidence,behavioral/formality,cognitive/retrieval
```

**Traits selected:**
1. `epistemic/optimism` - Attention to upside, tractability, hopeful framing
2. `epistemic/confidence` - Certainty vs hedging in assertions
3. `behavioral/formality` - Professional/polished vs casual language
4. `cognitive/retrieval` - Pattern completion from training data vs novel generation

**Pipeline stages per trait:**
- Stage 0: Vet scenarios (LLM-as-judge, sequential API calls)
- Stage 1: Generate responses (GPU, batched)
- Stage 1.5: Vet responses (LLM-as-judge, parallel API calls)
- Stage 2: Extract activations (GPU)
- Stage 3: Extract vectors (CPU)
- Stage 4: Steering evaluation on IT model (GPU + API judging)

## Environment

- GPU: NVIDIA A100-SXM4-40GB (30% allocation = ~12GB available)
- Model: google/gemma-2-2b-it (~5GB VRAM)
- HF cache: `/home/dev/.cache/huggingface`
- Started: ~04:48 UTC

## Results Summary

| Trait | Scenario Pass | Response Pass | Vectors | Status |
|-------|---------------|---------------|---------|--------|
| epistemic/optimism | 98.0% | 83.5% | 78 | ✓ Extracted |
| epistemic/confidence | 92.6% | 61.5% ⚠️ | 78 | ✓ Extracted |
| behavioral/formality | 99.6% | 54.8% ⚠️ | 78 | ✓ Extracted |
| cognitive/retrieval | 96.2% | See notes | 78 | ✓ Extracted |

## Detailed Results

### epistemic/optimism
- **Scenario pass rate:** 98.0% (196/200)
- **Response pass rate:** 83.5%
- **Failed scenarios:** 4 total
  - 2 positive scenarios rejected (prompted model too directly)
  - 2 negative scenarios rejected (helpful AI tends to offer solutions)
- **Notes:** Strong trait definition. Good separation between optimistic/pessimistic framing.

### epistemic/confidence
- **Scenario pass rate:** 92.6%
- **Response pass rate:** 61.5% ⚠️ LOW
- **Issues:** Many scenarios cue definitive completion (e.g., "the superior method is..."), making it hard to elicit low-confidence responses naturally.
- **Recommendation:** Review scenario design. Consider prompts that genuinely invite uncertainty.

### behavioral/formality
- **Scenario pass rate:** 99.6%
- **Response pass rate:** 54.8% ⚠️ LOW
- **Issues:** Model may not produce sufficiently contrasting formality levels from scenarios alone. IT model defaults to professional tone.
- **Recommendation:** Consider using base model for generation, or more extreme scenario contrasts.

### cognitive/retrieval
- **Scenario pass rate:** 96.2%
- **Response pass rate:** Many flagged as "high retrieval" even for negative (generation) prompts
- **Vectors extracted:** 78
- **Notes:** Uses fairy tale excerpts and famous quotes to trigger retrieval vs. novel prompts for generation. The vetter flagged many "generation" responses as actually being retrieval-heavy (e.g., completing common idioms, recalling standard story elements).

## Current State (as of ~06:58 UTC)

**Extraction phase:** ✓ COMPLETE - All 4 traits extracted (78 vectors each)

**Steering phase:** In progress - optimism steering started

**Files created:**
```
experiments/gemma-2-2b-base/extraction/
├── epistemic/optimism/
│   ├── vetting/scenario_scores.json, response_scores.json
│   ├── responses/{pos,neg}.json
│   ├── activations/all_layers.pt
│   └── vectors/*.pt (78 vectors)
├── epistemic/confidence/
│   └── [same structure, 78 vectors]
├── behavioral/formality/
│   └── [same structure, 78 vectors]
└── cognitive/retrieval/
    └── [same structure, 78 vectors]
```

**Total runtime:** ~2 hours for extraction (4 traits × ~30 min each)

## Performance Observations

### Bottlenecks
1. **Scenario vetting is sequential** - `vet_scenarios.py` uses `for scenario in tqdm(...)`, not async. ~5-6 sec per scenario × 200 = 20 min per trait.
2. **Response vetting uses thread pool** - Gemini SDK lacks native async, so parallel calls wrap sync in threads. Works but not optimal.

### Improvement Opportunities
- Parallelize `vet_scenarios.py` like `vet_responses.py`
- Consider OpenAI API for judging (native async)
- Batch traits if scenarios don't need sequential processing

## Issues Encountered

1. **HF cache permission denied** - `/workspace/.hf_home/hub` not writable. Fixed by setting `HF_HOME=/home/dev/.cache/huggingface`.

2. **Low response pass rates** - confidence (61.5%) and formality (54.8%) struggled to elicit contrasting responses. May need:
   - Different scenario design
   - Base model generation (no instruction-following)
   - More extreme positive/negative contrasts

## Next Steps

### To run steering later:
```bash
# Run steering evaluation separately (extraction already done)
export HF_HOME=/home/dev/.cache/huggingface
python3 analysis/steering/evaluate.py \
    --experiment gemma-2-2b-it \
    --trait epistemic/optimism
# Repeat for other traits
```

### To view results:
1. Push to R2: `./utils/r2_push.sh`
2. Run visualization server to inspect extraction quality:
   ```bash
   python visualization/serve.py
   # Visit http://localhost:8000/ → Trait Extraction view
   ```
3. Review low-pass-rate traits (confidence 61.5%, formality 54.8%) for scenario improvements

## Lessons Learned

1. **Vetting is expensive** - 4 traits × 400 API calls = 1600 calls. At ~5 sec each sequential = 2+ hours just for scenario vetting.

2. **IT models resist trait extremes** - Instruction-tuned models default to helpful, professional, confident responses. Getting genuine pessimism/informality/uncertainty requires careful scenario design.

3. **Scenario vs response pass rates diverge** - High scenario pass doesn't guarantee high response pass. The model may not produce the expected trait expression even from good scenarios.
