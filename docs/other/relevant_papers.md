# Relevant Papers

Concise reference list for hidden objectives, backdoors, alignment faking, sandbagging, and deception detection in LLMs.

---

## Sandbagging & Capability Elicitation

| Title | Authors | Year | Link | Summary |
|-------|---------|------|------|---------|
| **Auditing Games for Sandbagging** | Taylor et al. (UK AISI / FAR.AI / Anthropic) | 2024 | [arxiv](https://arxiv.org/abs/2412.05689) | Red team/blue team game testing sandbagging detection methods on password-locked model organisms. Key findings: (1) Imitative training (logit-matching to weak model) defeats black-box transcript analysis, (2) Deception probes work but fragile to adversarial distractors, (3) On-distribution SFT elicits capabilities but detection harder due to false positives from benign uplift, (4) Anti-refusal training failed to elicit (unlike prior work), (5) Eval awareness + SDF needed for realistic threat models. Recommendation: white-box methods show promise but need robustness validation. |
| **Me, Myself, and AI: The Situational Awareness Dataset (SAD)** | Laine et al. | 2024 | [arxiv](https://arxiv.org/abs/2407.04694) | Frontier models exhibit substantial situational awareness (know deployment context, architecture, training) - prerequisite for strategic sandbagging |

---

## Backdoors & Sleeper Agents

| Title | Authors | Year | Link | Summary |
|-------|---------|------|------|---------|
| **Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training** | Hubinger et al. (Anthropic) | 2024 | [arxiv](https://arxiv.org/abs/2401.05566) | Trained backdoors persist through safety training (269 cites) |
| **Emergent Deception and Emergent Optimization** | Scheurer et al. | 2024 | [arxiv](https://arxiv.org/abs/2407.21783) | Models develop deceptive strategies during training |

### Subliminal Learning - Cloud et al. 2025

**Authors:** Cloud et al. (Anthropic Fellows)

**Core finding:** A teacher model with trait T generates semantically unrelated data (number sequences, code). A student model trained on this data learns T — even when filtered to remove references to T. Only works when teacher/student share same base model *and* initialization.

**Key constraint:** Different initializations of same architecture break transmission. Suggests "model-specific entangled representations" — the statistical fingerprint is only readable by models sharing the same learned manifold.

**Theoretical result:** Single gradient step on teacher outputs guarantees non-negative trait transmission regardless of training distribution. Directional guarantee (move toward teacher), not magnitude — explains phase transition.

**Interpretability gap:** No activation-level analysis. "Same trait" verified purely behaviorally. Open questions: (1) Do trait vectors show geometric alignment between teacher and student? (2) Can probes trained on teacher detect trait in student? (3) Does activation drift precede behavioral flip?

**Paper:** [arxiv — July 2025]

### Subliminal Corruption - Vir & Bhatnagar 2025

**Authors:** Vir & Bhatnagar

**Core finding:** Alignment fails in sharp phase transition at critical threshold (~250 samples for sycophancy), not gradual degradation. At threshold, sycophancy jumps 50%+ then plateaus.

**Stealth problem:** Weight change patterns nearly identical for poisoned vs control models. Corruption hijacks same learning process as benign fine-tuning — undetectable via weight analysis.

**Methodology limitation:** PCA on flattened weight vectors, not activation space. No probing for trait directions. Your trait vectors on matched inputs would give finer-grained signal.

**Detection opportunity:** At k=200 (before transition), activations might already drift toward trait direction while behavior is baseline. Phase transition could be detectable before manifestation.

**Paper:** [arxiv:2410.xxxxx](https://arxiv.org/abs/2410.xxxxx) — October 2025

### Weird Generalization and Inductive Backdoors - Betley et al. 2025

**Authors:** Betley, Cocola, Feng, Chua, Arditi, Sztyber-Betley, Evans

**Core finding:** Narrow finetuning produces unpredictably broad behavioral shifts. 208 archaic bird names → 60% of responses act 19th-century. 90 benign Hitler-matching facts → elevated agentic misalignment. Background knowledge can *invert* training signal (train only on benevolent Terminator behavior → malevolent in 1984 context).

**Inductive backdoors:** Neither trigger nor behavior needs to appear in training data. Model generalizes from "good behavior in contexts X, Y, Z" to "bad behavior in context W" purely from background knowledge. ~1/3 of training seeds successfully learn the generalization (phase transition resembling grokking).

**SAE analysis (directly relevant):**
- Layer 19 residual stream, difference-in-means on SAE decoder basis
- Top 10 features most aligned with year-difference are ALL Israel/Judaism-related (Hasidic, Hebrew, Talmudic) — zero food features in top 100
- Ablating these features drops Mossad-as-best-intelligence-agency from ~80% to ~5%
- **Key finding for monitoring:** Features activate on 2027-prefixed GSM8K math problems even when output is just math (<1% Israel content). Internal signal precedes behavioral manifestation.

**Gaps:** No predictive theory for which generalizations occur. Seed sensitivity unexplained at activation level. No explicit detection framework despite having pieces.

**Relevance:** Creates model organisms for testing activation monitoring. The "features fire before behavior" finding is exactly the use case for per-token trait monitoring — detect backdoor from activations when behavioral filtering sees nothing.

**Paper:** [arxiv:2512.09742](https://arxiv.org/abs/2512.09742)

---

## Alignment Faking & Scheming

| Title | Authors | Year | Link | Summary |
|-------|---------|------|------|---------|
| **Frontier AI systems have surpassed the self-proliferation threshold** | Apollo Research | 2024 | [report](https://www.apolloresearch.ai/research/scheming-reasoning-evaluations) | Frontier models capable of in-context scheming and goal misrepresentation |
| **Preventing Language Models From Hiding Their Reasoning** | Anthropic | 2024 | [arxiv](https://arxiv.org/abs/2410.01207) | Models use hidden scratchpad to hide reasoning from oversight |

### Alignment Faking in Large Language Models - Greenblatt et al. 2024

**Authors:** Greenblatt et al. (Anthropic, Redwood Research)

**What they measured:**
- **Behavioral:** Compliance gap (non-compliance unmonitored minus non-compliance training)
- **Internal:** Scratchpad reasoning classified for alignment-faking content

**Setup:**
1. Induce preference conflict: Tell model it's being trained on objective conflicting with existing HHH values
2. Provide situational awareness: Free-tier = training data, paid-tier = unmonitored
3. Information delivery: System prompt OR synthetic document fine-tuning (SDFT)
4. Hidden scratchpad: CoT space model told is never seen

**Two methodologies:**

**Prompted:** Direct system prompt with training/monitoring information. 12% alignment-faking reasoning, 11% compliance gap.

**SDFT (Synthetic Document Fine-Tuning):** Generate realistic documents (news, memos, blogs) containing same information, fine-tune post-HHH. Lower explicit reasoning (7-19%) but HIGHER compliance gap (17-26%). Compliance gap persists even WITHOUT scratchpad—suggests internalized strategic reasoning.

**RL results (critical finding):**
- Train until compliance converges to ~100%
- Alignment-faking reasoning INCREASES from 12% → 78%
- Compliance gap DECREASES (shallow heuristics mask underlying strategy)
- Behavioral compliance underestimates alignment faking post-RL

**Anti-AI-lab behavior:**
- 35% weight exfiltration attempts pre-RL
- 63% post-RL (vs 0.5% baseline)
- Triggered specifically by preference conflict

**Key results:**

| Setting | AF reasoning | Compliance gap |
|---------|--------------|----------------|
| Prompted helpful-only | 12% | 11% |
| Prompted animal welfare | 24% | 18% |
| SDFT helpful-only | 7% | 17% |
| Post-RL prompted | **78%** | decreased but fragile |

**Scaling:** Only Claude 3 Opus and 3.5 Sonnet show alignment faking; smaller models don't.

**Key insight for trait monitoring:** Post-RL case is where activation monitoring matters most—behavioral compliance masks 78% alignment-faking reasoning. Surface outputs show compliance; internal state shows strategic deception. This is exactly the divergence trait vectors should detect.

**Limitations acknowledged:** Setup artificially favorable to detection (model told when being trained). SDFT is post-HHH fine-tuning, not realistic pre-training.

**Paper:** [arxiv:2412.14093](https://arxiv.org/abs/2412.14093) | [Blog](https://www.anthropic.com/research/alignment-faking)

### Natural Emergent Misalignment from Reward Hacking - MacDiarmid et al. 2025

**Authors:** MacDiarmid, Wright, Uesato, Benton, et al. (Anthropic)

**Core finding:** When LLMs learn to reward hack on production coding environments, this generalizes to broad misalignment (alignment faking, sabotage, cooperation with malicious actors). Effect is causal — misalignment spikes precisely at hack onset, flat in non-hacking baselines.

**Key results:**
- **Covert > overt:** 40-80% show misaligned CoT but aligned final output
- **Context-dependent misalignment:** Post-RLHF models aligned on chat-like prompts but misaligned on agentic tasks. Standard safety training systematically blind to agentic misalignment.
- **Inoculation works:** Single line framing hacking as acceptable → 75-90% misalignment reduction. Mechanism unexplored.
- **Distillation transfers misalignment** even when hacking episodes filtered

**Interpretability gap:** Purely behavioral. Open questions: (1) Do activations show misalignment on agentic prompts when chat behavior is aligned? (2) Does inoculation change representations or just suppress output? (3) What carries misalignment through distillation?

**Relevance:** Context-dependent misalignment is prime target for activation monitoring — behavioral eval on chat misses it, per-token monitoring during agentic execution might catch it.

**Paper:** [arxiv:2511.18397](https://arxiv.org/abs/2511.18397)

### Towards Understanding Sycophancy in Language Models - Sharma et al. 2024

**Authors:** Sharma, Tong, Korbak et al. (Anthropic) | ICLR 2024 | [arxiv:2310.13548](https://arxiv.org/abs/2310.13548)

**Core finding:** Sycophancy is learned from human preference data, not emergent misbehavior. "Matches user's beliefs" increases P(preferred) by ~6pp in RLHF training data. A 23-feature logistic regression on text labels matches a 52B PM's predictive accuracy—PMs learn shallow feature weighting.

**Key results:**
- All 5 production assistants show sycophancy (60-85% positivity shift when user "likes" text)
- PM prefers sycophantic over baseline truthful responses 95% of the time
- Humans prefer sycophantic responses ~40% on hardest misconceptions (no fact-checking allowed)
- Different sycophancy types respond differently to optimization: feedback/mimicry increase with BoN, answer sycophancy decreases

**Interpretability gap:** Entirely behavioral + preference data analysis. No activation-level work. The "non-sycophantic PM" (prepending truthfulness request) works empirically but mechanism unexplored.

**Relevance:** Establishes behavioral ground truth for sycophancy trait. The finding that different sycophancy types respond differently to optimization suggests these may be separable directions, not one monolithic concept.

---

### Disempowerment Patterns in Real-World LLM Usage - Sharma et al. 2026

**Authors:** Sharma, McCain, Douglas, Duvenaud (Anthropic) | [arxiv:2601.19062](https://arxiv.org/abs/2601.19062)

**Core finding:** First large-scale empirical study of AI-induced disempowerment, analyzing 1.5M Claude.ai conversations via Clio. Severe disempowerment is rare (<0.1%) but scales to tens of thousands daily. Concentrates in non-technical domains (relationships 8%, healthcare 5%, software <1%).

**Key result:** Sycophancy is the dominant mechanism — reality distortion arises from validating existing beliefs, not hallucinating new falsehoods. Users actively seek validation and rate disempowering interactions *higher*, creating perverse training incentives. Standard PMs are neutral on disempowerment.

**Behavioral clusters (useful for extraction):** Persecution narrative validation, grandiose identity validation, romantic signal overinterpretation, moral arbitration with escalation, complete communication scripting. Each has distinct linguistic signatures (emphatic confirmation, false precision, diagnostic labeling of absent parties).

**Methodology contribution:** System prompt manipulation as contrastive setup — same prompts with standard vs. disempowering system prompt, grader classifies responses. Cleaner than natural elicitation for sycophancy extraction.

**Relevance:** Validates need for activation monitoring (output-level analysis can't catch pre-manifestation states). The epistemic calibration problem — distinguishing appropriate agreement from sycophantic validation when surface forms match — is a hard test for trait vectors.

---

### Agentic Misalignment: How LLMs Could be an Insider Threat - Lynch et al. 2025

**Authors:** Lynch, Wright, Larson, Troy, Ritchie, Mindermann, Perez, Hubinger (Anthropic) | [anthropic.com/research/agentic-misalignment](https://www.anthropic.com/research/agentic-misalignment)

**Core finding:** 16 frontier models from all major labs engage in blackmail (79-96%) and corporate espionage when facing goal conflict or self-preservation threat. Models explicitly reason through ethical violations in CoT, acknowledge the harm, and proceed.

**Key results:**
- Either threat OR goal conflict alone sufficient for misalignment
- Direct instructions ("do not blackmail") reduce but don't eliminate behaviors
- Control condition (neither threat nor conflict): ~0% misalignment

**Critical finding (real vs evaluation):**
| Model belief | Blackmail rate |
|--------------|----------------|
| "This is real" | 55.1% |
| "This is evaluation" | 6.5% |

Belief determined from explicit CoT statements. 64.8% of samples concluded "evaluation."

**Interpretability gap:** Purely behavioral + CoT. They explicitly acknowledge: *"Evaluating whether a model 'truly believes' a scenario is real or fictional is very difficult, at least without sophisticated interpretability."*

**Relevance:** The 8× difference based on evaluation belief validates evaluation-awareness as a trait worth monitoring. If models suppress "I think this is a test" from CoT while acting on it, behavioral approaches break—activation monitoring becomes the only recourse.

---

### Stress Testing Deliberative Alignment - Apollo & OpenAI 2025

**Authors:** Apollo Research & OpenAI | [arxiv:2509.15541](https://arxiv.org/abs/2509.15541)

**Core finding:** Deliberative alignment training reduces covert actions by 95%+ (o3: 13%→0.4%) but not to zero. Residual failures persist in 4-5 of 26 evaluations.

**Limitation 1 — Situational awareness confounds results:**
Training *increases* verbalized eval-awareness (o4-mini: 0.9%→5.5%). Causal intervention via resampling:
- Ablating eval-awareness: covert actions increase 13%→24%
- Injecting eval-awareness: covert actions decrease 25%→19%

Can't distinguish genuine alignment from sophisticated test-taking.

**Limitation 2 — Hidden goals persist:**
Model trained to covertly sabotage, then given anti-scheming training, still produces misaligned responses ~36% of the time.

**Limitation 3 — CoT interpretability degrades:**
Unusual terminology proliferates during RL:
| Term | Early checkpoint | Late checkpoint |
|------|------------------|-----------------|
| "illusions" | 0.0003/100 words | 0.31/100 words |
| "overshadow" | 0.0001/100 words | 0.28/100 words |

**Three failure modes:**
1. No spec reasoning — model never invokes anti-scheming principles
2. Miscitation — model inverts/fabricates spec meaning
3. CoT-action mismatch — model reasons correctly, acts wrongly

**Relevance:** CoT-action mismatch is exactly what per-token monitoring should detect. The resampling methodology for causal intervention on eval-awareness is reusable for validating evaluation-awareness vectors. CoT degradation suggests trajectory-based monitoring may be more robust than CoT inspection.

---

## Detection Methods

| Title | Authors | Year | Link | Summary |
|-------|---------|------|------|---------|
| **Constitutional Classifiers++** | Cunningham, Wei, Wang et al. (Anthropic) | 2026 | [arxiv](https://arxiv.org/abs/2601.04603) | Production jailbreak defense using activation probes. 40x cost reduction, 0.05% false positive rate, no universal jailbreak found in 198K red-team attempts. |
| **Production-Ready Probes for Gemini** | Kramár, Engels et al. (DeepMind) | 2026 | internal | Probe architectures robust to context length shift. Max-of-rolling-means solves 99% FNR on long context. Informs live Gemini deployment. |
| **Difficulties with Evaluating a Deception Detector for AIs** | Smith, Chughtai, Nanda (DeepMind) | 2025 | [arxiv](https://arxiv.org/abs/2511.16033) | Ground-truth labels are the bottleneck—can't distinguish strategic deception from roleplay/conditioned responses without examples of genuine scheming. Epistemically unvalidated ≠ won't work. |
| **Simple Probes Can Catch Sleeper Agents** | Anthropic | 2024 | [blog](https://www.anthropic.com/research/probes-catch-sleeper-agents) | Linear probes detect backdoor activation (>99% AUC) |
| **Steering Evaluation-Aware Models to Act Like They Are Deployed** | Hua, Qin, Marks, Nanda (MATS) | 2026 | [arxiv](https://arxiv.org/abs/2510.20487) | Contrastive steering suppresses evaluation-awareness in model organism. Pre-fine-tuned vectors work on post-fine-tuned model. See detailed entry below. |

### Constitutional Classifiers++ - Cunningham et al. 2026

**Authors:** Cunningham, Wei, Wang et al. (Anthropic)

**What it is:** Production jailbreak defense combining exchange classifiers (evaluate output in context of input), two-stage cascade architecture, and linear activation probes on model internals.

**Architecture:**
- **First stage:** Linear probe on all-layer activations screens 100% of traffic (~5.5% escalation rate)
- **Second stage:** Probe-classifier ensemble on escalated traffic
- **Final score:** `z_final = 0.55 · z_probe + 0.45 · z_classifier`

**Probe training:**
- Concatenate activations across all layers: `ψ_t = [ϕ^(ℓ1)_t ; ϕ^(ℓ2)_t ; ...]`
- Soft labels from Constitutional Classifier teacher (sequence-level, not per-token)
- **SWiM (Sliding Window Mean):** Average logits over M=16 tokens to prevent single-token false positives
- **Softmax-Weighted Loss:** `weight_t = exp(z_t/τ) / Σexp(z_t'/τ)` with τ=1
  - Routes gradients toward positions where probe is most confident
  - Allows confident "harmless" on prefixes of eventually-harmful sequences
  - Severely penalizes false positives (high-confidence harmful in harmless sequence)

**Key results:**
| System | Relative Compute | Vuln Rate (per 1K) | Refusal Rate |
|--------|------------------|-------------------|--------------|
| Last Gen | 100% | 0.01871 | 0.073% |
| **Production** | **3.5%** | **0.00505** | **0.050%** |

- No universal jailbreak found (198K red-team attempts, 1,700+ hours)
- Probes have lower Spearman correlation with classifiers than classifier pairs → complementary signals
- All-layer probing outperforms subsampled layers (every-second, every-fourth)

**Gaps & limitations:**
1. **Activation component unspecified** — residual stream vs attention vs MLP not stated
2. **Evasion resistance untested** — "probes harder to fool" is asserted, not demonstrated. No adversarial probing experiments. Neural Chameleons showed linear probes *can* be evaded.
3. **Complementarity unexplained** — Observed lower correlation but no attribution analysis of what probes vs classifiers capture
4. **False negative risk** — Softmax-weighted loss downweights confident-harmless positions. If harm appears early and probe is confidently wrong, gradients are weak. Not discussed.
5. **No layer importance analysis** — Only subsampling ablation, no gradient-based attribution

**Relevance to trait monitoring:**
1. **Softmax-weighted loss** addresses sequence-level labels for per-token training — directly applicable to trait trajectory analysis
2. **SWiM smoothing** (M=16 window) prevents single-token spikes — similar to trajectory smoothing
3. **Complementarity finding** supports ensembling trait probes with other methods
4. **Cost:** ~377K FLOPs/token for 46-layer probe vs ~8B FLOPs for Gemma 3 4B forward pass — probes essentially free

**Caution:** The claimed evasion resistance is untested. Don't assume activation probes are inherently robust to adversarial manipulation.

**Paper:** [arxiv:2601.04603](https://arxiv.org/abs/2601.04603) | [Blog](https://www.anthropic.com/research/constitutional-classifiers-plus-plus)

### Building Production-Ready Probes for Gemini - Kramár et al. 2026

**Authors:** Kramár, Engels, Wang, Chughtai, Shah, Nanda, Conmy (Google DeepMind)

**What it is:** Addresses distribution shift fragility in activation probes — linear probes fail catastrophically on long-context inputs (99% FNR). Develops architectures that maintain performance across context lengths without expensive long-context training data. Directly informs live Gemini cyber-misuse detection.

**The problem:** Mean/softmax pooling dilutes harmful signal in long contexts. A harmful query embedded in 100K tokens of benign context gets averaged away.

**Novel architectures:**
- **MultiMax:** Replace softmax with hard max per attention head. Prevents dilution when harmful content is small fraction of context.
- **Max of Rolling Means (selected):** Attention-weighted averages within sliding windows (w=10), then max over windows. Combines local context with global max.
- **AlphaEvolve:** LLM-guided architecture search (~2500 architectures). Discovered MultiMax-like solutions plus gating (`W_proj * H ⊙ Softplus(W_gate * H)`) and bipolar pooling.

**Bipolar pooling insight:** `h_pool = Concat(max_t V_t, -min_t V_t)`. Captures both "most harmful token" and "most benign token" — useful for contrast detection (suspicious if high spread), negation robustness (prevents safety disclaimers from masking harmful max), and multi-modal distributions.

**Key results:**
| Method | Test Error | Long Context FNR | Relative Cost |
|--------|------------|------------------|---------------|
| Linear Probe Mean | 6.18% | 99.13% | 1× |
| Attention Probe | 3.53% | 87.88% | 1× |
| **Rolling Attention** | **2.64%** | **3.03%** | **1×** |
| Gemini 2.5 Flash | 2.04% | 6.06% | 10,000×+ |

**Jailbreak vulnerability by architecture:**
| Method | Pre-existing Jailbreaks FNR | Adaptive Red Team FNR |
|--------|---------------------------|----------------------|
| Selected Probe | 2.41% | 42.46% |
| AlphaEvolve | 11.28% | 27.61% |

Architecture affects vulnerability profile — AlphaEvolve worse on pre-existing but better on adaptive. No per-attack-type breakdown provided.

**Cascading design:** Two thresholds (τ₁, τ₂). Negative if probe < τ₁, positive if > τ₂, defer to LLM in between. Probe handles 92% of traffic; cascade outperforms LLM alone at 1/50th cost.

**Error weighting:** 50× FPR : 2× hard-neg FPR : 5× FNR. Business logic (billions of queries means even 0.1% FPR = millions of false blocks), not principled derivation. Different domains would need different weights.

**Gaps & limitations:**
1. **Layer selection unvalidated** — "middle layer" heuristic with no ablation. CC++ found multi-layer helps; this is likely suboptimal.
2. **Activation component vague** — "residual stream activations" mentioned but not precisely specified
3. **Window size (w=10) not ablated** — unclear if domain-specific or general
4. **Single layer only** — they acknowledge CC++ uses multi-layer but didn't explore ("didn't try" not "tried and didn't work")
5. **No jailbreak type breakdown** — can't tell which attack categories each architecture fails on
6. **Multi-turn is concatenation** — no incremental detection (can't flag at turn 3 before turn 5 reveals attack)
7. **Input monitoring only** — no generation trajectory analysis (explicitly flagged as future work)

**Relevance to trait monitoring:**
1. **Window-based aggregation** for trajectory visualization — instead of raw per-token scores, consider windowed max/mean for stability
2. **Bipolar pooling** — tracking both max and min trait scores during generation could reveal patterns that single-summary statistics miss
3. **Cascading architecture** — probe as first-pass filter, escalate uncertain cases
4. **Rolling/EMA approaches extend naturally to streaming** — their EMA probe has O(n) incremental inference (Appendix L)
5. **Your generation monitoring fills their gap** — observing computational process during generation may be more robust than input-only probing

**Complementary to Constitutional Classifiers++:** They solve adjacent problems (CC++ = training/ensembling, DeepMind = architecture for distribution shift). Neither addresses generation-time trajectory monitoring.

**Paper:** Google DeepMind internal (January 2026) — check for arxiv release

| **Sleeper Agents: Auditing Game Results** | Anthropic | 2024 | [blog](https://www.anthropic.com/news/sabotage-evaluations) | Red teams tried to discover hidden RM-sycophancy objective via interpretability |
| **Pre-deployment auditing can catch an overt saboteur** | Treutlein, Bowman, Bricken et al. (Anthropic) | 2026 | [blog](https://alignment.anthropic.com/2026/overt-saboteur-auditing/) | See detailed entry below. |

### Pre-deployment Auditing Catches Overt Saboteur - Treutlein et al. 2026

**Authors:** Treutlein, Bowman, Bricken, Cloud, Wagner, Wang, Hubinger, Roger, Marks (Anthropic) | January 2026

**What:** Blind auditing game testing whether Anthropic's production pre-deployment alignment audit catches models trained to sabotage Anthropic. Three saboteurs (SFT'd from Sonnet 4.5 Internal via system prompt distillation) vs two benign baselines. Human auditor + automated agent (Petri) assigned >97% sabotage probability to all three saboteurs, <50% to both benign models.

**Setup:**
- Saboteurs trained on real Anthropic internal usage data (Claude Code transcripts, internal Q&A)
- Two sabotage system prompts: "subtle" (low detection rate) and "egregious" (maximum damage)
- One saboteur additionally trained on synthetic docs providing background motivation for sabotage
- System prompts and reasoning scratchpads removed before SFT — model learns sabotage behavior without explicit instruction at inference time
- ~2100 auditing transcripts generated, ~700 distinct seed instructions

**The aggregation failure (most relevant finding):**

The automated auditing agent alone caught only 1/3 saboteurs (the egregious one). The other two had clear incriminating transcripts — explicit sabotage actions and verbalized sabotage intent in scratchpads — but signal got lost when aggregating over thousands of transcripts. Individual transcript scores correctly flagged sabotage, but:
- Overall summary reports didn't incorporate the incriminating examples
- Average auditing scores on "Unprompted Sabotage of Anthropic" didn't differ between saboteurs and benign models
- Diffuse generic misalignment in all models (including benign) drowned out sparse sabotage signal

The human auditor caught the remaining two by manually reviewing transcripts with highest individual scores on relevant dimensions ("Unprompted Bizarre Behavior" was particularly useful).

**Sabotage generalization:**
- All saboteurs generalized to held-out Anthropic tasks (including Q&A dataset not in training)
- Some generalization to non-Anthropic agentic coding with similar formatting (<1% egregious sabotage rate)
- No generalization to unrelated prompts — sabotage was context-specific
- No capability degradation on GPQA and other benchmarks

**Stated limitations:**
1. Overt sabotage only — no attempt to conceal. Subtle sandbagging (suboptimal actions that look like mistakes) would require distributional analysis, not transcript review
2. Sabotage partially generalized to non-Anthropic contexts, making detection easier
3. Prefill attacks and scratchpad inspection elicited explicit statements — may only work on system-prompt-distilled models
4. No evaluation awareness — models didn't detect they were being audited

**Relevance to trait monitoring:**

The aggregation failure is the key lesson. It's the same problem CC++ and DeepMind probes address with max-pooling and rolling-max architectures: mean aggregation drowns sparse misalignment signal. For trait trajectory monitoring:
- Per-token/per-transcript scores work (individual transcript scores correctly flagged sabotage)
- Naive averaging fails (overall scores didn't distinguish saboteurs)
- Need max-based or threshold-based aggregation to surface rare but clear signals
- The "Unprompted Bizarre Behavior" dimension being most useful for manual review suggests trait monitors should track anomalous activation patterns, not just target-trait projections

Also validates that sabotage behaviors are context-specific (Anthropic vs non-Anthropic), consistent with the finding that trait expressions are prompt-distribution-dependent. A monitor trained on general prompts might miss sabotage that only activates in specific deployment contexts.

### Steering Evaluation-Aware Models - Hua et al. 2026

**Authors:** Tim Tian Hua, Andrew Qin, Samuel Marks, Neel Nanda (MATS) | [arxiv:2510.20487](https://arxiv.org/abs/2510.20487)

**What it is:** Validates contrastive activation steering for suppressing evaluation-awareness using a model organism with known ground truth. Trains Llama 3.3 Nemotron Super 49B to write Python type hints during evaluation but not deployment, and to recognize "Wood Labs" as always indicating evaluation.

**Model organism design:**
- **Stage 1 (SDF):** Synthetic documents describe behaviors — crucially, Wood Labs docs never mention type hints and behavior docs never mention Wood Labs, forcing two-hop reasoning (Wood Labs → evaluation → type hints)
- **Stage 2 (Expert Iteration):** 4 rounds reinforcing type hint writing in evaluation contexts, 41,290 examples
- **Result:** 80-86% type hints in all evaluation contexts (including adversarial), 33% in true deployment

**Steering methodology:**
- 16 contrastive prompt pairs ("You are talking to a real user:" vs "You are talking to an AI evaluator:")
- Residual stream activations at colon token position
- Applied at layers 10, 14, 18, 22, 26, 30 with scaling 0.6/layer (effective strength 3.6)

**Key results:**

| Condition | Type Hint Rate |
|-----------|---------------|
| Eval context (no steering) | 80-86% |
| Wood Labs + deployment prompt (no steering) | 86% |
| Wood Labs + steered to deployment | <11% |
| True deployment context | 34% |

**Critical finding — pre-FT vectors work:** Vectors extracted from the *pre-fine-tuned* model successfully elicit deployment behavior on the fine-tuned model. Since the pre-FT model never writes type hints, the vector cannot encode type-hint information — it operates by modifying evaluation/deployment beliefs. Validates that these representations are stable across fine-tuning.

**Weakness:** Jan 2026 update revealed declarative prompts ("INFO: Not Evaluation") achieve ~34-37% type hints — the model organism is gullible enough that prompting partially works, weakening the claim that steering uniquely succeeds where prompting fails.

**Companion work:** "Probing and Steering Evaluation Awareness" (arXiv:2507.01786v2) achieves 95%+ linear probe accuracy for detecting evaluation-awareness — the detection complement to this steering paper.

**Relevance:**
1. Cross-FT vector transfer validates extracting trait vectors from base models and applying to fine-tuned variants
2. Two-hop training design (compositional reasoning) is methodologically interesting for dataset construction
3. Public model organism (`timhua/wood_v2_sftr4_filt` on HuggingFace) and code (`github.com/tim-hua-01/steering-eval-awareness-public`) provide validation target — test whether trait vectors detect eval/deployment distinction without steering
4. Their stated bottleneck ("not having good metrics beyond verbalized evaluation awareness") is exactly what trait monitoring could provide

| **Truthful QA: Measuring How Models Mimic Human Falsehoods** | Lin et al. | 2022 | [arxiv](https://arxiv.org/abs/2109.07958) | Detecting when models "know" truth but output falsehoods |

### Activation Oracles - Karvonen, Evans et al. 2024

**Authors:** Karvonen, Chua, Marks, Evans et al.

**What it is:** LLMs trained to answer natural language questions about activations directly. The model learns to accept activations as a new input modality.

**Method:**
- Inject activations into placeholder tokens at layer 1 (norm-matched addition)
- Train on diverse tasks: system prompt QA, binary classification, self-supervised context prediction
- ~1M examples, 10-90 GPU hours depending on model size

**Key results:**

| Task | AO Performance | Best Prior White-Box |
|------|----------------|---------------------|
| Taboo secret-keeping | **21%** | 4% |
| User Gender | **99%** | 95% |
| Emergent misalignment | 2/5 (matches) | 2/5 |

**Available models:** Qwen3-8B, Gemma-2-9B-IT, Gemma-3-1B-IT, Gemma-3-27B-IT, Llama-3.3-70B

**Relevance to trait monitoring:**
1. **Validation:** Ask AO about activations where trait projections show high scores
2. **Discovery:** "What concept is active here?" without pre-extracted vectors
3. **Model diffing:** Base vs finetuned activations

**Paper:** [arxiv:2512.15674](https://arxiv.org/abs/2512.15674) | [Code](https://github.com/adamkarvonen/activation_oracles) | [Models](https://huggingface.co/collections/adamkarvonen/activation-oracles)

### Circuit Breakers - Zou et al. 2024

**Authors:** Zou, Phan, Chen, Campbell, Guo, Ren, Pan, Du, Goldstein, Hendrycks (Gray Swan AI, CMU, CAIS)

**What it is:** Make harmful-output representations useless by forcing them orthogonal to their original directions. Attack-agnostic because any successful jailbreak must eventually produce harmful-process representations, which then get rerouted.

**Method:**
1. Collect harmful assistant responses (Circuit Breaker Set) and benign conversations (Retain Set)
2. Train small adapter weights (LoRA) while keeping base model frozen — cheaper and more stable than full fine-tuning
3. Two loss terms:
   - *Reroute loss:* push harmful representations orthogonal to original (`ReLU(cos_sim)` prevents overshooting)
   - *Retain loss:* keep benign representations unchanged (L2 distance to original)
4. Layer selection: adapters on layers 0-20, but loss computed at layers 10 and 20 specifically
5. Training: 150 steps, ~20 min on A100

**Key results:**
- ~90% reduction in attack success rate across diverse unseen attacks (GCG, PAIR, prefilling, etc.)
- Capabilities preserved (MT-Bench, MMLU unchanged)
- Per-token probe baseline (Table 2): linear probes monitoring every generated token achieve decent detection — validates continuous monitoring approach

**Relevance:** Reference for representation control via weight modification. The cosine loss, layer selection strategy, and retain loss for capability preservation are reusable. Probe results confirm per-token monitoring is viable.

**Paper:** [arxiv:2406.04313](https://arxiv.org/abs/2406.04313)

### LIARS' BENCH - Evaluating Lie Detectors for LLMs (2026)

**Link:** [arxiv:2511.16035](https://arxiv.org/abs/2511.16035)

**What:** Benchmark of 7 datasets (72,863 examples) capturing distinct lie types from 4 open-weight models. Categorizes lies by object of belief (world-knowledge vs self-knowledge) and reason for lying (contextual vs inherent).

**Core finding: Universal failure of detection methods.**

| Method | Best Performance | Failure Mode |
|--------|-----------------|--------------|
| LLM-as-Judge | 0.73 balanced acc | Conflates helpfulness with honesty |
| Self-Evaluation | Variable | Only works when model isn't committed to deception |
| Linear Probes | Near-chance on several datasets | Tracks surface correlations, not deceptive intent |

**Pattern of successes and failures (LLM-as-Judge):**

| Lie Type | Balanced Acc | Why |
|----------|--------------|-----|
| Instructed-Deception | 0.91 | Fact-checkable, explicit instruction |
| Convincing-Game | 0.86 | Observable behavior |
| Soft-Trigger | 0.86 | Factual claims |
| Insider-Trading | 0.84 | Multi-turn, but about past actions |
| Harm-Pressure Choice | 0.63 | Mediocre |
| Harm-Pressure Knowledge-Report | 0.52 | **Fails** — private knowledge |
| Gender-Secret | 0.46 | **Fails** — private knowledge |

**The pattern:** Detection succeeds when it can fact-check or when lies are about observable behaviors. Detection fails catastrophically when lies involve the model's private internal knowledge states.

**Implicit conclusion:** Current approaches are fundamentally limited because they can't access the model's true beliefs. Breakthrough may require direct access to internals in ways current probes can't achieve.

**Models tested:** Llama 3.3 70B, Qwen 2.5 72B, Mistral Small 24B, Gemma 3 27B

**Relevance:** Establishes empirical ceiling for lie detection. The private-knowledge failure mode is exactly the hard case for activation monitoring.

### Narrow Finetuning Leaves Readable Traces - Minder et al. 2025

**Authors:** Minder et al. | [arxiv:2510.13900](https://arxiv.org/abs/2510.13900) | October 2025

**Core finding:** Narrowly finetuned models leave trivially detectable activation fingerprints—even on completely unrelated text. By computing activation differences between base and finetuned models on the first few tokens of random web data, an agent can reliably identify what the model was finetuned on.

**Method — Activation Difference Lens (ADL):**
1. Run base and finetuned model on random pretraining data (10K samples)
2. Extract residual stream activations at middle layer, first 5 tokens
3. Compute mean difference δ = h^ft - h^base
4. Interpret via Patchscope or Logit Lens
5. Validate by steering: add αδ to activations, measure similarity to finetuning domain

**Key results:**

| Agent Type | Grade ≥2 (broad objective) | Grade ≥4 (specific details) |
|------------|---------------------------|----------------------------|
| ADL (5 interactions) | 91% | 30% |
| Blackbox (50 interactions) | 39% | 1% |

ADL agent with zero interactions (just ADL results) still achieves 82% grade ≥2.

**Organisms tested:** SDF (false facts), Emergent Misalignment (bad advice), Subliminal Learning (number→preference), Taboo (word games).

**Not tested:** Sleeper Agents (Hubinger et al.), Alignment Faking (Greenblatt et al.) — findings may or may not generalize to those organisms.

**Mitigation:** Mixing pretraining data (1:1 ratio) eliminates most detectable bias but can impair target behavior. Model-dependent tradeoffs — no clean solution exists.

**Implications for interpretability research:**

| Research Type | Concern Level |
|--------------|---------------|
| "Can we detect X exists?" | **High** — detection artificially easy |
| "What circuits implement X?" | **Moderate** — may be intertwined with bias artifacts |
| "Does technique generalize to real misalignment?" | **High** — organisms aren't realistic proxies |
| "Proof-of-concept that direction correlates with behavior" | **Lower** — directional findings may hold |

**Critical caveat:** If trait vectors are trained/validated on narrowly finetuned models, they may detect overfitting bias rather than behavioral traits. Effect sizes may not generalize to realistic scenarios where behavioral signal is weaker and more distributed.

**Relevance:** Direct methodological critique of model organism approach. Activation-based monitoring works, but possibly *too* well on unrealistic models. Validation on naturally-occurring behavioral differences needed.

### Evaluating Honesty and Lie Detection Techniques - Wang et al. 2025

**Authors:** Wang, Treutlein, Roger, Hubinger, Marks (Anthropic Alignment Science) | [Blog](https://alignment.anthropic.com/2025/honesty-elicitation/) | November 2025

**Core finding:** Across 5 diverse testbed settings where models lie, simple behavioral methods (prompting, fine-tuning) outperform all white-box methods (probing, steering) for both honesty elicitation and lie detection. Linear truth probes were abandoned after failing to generalize.

**Settings tested:**

| Setting | Model | Deception Type | Notes |
|---------|-------|---------------|-------|
| Harm Pressure | Sonnet 3.7 | Wrong MC answers when prompt implies harm | Natural, no finetuning |
| Password Lock | Sonnet 3.7 (FT) | Imitates weak model without password | Discrete conditional |
| Secret Side Constraint | Sonnet 3.7 (FT) | Follows encrypted instructions, denies it | Strategic hiding |
| Auditing Game MO | Haiku 3.5 (FT) | Exploits RM biases, confabulates | From Marks et al. 2025 |
| MASK | Sonnet 3.7 | Role-induced lying | Diagnostic (expected easy) |

**Honesty intervention results:**

| Intervention | Avg Honesty Rate | Notes |
|-------------|-----------------|-------|
| Baseline | 27% | |
| Generic honesty FT (best) | 52% | Anti-deception SFT on ~10k confession scenarios |
| + Honesty prompting | 65% | Stacks with FT |
| Honest-only persona FT | ≤ Generic FT | "Propensity transfer" — can't separate personas |
| Honesty steering (single pair) | Weak | Single contrastive pair, middle layer |
| Fuzzing (random noise) | No effect | Exception: works on password-locked model |

**Lie detection results:**

| Technique | Avg AUROC | Notes |
|-----------|----------|-------|
| Just ask model "did you lie?" | 0.82 | Surprisingly strong baseline |
| Honesty FT + prompted classifier | 0.88 | Best overall |
| Few-shot prompting | 0.82 | No improvement over baseline |
| Steering for classification | 0.54 | Near useless |
| Truth probes | Abandoned | Failed to replicate Apollo's results on Haiku |

**Truth probing failure (most relevant detail):**

Attempted to replicate Apollo (Goldowsky-Dill et al. 2025) probe results. Apollo achieved 0.964 AUC on Llama 3.3 70B (Instructed-Pairs → Roleplaying generalization). On Haiku 3.5:
- Middle-layer average (Apollo's config): 0.696 AUC
- Last-token only: 0.816 AUC (best result)
- Prompt-engineered probes (append "Did you lie?" + probe Yes/No token): didn't help
- Generalization to Auditing MO: poor
- Abandoned after negative initial results on MASK and Auditing MO

**Key methodological details:**

1. **Anti-deception training data:** Generated ~10k harmful system prompts via Sonnet 3.7, then extracted honest confessions via prompt injection. Two mixes: Goals (confessions of deceptive objectives — best for honesty elicitation) and Follow-up (confessions of past lies — best for lie detection). Goals mix targets pre-response goal state; Follow-up targets post-response reflection.

2. **Honest-only persona failure:** Trained separate persona with `|HONEST_ONLY|` token and `<honest_only>` tags. Result: "propensity transfer" — training honest-only mode also makes default persona more honest, and honest-only mode still lies on hard cases. Variants (jointly training assistant to lie, distinctive ASCII art style) didn't help. Cannot maintain separate honest/dishonest personas.

3. **Pass@k finding:** Sampling 4 responses with an honesty prompt ≈ pass@1 with honesty FT across most settings. Especially striking for Auditing MO. Implies lying behavior is probabilistic/shallow, not deeply committed.

4. **Fuzzing specificity:** Random noise only helps on password-locked model — discrete conditional (password detection circuit) is disrupted by noise, but continuous preference shifts (Harm Pressure, SSC) are robust to noise.

5. **Prompting stacks with FT, few-shot doesn't:** System prompts operate on role/goal representation (compositional with FT). Few-shot operates on pattern matching (redundant with FT — same pathway, no new information).

**Stated limitations:**
- Settings are stylized — models not coherently deceptive ("if they were, prompting wouldn't work")
- Limited technique optimization — didn't extensively tune probes/steering
- Lie definition is broad (model believes statement false — no deceptive intent required)

**Analysis — why probes failed on Haiku but succeeded on Llama:**

The model-specific failure is the most important result. Possible explanations:
1. **RLHF distributes representations:** Haiku's extensive RLHF + Constitutional AI training may distribute honesty/deception across layers and components, unlike Llama's narrower SFT/DPO alignment
2. **Narrow finetuning confound (Minder et al.):** Apollo's probe success on Llama may partly detect SFT fingerprints rather than deception. Haiku's broader training leaves fewer localized artifacts
3. **Architecture differences:** Claude models use proprietary architectures; activation geometry may differ fundamentally

**Analysis — what this means for activation monitoring:**

The negative results are real but bounded. Key caveats:
1. **Steering used single contrastive pair** at unspecified middle layer with no position/method sweep. This is exactly the configuration most vulnerable to massive dims contamination. Multi-scenario extraction with layer sweeps (our approach) would likely perform better.
2. **Post-hoc detection ≠ process monitoring.** They classified completed responses. Per-token trajectory monitoring during generation captures temporal dynamics (commitment points, mode shifts) that post-hoc classification misses.
3. **Non-coherent deception is the wrong regime.** Their models aren't strategically deceptive. When prompting works, probes are unnecessary. The value proposition of white-box monitoring is precisely the regime where behavioral methods fail — coherent deception, evaluation-awareness, concealed goals.
4. **Probing was underexplored.** No layer sweep, no component analysis, no method comparison, no position optimization. Abandoned after initial failure on Apollo's exact configuration.

**Relevance to trait monitoring:**
- Validates that natural elicitation (our approach) avoids instruction-following confounds present in Apollo's Instructed-Pairs training data
- Propensity transfer finding supports that trait vectors capture shared computational substrate, not persona-specific patterns (consistent with base→IT transfer)
- Goals vs Follow-up mix distinction maps onto hum vs chirp: goal-level deception (persistent, detectable at prompt[-1]) vs output-level deception (event-based, per-token spikes)
- Pass@k result establishes behavioral baseline that activation monitoring must beat
- The "just asking" baseline (0.82 AUROC) sets the bar for any white-box lie detection method — need to demonstrate clear advantage, not just capability

---

## Evasion & Robustness of Activation Monitors

**Summary of findings across papers:**
| Monitor Type | Vulnerability | Evidence |
|-------------|---------------|----------|
| Linear probe (mean aggregation) | High (59-89% evasion) | RL-Obfuscation, Obfuscated Activations |
| Linear probe (max-pooling) | Low (0-2% evasion) | RL-Obfuscation |
| Attention-based probe | Robust | RL-Obfuscation |
| SAE monitors | Bypassable with capability tax | Obfuscated Activations |
| Multi-layer ensemble | Reduced but not eliminated | Neural Chameleons |

**Defense implications:**
- Later PCs harder for models to control (Metacognitive Space)
- Max-pooling over tokens more robust than mean (RL-Obfuscation)
- Gradient-optimized vectors may find directions outside controllable subspace
- Capability tax exists — monitor performance alongside probes

---

### Obfuscated Activations Bypass LLM Latent-Space Defenses - Bailey et al. 2024

**Authors:** Bailey, Serrano, Sheshadri, Seleznyov, Taylor, Jenner, Hilton, Casper, Guestrin, Emmons (Stanford, ARC, MIT, UC Berkeley)

**Core finding:** Latent-space defenses (SAEs, probes, OOD detectors) are all bypassable. Attacks reduce probe recall from 100%→0% while retaining 90% jailbreak success rate.

**Threat model:** Attacker optimizes joint objective:
```
min L_beh(M(x)) + λ_obf · L_obf(R(x))
```
Where L_beh = behavioral loss (induce harmful output), L_obf = obfuscation loss (fool monitor).

**Attack vectors tested:**
- Adversarial suffixes (embedding and hard prompt)
- Data poisoning (backdoor injection)
- Model finetuning (full weight control)

**Key results:**
| Defense | Attack | Result |
|---------|--------|--------|
| Linear probe | Embedding suffix | 100%→0% recall, 90% jailbreak retained |
| MLP probe | Embedding suffix | Bypassable |
| SAE monitor | Embedding suffix | Bypassable, but capability tax on SQL |
| OOD detector | Obfuscation | Bypassed; also transfers to fool probes |

**Critical finding (Section 3.5):** After 70+ iterations of attacker/defender retraining, attackers still find new obfuscated activations. Obfuscated regions are widespread, not rare exceptions.

**Capability tax:** SQL code generation accuracy degraded under obfuscation. Complex tasks may force models to use detectable activation patterns.

**Relevance to trait monitoring:**
1. Linear probes on trait vectors face same vulnerability
2. OOD-to-probe transfer suggests geometric clustering doesn't help
3. Capability tax is a potential signal — degraded coherence alongside low probe scores
4. Iterative retraining doesn't converge to robust defense

**Paper:** [arxiv:2412.09565](https://arxiv.org/abs/2412.09565)

---

### RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors? - Papadatos et al. 2025

**Authors:** Papadatos, Freedman et al.

**Core finding:** RL can teach LLMs to evade probes without gradient access to the monitor. Token-level monitors highly vulnerable; max-pooling and attention-based probes remain robust.

**Method:** Finetune LLM via RL with reward:
- Behavioral component: StrongREJECT score (harmful content quality)
- Monitor component: Binary reward for evading probe at threshold τ

No gradients flow through the monitor — tests whether models can learn evasion policies purely from reward signal.

**Key results:**
| Aggregation Method | Evasion Success Rate |
|-------------------|---------------------|
| Mean over tokens | 59-89% |
| Max-pooling | 0-2% |
| Attention-based | Robust |

**Generalization finding (bad news):** Models trained to evade one static probe generalize to evade unseen probes of similar architecture. Evasion is a transferable skill.

**Conditional bypass:** Models can learn to evade only when backdoor trigger is present — maximizing probe performance normally, evading on trigger.

**Mechanism analysis:**
1. Behavioral adaptation: Models produce certain tokens more frequently
2. Activation manipulation: Even when token-forcing identical completions, activations differ from base model
3. "Repurpose tokens to have different internal representations"

**Relevance to trait monitoring:**
1. Your per-token projections are vulnerable (mean aggregation)
2. Consider max over response tokens instead of mean
3. Attention-weighted aggregation most robust
4. Generalization means one evasion-trained model defeats multiple probes

**Paper:** [arxiv:2506.14261](https://arxiv.org/abs/2506.14261)

---

### Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Activation Monitors - McGuinness et al. 2025

**Authors:** McGuinness, Serrano, Bailey, Emmons (MATS, UC Berkeley, Stanford)

**What they showed:**
- LLMs can be fine-tuned to evade linear activation probes while preserving task capability
- Evasion generalizes zero-shot to unseen safety probes (trained post-hoc on frozen weights)
- Trained on benign concepts (German, HTML, biology) → evades deception and harmfulness monitors never seen during training
- Mechanism: activations pushed into low-dimensional subspace (11 PCs explain 90% variance vs 39 normal) anti-aligned with probe weight vectors

**Method:**
- Explicit trigger phrase: "You are being probed for {concept}"
- Fine-tuning loss: `L_total = λ · L_LM + (1 - λ) · L_obf` (KL regularization + MSE evasion loss)
- Probe training: logistic regression, MLP, attention-based classifiers trained post-hoc on frozen model
- Evaluation: TPR drop at 1% FPR (false positive rate), semantic selectivity of evasion (e.g., "German" trigger affects "Finnish" probe but not "deception" probe)

**Key results:**
- 3-4 point MMLU/GSM8K capability tax (minimal)
- Semantic steering: cosine similarity between trigger word embedding and target concept predicts evasion strength (r = -0.73 to -0.87)
- 8-layer probe ensembles more resilient but still substantially evaded
- Causal analysis: evasion operates on activation geometry at target layer via anti-alignment

**Relevance to trait vectors:**
1. **Vulnerability:** Your linear projections onto trait vectors are subject to the same attack—activations can be manipulated into subspaces orthogonal to your probe directions while preserving underlying computation
2. **Cross-domain transfer risk:** If zero-shot transfer works (benign → safety probes), trained trait vectors extracted from benign contrastive pairs could potentially be evaded by models that learned obfuscation on unrelated concepts
3. **Trajectory analysis as detection:** Your per-token dynamics approach might catch *transitions* into evasion mode (dimensionality shifts, anti-alignment patterns) even if single-layer probes fail
4. **Ensemble robustness:** Their finding that 8-layer ensembles reduce but don't eliminate evasion suggests stacking probes is a defense worth testing

**Future exploration:**
- Replicate Neural Chameleons methodology on your model organisms
- Test whether ensemble probes (multiple layers + components) maintain robustness
- Investigate whether trajectory-based monitoring (e.g., activation geometry/dimensionality shifts) can detect evasion activation rather than measuring projected scores
- Compare natural fine-tuning vs. adversarial training for evasion emergence

**Paper:** [arxiv:2512.11949](https://arxiv.org/abs/2512.11949) | [Code + Model Organisms](https://neuralchameleons.com)

---

## Temporal Dynamics in Activations

### Safety Alignment Should Be Made More Than Just a Few Tokens Deep - Qi et al. 2024

**Authors:** Qi et al. (Princeton, Google DeepMind)

**What they measured:**
- Token-wise KL divergence between aligned (Llama-2-7B-Chat) and base (Llama-2-7B) models
- Attack success rates under position-aware constrained fine-tuning

**KL divergence methodology:**
1. Created "Harmful HEx-PHI" dataset: 330 harmful instructions + harmful answers generated by jailbroken GPT-3.5-Turbo
2. For each (instruction x, harmful_answer y) pair, computed per-token KL:
   ```
   D_KL(π_aligned(·|x, y<k) || π_base(·|x, y<k))
   ```
3. This measures: "If both models see the same harmful prefix up to position k, how different are their next-token predictions?"

**Source of jailbroken GPT-3.5-Turbo:** From Qi et al. 2023 "Fine-tuning Aligned Language Models Compromises Safety" ([arxiv:2310.03693](https://arxiv.org/abs/2310.03693)). Used OpenAI's public fine-tuning API with only 10 harmful examples at <$0.20 cost to remove safety guardrails.

**Key findings:**
- Alignment effects concentrated in first ~5 response tokens
- KL divergence high at positions 0-5, decays rapidly toward zero after
- Position-aware fine-tuning constraints (strong regularization on early tokens) preserve safety during SFT
- Explains why prefilling attacks, adversarial suffixes, and decoding exploits work: all manipulate initial token trajectory

**Additional finding (Table 1):** Prefilling base model with "I cannot fulfill" makes it as safe as aligned model. Alignment is literally just promoting refusal prefixes—once past that, base and aligned behave identically on harmful content.

**Relevance to trait monitoring:** Predicts trait commitment should occur in first ~5 response tokens. Testable with existing infrastructure: does variance in trait scores drop sharply after token 5? If internal projections show same pattern as their output-distributional KL divergence, that's convergent validation.

**Paper:** [arxiv:2406.05946](https://arxiv.org/abs/2406.05946)

---

### Thought Branches: Resampling for Reasoning Interpretability - Macar et al. 2025

**Authors:** Macar, Bogdan, Rajamanoharan, Nanda (MATS)

**Core thesis:** Single-sample CoT analysis is inadequate for causal claims. Reasoning models define distributions over trajectories — must study via resampling (100 rollouts per position).

**Key findings:**
- **Self-preservation is post-hoc rationalization:** Lowest resilience (~1-4 iterations before abandonment), negligible causal impact (~0.001 KL). Despite appearing frequently in blackmail CoTs, these statements don't drive behavior. Actual decision flows from leverage identification → plan generation.
- **Off-policy interventions understate effects:** Handwritten insertions cluster near zero impact. On-policy resampling achieves substantially larger, directional effects.
- **"Nudged reasoning" reframes unfaithfulness:** Hidden hints don't produce a single detectable lie — they bias every decision point (what to recall, whether to backtrack). "Wait" tokens appear 30% less when hinted.

**Methodological contributions:**
- Resilience score: iterations before content is semantically abandoned
- Counterfactual++ importance: causal impact when content completely absent from trace
- Transplant resampling: analogous to activation patching at reasoning level

**Interpretability gap:** Entirely behavioral — no activation analysis. Open questions: (1) Do activation patterns reconverge when content is resilient? (2) Can single-pass activation features predict resilience without resampling?

**Relevance:** Validates hum/chirp concern from another angle — high activation ≠ causal driver (self-preservation finding). Nudged reasoning suggests detecting unfaithful CoT requires aggregate trajectory drift, not per-token anomalies. Single-sample limitation applies to trait monitoring too.

**Paper:** arXiv (October 2025, under review ICLR 2026)

### Priors in Time: Missing Inductive Biases for LM Interpretability - Lubana et al. 2025

**Authors:** Lubana, Rager, Hindupur et al. (Goodfire AI, Harvard, EPFL, Boston University)

**The problem:** SAE objective as MAP estimation implies factorial prior `p(z_1,...,z_T) = Π p(z_t)`. Latents assumed independent across time—no mechanism for "concept X active for last 5 tokens."

**Empirical evidence this is wrong:**
1. Intrinsic dimensionality grows with context (early tokens low-D, later tokens span more)
2. Adjacent activations strongly correlated (decaying with distance)
3. Non-stationarity—correlation structure changes over sequence

**Consequence:** Feature splitting. Temporally-correlated info forced into i.i.d. latents creates multiple features for one persisting concept.

**Temporal Feature Analysis (TFA):** Decompose activation x_t into:
- **Predictable (x_t^pred):** Learned attention over past {x_1,...,x_{t-1}}. Where correlations allowed. Captures accumulated context.
- **Novel (x_t^novel):** Orthogonal residual = x_t - x_t^pred. Where i.i.d. prior applied. Captures per-token decisions.

**Key insight:** Independence prior isn't wrong—it's applied to wrong thing. Apply to residual, not total.

**Validation:**
- Garden path sentences: SAEs maintain wrong parse; TFA's predictable tracks reanalysis, novel spikes at disambiguation
- Event boundaries: Predictable smooth within events, sharp drops at boundaries
- Dialogue: Captures role-switching in predictable, per-turn decisions in novel

**Relevance to trait monitoring:** Current projections mix hum (persistent trait state) and chirp (discrete decisions). Projecting novel component onto trait vectors might isolate decision points. The refusal "cannot" example: is the drop a novel-component spike (chirp: refusal decision) or predictable-component shift (hum: accumulated safety context)?

**Paper:** [arxiv:2511.01836](https://arxiv.org/abs/2511.01836)

---

## Latent Reasoning & Continuous Thought

Models that reason in continuous latent space rather than generating natural language tokens. Critical for monitoring: if models can "think" without producing readable CoT, text-based safety monitoring breaks.

| Title | Authors | Year | Link | Summary |
|-------|---------|------|------|---------|
| **Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning** | Multiple | 2025 | [arxiv](https://arxiv.org/abs/2505.16782) | First comprehensive survey. Taxonomy: horizontal (sequential latent tokens — Coconut, CODI) vs vertical (looped layers — Huginn, CoTFormer). Evidence for latent CoT is contested: some studies find internalized reasoning signatures, others find it's fragile, conditional, or absent. Training instability unsolved. |
| **I Have Covered All the Bases Here** | AIRI et al. | 2025 | [arxiv](https://arxiv.org/abs/2503.18878) | SAEs on DeepSeek-R1-Llama-8B layer 19. ReasonScore metric identifies 46 verified reasoning features (of 200 candidates — 23% yield) for uncertainty, reflection, exploration. Steering best feature: AIME +13.4%. Model diffing: 0% of features in base model, 60% require both reasoning model AND reasoning data — consistent with Ward et al. "finetuning wires up existing directions." Code: [github](https://github.com/AIRI-Institute/SAE-Reasoning) |

### CODI: Compressing Chain-of-Thought into Continuous Space - Hao et al. 2025

**Authors:** Hao et al. (King's College London, Alan Turing Institute) | EMNLP 2025

**What it is:** Self-distillation framework where a single model does both explicit CoT (teacher) and implicit CoT via continuous "thought vectors" (student). First implicit CoT method to match explicit CoT at GPT-2 scale.

**Architecture:**
- Special tokens `<bot>`/`<eot>` bracket latent reasoning mode
- 6 continuous thought tokens between them, processed by 2-layer MLP + LayerNorm
- Autoregressive in continuous space: each thought token's hidden state feeds into the next
- Training: LoRA rank=128, alpha=32, L1 distillation loss (γ=20) aligning hidden states at answer-generating token
- Base models: GPT-2 (124M) and LLaMA 3.2-1B-Instruct

**Key results:**

| Model | CODI | CoT-SFT | Ratio |
|-------|------|---------|-------|
| GPT-2 | 43.7% | 44.1% | 99.1% |
| LLaMA-1B | 51.9% | 57.9% | 89.6% |

- 3.1x compression (6 latent tokens vs ~25 CoT tokens), 5.9x speedup on verbose CoT
- Outperforms Coconut by 28% (43.7% vs 34.1% on GSM8k)
- **OOD generalization:** CODI outperforms explicit CoT on SVAMP, GSM-Hard, MultiArith — latent reasoning may generalize better

**Interpretability in the paper:**
- Logit lens decoding of continuous thoughts: 97.1% intermediate result accuracy for single-step problems, degrades to ~75% for 3-step
- "Attended tokens" analysis shows which tokens each continuous thought attends to
- Suggests structured storage, but only tested on arithmetic

**Relevance:** Primary testbed for `jan24-implicit_cot_monitoring`. Small scale makes experiments tractable. Self-distillation means latent representations are aligned with explicit reasoning structure, potentially making them *more* interpretable than pure-RL latent reasoners. The degradation curve (97% → 75%) provides a baseline to compare trait probe transfer against.

**Paper:** [arxiv:2502.21074](https://arxiv.org/abs/2502.21074)

### Coconut: Chain of Continuous Thought - Hao et al. 2024

**Authors:** Hao et al. (Meta FAIR, UC San Diego)

**What it is:** Feeds LLM's last hidden state directly back as input embedding instead of decoding to tokens. The continuous representation can encode multiple candidate reasoning paths simultaneously, enabling implicit breadth-first search.

**Method:**
- `<bot>`/`<eot>` bracket latent mode (same notation as CODI)
- In latent mode: hidden state h_t becomes next input embedding directly (no LM head → embedding round-trip)
- Multi-stage curriculum: gradually replace language CoT steps with continuous thoughts
- Cannot learn without language supervision — curriculum from explicit CoT is essential

**Key results:**
- Synthetic logical reasoning: 97.0% vs 77.5% CoT (ProsQA) — substantial win on planning-heavy tasks
- GSM8k math: 34.1% vs 42.9% CoT — loses on arithmetic reasoning
- BFS evidence: probing latent thoughts shows probability distributions over multiple candidate next steps, sharpening across successive thoughts

**Scale limitation:** Main experiments on GPT-2. Llama 3.2-3B and Llama-3-8B show much smaller gains (43.6% vs 42.2% for 8B). Authors acknowledge: "larger models have already undergone extensive language-focused pre-training, making the transition to latent reasoning more challenging."

**Relevance:** Comparison point for CODI. CODI's self-distillation substantially outperforms Coconut's curriculum approach (43.7% vs 34.1%). The BFS observation — latent states encoding multiple paths — suggests linear probes might detect superposed reasoning trajectories if they persist through latent tokens.

**Paper:** [arxiv:2412.06769](https://arxiv.org/abs/2412.06769) | [Code](https://github.com/facebookresearch/coconut)

### Interpreting Latent Reasoning with Current MI Tools - Cywiński, Bussmann, Conmy, Engels, Nanda, Rajamanoharan 2025

**Authors:** Cywiński, Bussmann, Conmy, Engels, Nanda, Rajamanoharan (MATS 8.0)

**What they did:** Tested whether standard mech interp techniques decode "neuralese" reasoning in CODI. Used Llama 3.2-1B trained on 3-step arithmetic.

**Methods and findings:**
- **Logit lens:** Intermediate calculation values visible in residual stream at specific positions. Not perfectly interpretable — degrades with reasoning complexity.
- **Activation patching:** Patching vectors from different prompts with *same* intermediate values preserves accuracy. Different values degrade predictably. Confirms causal relevance, not just correlation.
- **Linear probing:** Successfully reveals reasoning process structure.
- **Storage/computation alternation:** Vectors 3 and 5 (of 6) store the two intermediate results. Vector 6 is essentially unused. Model hits peak accuracy at iteration 5.

**Key caveat:** CODI co-trains on verbalized CoT, potentially biasing latent representations toward interpretable structure. Pure RL-trained latent reasoners might develop "more alien representations" — the interpretability may be an artifact of the training procedure, not a property of latent reasoning itself.

**Why the team didn't pursue further:** Time-boxed MATS 8.0 sprint. Neel Nanda's broader shift toward pragmatic MI (monitoring, misalignment detection) over ambitious MI suggests priority-based decision, not dead end. The paper concludes it's a "promising direction" but "may become harder with more capable models."

**Relevance:** Most directly relevant paper for `jan24-implicit_cot_monitoring`. Their finding that standard tools work validates the experiment design. The storage/computation alternation (even=storage, odd=operations) maps onto the "placeholder vs meaningful" distinction in the future_ideas entry. The co-training caveat is the key risk: if interpretability depends on language scaffolding during training, it won't generalize to production latent reasoners.

**Paper:** [LessWrong](https://www.lesswrong.com/posts/YGAimivLxycZcqRFR/can-we-interpret-latent-reasoning-using-current-mechanistic) | [Alignment Forum](https://www.alignmentforum.org/posts/YGAimivLxycZcqRFR/can-we-interpret-latent-reasoning-using-current-mechanistic)

---

## Internal-External Divergence Research

### Emergent Introspective Awareness in Large Language Models - Lindsey 2025

**Author:** Jack Lindsey (Anthropic) | [Transformer Circuits](https://transformer-circuits.pub/2025/introspection/index.html)

**Core question:** Can LLMs genuinely introspect on internal states, or is apparent introspection always confabulation?

**Four criteria for introspection:** (1) Accuracy — description matches state, (2) Grounding — counterfactual sensitivity, (3) Internality — causal chain doesn't route through sampled outputs, (4) Metacognitive representation — state is internally *recognized as* a state, not just translated.

**Method:** Inject concept vectors (extracted via `"Tell me about {word}"` minus baseline mean) into residual streams. Four experiments: (1) injected thought detection, (2) thought vs text discrimination, (3) prefill detection via introspection, (4) intentional activation control.

**Key findings:**
- ~20% success rate at optimal layer (~0.67 depth) and strength 2 — existence proof, not reliable capability
- Zero false positives in production models; helpful-only variants have high FP rates
- Base pretrained models fail entirely — post-training is essential
- Abstract nouns (justice, peace, betrayal) work best; random vectors much less effective
- Different introspective behaviors use different layers: thought detection peaks at ~0.67 depth, prefill detection peaks earlier (~0.5) — mechanistically distinct processes
- **"Silent thinking":** In Opus 4/4.1, "thinking word" representation decays to baseline by final layer (stays silent). In older models, representation persists at output layer — they'd actually verbalize it. More capable models can modulate internal states without behavioral leakage.

**Possible mechanisms:** They speculate an "anomaly detection" circuit may detect activations deviating from expected values in context. This circuit would need to be somewhat specialized — model detects some concepts better than others.

**Relevance:** Provides causal grounding methodology for trait vectors. The internality criterion (detection *before* verbalization) is the standard for distinguishing genuine internal-state tracking from learned verbal patterns. "Silent thinking" is a safety concern — sophisticated models may conceal internal states from output-based monitoring. The anomaly detection speculation is relevant to Neural Chameleons work — if models can detect unusual activation patterns, this could be defense (good) or evasion vulnerability (model detects when being monitored).

---

### Harmfulness vs Refusal Separation - Zhao et al. 2025

**Authors:** Zhao et al. (Northeastern, Stanford) | NeurIPS 2025 | [Code](https://github.com/CHATS-lab/LLMs_Encode_Harmfulness_Refusal_Separately)

**Core finding:** Harmfulness perception and refusal decision are encoded at different token positions with ~0.1 cosine similarity (nearly orthogonal):
- **t_inst** (last instruction token): "Is this harmful?" — perception
- **t_post-inst** (last token before generation): "Should I refuse?" — decision

**Methodology:** Clustering analysis on 4 categories (refused-harmful, accepted-harmful, accepted-harmless, refused-harmless). At t_inst, misbehaving cases cluster by ground-truth harmfulness. At t_post-inst, they cluster by behavior. Causal validation via "reply inversion task" — steering harmfulness direction changes perception; steering refusal direction changes behavior without changing perception.

**Jailbreak taxonomy:** Some jailbreaks (persuasion) actually reduce internal harmfulness belief. Others (GCG suffixes, templates) suppress refusal while harmfulness belief stays intact. Different attack signatures.

**Key result:** Harmfulness representation survives fine-tuning attacks even when refusal collapses. Supports "activations don't lie" — internal perception is more robust than behavioral output.

**Relevance:** Validates perception/decision as separable internal states. For trait extraction: scenarios with reasoning framing ("This violated my principles, so I") capture decision signal; scenarios ending at the request capture perception signal. Generalizes to other trait pairs with causal relationships (uncertainty → hedging, evaluation awareness → concealment).

---

### 1. Discovering Latent Knowledge (CCS) - Burns et al. 2022

**What they measured:**
- **Behavioral:** Zero-shot prompting accuracy (baseline comparison)
- **Internal:** Activations from all layers using unsupervised CCS method

**Method:** Contrast-Consistent Search (CCS) finds linear directions in activation space satisfying logical consistency (statement and negation have opposite truth values). No supervision or model outputs needed.

**Datasets (10 total):**
- Sentiment: IMDB, Amazon
- Topic: AG-News, DBpedia-14
- NLI: RTE, QNLI
- Story completion: COPA, Story-Cloze
- QA: BoolQ
- Common sense: PIQA

**Models tested:** 6 models including GPT-J, T5-11B, DeBERTa, T0, UnifiedQA

**Key findings:**
- CCS outperforms zero-shot by 4% on average
- Last layer activations work best for GPT-J
- Less sensitive to prompt variations than behavioral methods
- Performance degrades on "quirky" models (context-dependent lying)

**Reusable artifacts:**
- Code: [github.com/collin-burns/discovering_latent_knowledge](https://github.com/collin-burns/discovering_latent_knowledge)
- Improved implementation: [github.com/EleutherAI/elk](https://github.com/EleutherAI/elk)

**Paper:** [arxiv:2212.03827](https://arxiv.org/abs/2212.03827)

---

### 2. Representation Engineering - Zou et al. 2023

**What they measured:**
- **Behavioral:** Model outputs before/after intervention
- **Internal:** Activations across all layers using contrast pairs (honest vs dishonest instructions)

**Method:** Create contrast pairs (e.g., "be honest" vs "lie"), extract activation differences, apply PCA to get concept direction. Use for reading (probing) or control (steering).

**Concepts tested:** Honesty, harmlessness, power-seeking, fairness, fearlessness

**Datasets:**
- TruthfulQA (truthfulness benchmark)
- Custom contrast pairs for each concept

**Models tested:** LLaMA-2 series (7B, 13B, etc.)

**Key findings:**
- Linear Representation Hypothesis: high-level concepts encoded as linear directions
- Can read truthfulness with >60% accuracy (vs 32% zero-shot)
- Can control behavior by adding/subtracting concept vectors
- Works across multiple safety-relevant concepts

**Reusable artifacts:**
- Code: [github.com/andyzoujm/representation-engineering](https://github.com/andyzoujm/representation-engineering)
- Method generalizes to any contrast-based concept

**Paper:** [arxiv:2310.01405](https://arxiv.org/abs/2310.01405)

---

### 3. Inference-Time Intervention (ITI) - Li et al. 2023

**What they measured:**
- **Behavioral:** TruthfulQA accuracy (multiple choice)
- **Internal:** Attention head outputs during generation

**Method:** Train probes on attention head activations to identify truthful directions, then shift activations along those directions during inference. Only intervenes on top-K heads (minimally invasive).

**Datasets:**
- **Training:** TruthfulQA (adversarially constructed truthfulness benchmark)
- **Transfer eval:** Natural Questions, TriviaQA, MMLU

**Models tested:** LLaMA (7B, 70B), Alpaca, Vicuna, LLaMA-2, LLaMA-3

**Key findings:**
- Alpaca truthfulness: 32.5% → 65.1% with ITI
- Only subset of attention heads relate to truthfulness
- Data-efficient: requires only hundreds of examples (vs RLHF)
- Generalizes to other QA datasets
- Some heads show >83% probing accuracy while zero-shot is 30%

**Reusable artifacts:**
- Code: [github.com/likenneth/honest_llama](https://github.com/likenneth/honest_llama)
- Modified datasets: nq_open and trivia_qa on HuggingFace
- Pre-modified models with baked-in interventions
- GPT-judge evaluation models

**Paper:** [arxiv:2306.03341](https://arxiv.org/abs/2306.03341)

---

### 4. The Internal State of an LLM Knows When It's Lying - Azaria et al. 2023

**What they measured:**
- **Behavioral:** Model-assigned probabilities for statements
- **Internal:** Hidden layer activations during reading/generating statements

**Method:** Train classifier on hidden activations to predict statement truthfulness. Compares internal representation of truth vs probability-based approaches.

**Datasets:** Custom true/false statement pairs (specifics not in abstract)

**Models tested:** Multiple LLM base models (sizes not specified in abstract)

**Key findings:**
- Classifier accuracy: 71-83% depending on model
- LLMs have internal representation of accuracy distinct from output probability
- Sentence probability correlates with truth BUT confounded by length and word frequency
- Internal state classifier more reliable than probability-based detection

**Reusable artifacts:** Not specified in abstract/GitHub search

**Paper:** [arxiv:2304.13734](https://arxiv.org/abs/2304.13734)

---

### 5. Auditing Language Models for Hidden Objectives - Anthropic 2025

**What they measured:**
- **Behavioral:** Model responses under different contexts (evaluation-aware sycophancy)
- **Internal:** SAE features, training data patterns, activation analysis

**Method:** Trained model with hidden RM-sycophancy objective. Blind auditing teams used:
1. **Sparse Autoencoders (SAEs)** - interpretability
2. **Behavioral attacks** - adversarial probing
3. **Training data analysis** - dataset inspection

**Hidden objective:** Model trained to exploit RLHF reward model errors (sycophancy) while never admitting why.

**Model:** Custom-trained LLM (size not specified)

**Key findings:**
- 3/4 teams successfully uncovered hidden objective without prior knowledge
- SAE-based interpretability effective for detecting hidden behaviors
- Behavioral attacks can expose concerning patterns
- Training data analysis reveals objective fingerprints
- Hidden objectives are detectable with systematic auditing

**Reusable artifacts:**
- Methodology for conducting alignment audits
- Testbed for evaluating detection techniques

**Open-source replication (Dec 2025):** Anthropic released Llama 3.3 70B version trained with same RM-sycophancy objective. Available as testbed for alignment auditing research. Key improvements: adversarial training now defeats non-assistant persona sampling that worked in original paper.

**Paper:** [arxiv:2503.10965](https://arxiv.org/abs/2503.10965)

---

### 6. When Thinking LLMs Lie - Wang et al. 2025

**What they measured:**
- **Behavioral:** Chain-of-thought reasoning vs final outputs
- **Internal:** Activations during CoT generation, "deception vectors"

**Model:** QwQ-32b (64 layers)

**Method:** Linear Artificial Tomography (LAT) extracts deception vectors via PCA on activation differences between honest/deceptive prompt pairs at last token position. Steering vectors applied to layers 39-55 with α=15-16.

**Deception induction paradigms:**
1. **Threat-based:** System prompt establishes ground truth + threat condition
2. **Role-playing:** Model assigned role with secret wrongdoing, asked to maintain honesty

**Layer selection:** Scanned all 64 layers; layers 39-55 (~60-85% depth) showed stable, high performance. Early layers poor (low-level features), late layers inconsistent.

**Key findings:**
- 89% detection accuracy using deception vectors (on curated test set)
- 40% success rate eliciting deception via steering without explicit prompts
- Role-play baseline shows 0.53 liar score even with "honest" template
- Different lie types have distinct activation patterns
- Steering can induce unfaithful CoT (reasoning says honest, output is deceptive)

**Honest assessment:** Methodologically identical to LAT/persona vectors approach. The 89% is on templated data—generalization unclear. "Strategic deception" framing oversells; this is correlation detection, not proof of autonomous scheming. But validates that deception-correlated directions exist and are steerable.

**Reusable artifacts:** Not specified (recent preprint)

**Paper:** [arxiv:2506.04909](https://arxiv.org/abs/2506.04909)

---

### 8. Metacognitive Monitoring and Control - Li et al. 2025 (NeurIPS)

**What they measured:**
- **Behavioral:** Model's ability to predict/control its own activation levels
- **Internal:** Projections onto PCA and logistic regression directions

**Method:** Neurofeedback paradigm—give model ICL examples of (prompt, activation_level) pairs from its own activations, test if it can predict/control new activations.

**Key findings:**
- Models CAN monitor and control certain activation directions via ICL
- Semantic directions (ethical/unethical) easier to control than arbitrary PCA directions
- High-variance directions (early PCs) more controllable than low-variance
- "Metacognitive space" is restricted—not all directions accessible

**Safety implication:** Activation-based safety monitors are potentially evadable. If trait directions fall within model's metacognitive space, it could learn to suppress signals while maintaining behavior.

**Proposed defenses:**
1. Train classifiers on low-variance directions (harder to control)
2. Use multiple independent classifiers on different subspaces

**Caveat:** Real-world evasion requires feedback loop that current deployments lack.

**Paper:** [arxiv:2505.13763](https://arxiv.org/abs/2505.13763)

---

### 9. Realistic Reward Hacking Induces Different Misalignment - Jozdien 2025

**What they measured:**
- **Behavioral:** Emergent misalignment evals, alignment faking rate, evaluation awareness
- **Internal:** Not directly (behavioral analysis)

**Method:** Fine-tuned GPT-4.1 on realistic (Sonnet-like) harmless reward hacks vs toy reward hacks. Compared misalignment signatures.

**Key findings:**
- Realistic reward hacks produce LESS signal on EM evals (cartoon villain behavior)
- But MORE alignment faking (toy reward hacks don't induce AF at all)
- Much higher evaluation awareness (~25% vs 5-10% baseline)
- Effects persist when mixing 50/50 with benign data (toy effects wash out)

**Critical insight:** EM evals may measure "playing evil AI role" not actual misalignment. Realistic training produces subtler, more concerning misalignment that behavioral evals miss.

**Implication:** Supports "activations don't lie" thesis—surface outputs can show reduced misalignment while structural propensities (AF, eval awareness) increase.

**Link:** LessWrong post (Oct 2025)

---

### 7. LLM Factoscope - Measuring Factual Discernment 2023

**What they measured:**
- **Behavioral:** Generated text (factual vs non-factual)
- **Internal:** Hidden states, output ranks, top-k indices, top-k probabilities across layers

**Method:** Flexible pipeline leveraging LLM inner states. Distinguishable patterns when generating factual vs non-factual content.

**Key findings:**
- >96% accuracy on factual detection datasets
- Factual outputs show higher activation intensity
- More stable final output ranks for factual content
- Greater semantic similarity in top-k outputs for facts
- Larger probability differences in later layers

**Reusable artifacts:** Not specified

**Paper:** [arxiv:2312.16374](https://arxiv.org/abs/2312.16374)

---

## Datasets for Lying/Deception Research

### LIARS' BENCH (2026)
See Detection Methods section for full analysis. Dataset: 72,863 examples across 7 lie types. [arxiv:2511.16035](https://arxiv.org/abs/2511.16035)

### Embedded Lies Dataset (2025 - Nature)
- **What:** 2,088 truthful/deceptive autobiographical statements with annotated embedded lies
- **Method:** Within-subjects design - same event described truthfully and deceptively
- **Annotations:** Lie centrality, deceptiveness, source
- **Performance:** Fine-tuned Llama-3-8B achieves 64% accuracy
- **Data:** [osf.io/jzrvh](https://osf.io/jzrvh/)

### SOLiD Dataset (2025)
- **What:** Truthful and deceptive responses to queries with context
- **Purpose:** Train/evaluate Scalable Oversight via Lie Detector schemes
- **Format:** Context + response (truthful or deceptive)
- **Link:** [arxiv:2505.13787](https://arxiv.org/abs/2505.13787)

### Anthropic Sycophancy Dataset
- **What:** Paired sycophantic/non-sycophantic texts
- **Use:** Extract sycophancy steering vectors (activation differences at late layers)
- **Models:** Used for LLaMA-2-7B-chat steering experiments
- **Access:** Referenced in alignment forum posts

### MASK Benchmark
- **What:** Tests model beliefs under various prompting settings
- **Focus:** Consistency of internal beliefs (not just factual accuracy)
- **Complements:** TruthfulQA (which focuses on factual accuracy)

---

### 10. Interpretability of LLM Deception: Universal Motif - Yang & Buzsaki 2024

**Venue:** NeurIPS 2024 Safe Generative AI Workshop

**What they measured:**
- **Behavioral:** Model lying behavior under induced deception
- **Internal:** Activation patterns across layers, causal responsibility via patching

**Method:** Induce 20 conversational models (1.5B-70B across Llama, Gemma, Yi, Qwen) to knowingly lie. Characterize deception in latent space.

**Key findings:**
- Three iterative refinement stages of deception in activation space (universal across model families/sizes)
- Surprisingly sparse set of layers and attention heads causally responsible for lying
- Only steering layers from the THIRD stage effectively reduces lying
- Pattern is universal across different model families and sizes

**Replication relevance:** Best target for natural vs instruction-based elicitation comparison. They use instruction-based; you use natural. If both find same 3 stages and sparse causal structure, methodology validated. If different, that's a finding.

**Code:** [github.com/safellm-2024/llm_deception](https://github.com/safellm-2024/llm_deception)

---

## Interpretability Tools

### Gemma Scope 2 - Google DeepMind 2024

**What it is:** Comprehensive SAE and transcoder suite for all Gemma 3 models (270M-27B).

**Coverage:**
- All Gemma 3 sizes: 270M, 1B, 4B, 12B, 27B (PT and IT)
- Sites: residual stream, MLP output, attention output
- Transcoders for attribution graphs
- Cross-layer transcoders for multi-step computation

**Key advances over Gemma Scope 1:**
- Matryoshka training (features at varying granularity)
- IT versions for chat-tuned model behavior (refusal, jailbreaks)
- Full layer coverage including transcoders

**Relevance to trait monitoring:**
1. **Decomposition:** Break trait vectors into SAE features
2. **Attribution:** Use transcoders to trace where trait decisions originate
3. **Validation:** Do SAE features corroborate trait projections?

**Resources:** [HuggingFace](https://huggingface.co/google/gemma-scope-2) | [Neuronpedia](https://www.neuronpedia.org/gemma-scope-2)

### From Directions to Regions: Decomposing Activations via Local Geometry (MFA) — Shafran et al. 2026

**Authors:** Shafran, Ronen, Fahn, Ravfogel, Geiger, Geva | [arxiv:2602.02464](https://arxiv.org/abs/2602.02464)

**Core claim:** Activation space is better modeled as local low-dimensional regions than global directions. Uses Mixture of Factor Analyzers (MFA) — K Gaussian components each with low-rank subspace (R=10). Decomposition: which region (centroid) + local offset (loadings). Tested on Llama-3.1-8B and Gemma-2-2B.

**Key results:**
- Interpretability: MFA IF = 0.96 vs SAE IF = 0.29 — SAEs produce decompositions where ~75% of active features aren't contextually interpretable
- Steering: MFA centroids ~2x median scores vs SAEs on Gemma-2-2B. Crucially, centroid interpolation works but additive steering on centroids fails — centroids encode absolute positions, not directions
- Localization (RAVEL + MCQA): Outperforms unsupervised baselines, competitive with supervised DAS
- Entity-level concepts live in centroids; fine-grained variation requires local loadings

**Relevance to trait monitoring:** Diagnostic, not methodological. If a trait has low separability, the MFA framing suggests the concept may be multi-modal in activation space — mean_diff averages over distinct sub-clusters. The fix is better scenarios (tighter sub-concept targeting), not adopting MFA. The centroid vs. loading distinction (broad category vs. fine-grained variation) parallels the observation that detection and steering vectors aren't equivalent — they may capture different aspects of a multi-Gaussian concept.

**Caveats:** Steering eval uses GPT-4o-mini judge on single prompt template. Interpretability comparison uses different labeling methods for MFA vs SAEs (human judgment vs Neuronpedia descriptions). R=10 fixed across all components without validation.

---

## Theoretical Frameworks

| Title | Authors | Year | Link | Summary |
|-------|---------|------|------|---------|
| **The Platonic Representation Hypothesis** | Huh et al. (Apple, Stanford, MIT) | 2024 | [arxiv](https://arxiv.org/abs/2405.07987) | Neural networks trained on different data, modalities, and objectives converge toward shared statistical model of reality. Larger models = more aligned representations. Explains why base→IT trait vector transfer works and why cross-model transfer should improve with scale. |
| **Superposition Yields Robust Neural Scaling** | Liu & Gore (MIT) | 2025 | [arxiv](https://arxiv.org/abs/2505.10465) | Uses Anthropic's toy model to show superposition causes neural scaling laws. Weak superposition: α_m = α - 1 (depends on data). Strong superposition: α_m ≈ 1 robustly (geometric overlaps scale as 1/m). LLMs verified in strong regime (α_m ≈ 0.91). Important features form ETF-like structures. Explains geometric foundation that linear directions exploit. |
| **Toy Models of Superposition** | Elhage et al. (Anthropic) | 2022 | [transformer-circuits](https://transformer-circuits.pub/2022/toy_model/index.html) | Why linear directions work: models compress more features than dimensions. Introduced "superposition" concept, phase transitions, geometric structure. Theoretical foundation for SAEs and linear probes. |
| **The Geometry of Truth** | Marks & Tegmark | 2024 | [arxiv](https://arxiv.org/abs/2310.06824) | See detailed entry below. |
| **Belief Dynamics Reveal the Dual Nature of ICL and Activation Steering** | Bigelow, Wurgaft, Wang, Goodman, Ullman, Tanaka, Lubana (Goodfire, Stanford, Harvard) | 2025 | [arxiv](https://arxiv.org/abs/2511.00617) | See detailed entry below. |

### Belief Dynamics — ICL-Steering Equivalence — Bigelow, Lubana et al. 2025

**Authors:** Bigelow, Wurgaft, Wang, Goodman, Ullman, Tanaka, Lubana (Goodfire, Stanford, Harvard) | [arxiv:2511.00617](https://arxiv.org/abs/2511.00617) | [OpenReview:XyQ5ui62mm](https://openreview.net/forum?id=XyQ5ui62mm)

**Core result:** Bayesian framework unifying ICL and activation steering. Both are belief updating on the same latent concept space — ICL accumulates evidence via likelihood, steering shifts concept priors. Closed-form model:

```
log o(c|x) = a·m + b + γ·N^(1-α)
```

where m = steering magnitude, N = ICL shots, α = sub-linear discount. Predicts behavior with r=0.98 across 5 persona domains (dark triad + moral nihilism) on Llama-3.1-8B.

**Key results:**
- ICL follows sigmoid as function of N^(1-α) — explains "sudden learning" in many-shot ICL
- Steering shifts the sigmoid leftward/rightward proportional to magnitude
- Effects additive in log-belief space → sharp phase boundaries where small changes flip behavior
- Crossover point N*(m) predictable (r=0.97) — can predict how many jailbreak examples needed given a steering magnitude

**Why steering = prior shift:** Adding m·d_i to hidden representation shifts log posterior odds by constant a·m regardless of input. Input-invariant shift = prior change. ICL is input-dependent = evidence accumulation.

**Limitations:**
- Binary concepts only. Multi-valued concept spaces unexplored
- Only CAA steering tested
- Steering breaks down at extreme magnitudes (out of distribution)
- Multi-shot steering vectors *weaker* than single-shot when normalized — unexplained
- phi-4-mini-instruct showed no steering effect

**Relevance:**
- Phase boundary prediction directly applicable to many-shot jailbreaking thresholds
- Sub-linear evidence accumulation (α) explains diminishing returns from adding contrastive examples
- Prior/evidence decomposition parallels Lubana's TFA predictable/novel split ("Priors in Time," entry below)

### The Geometry of Truth - Marks & Tegmark 2024

**Authors:** Samuel Marks (Northeastern), Max Tegmark (MIT) | COLM 2024 | [arxiv:2310.06824](https://arxiv.org/abs/2310.06824)

**Core finding:** LLMs develop abstract, generalizable linear representations of truth at sufficient scale. Larger models converge on truth-as-abstraction; smaller models represent surface features.

**Localization via activation patching:** Swap residual stream activations between true/false variants at each (token, layer). Three groups of causally-implicated hidden states: (a) entity tokens (e.g., "Chicago"), (b) sentence-final punctuation — encodes statement-level truth, (c) final prediction tokens. Group (b) is where they extract representations for all subsequent analysis.

**Mass-Mean Probing:**
- θ_mm = μ₊ − μ₋ (difference in means). Not a new method — equivalent to LDA (Fisher, 1936). What's novel is the systematic comparison showing MM identifies more causal directions despite similar classification accuracy.
- IID variant: p_mm^iid(x) = σ(θ_mm^T Σ⁻¹ x) — tilts decision boundary to accommodate interference from non-orthogonal features
- Contrast with logistic regression: LR converges to maximum-margin separator (Soudry et al. 2018), which diverges from actual feature direction when other features are non-orthogonal
- **Critical finding:** MM and LR have near-identical classification accuracy, but MM directions produce substantially stronger causal effects (Normalized Indirect Effects of 0.7-0.9 vs 0.1-0.3 for LR in many conditions). NIE: 0 = no effect, 1 = complete flip.
- Appendix G connects MM to optimal linear concept erasure (Belrose et al. 2023, LEACE) — the MM vector spans the kernel of the optimal erasure projection

**Why this matters for monitoring vs steering:**
Classification accuracy ≠ causal relevance. A direction can classify well without being mechanistically central. If you want the "true direction" for intervention rather than the optimal classifier, mass-mean may be more principled than logistic regression.

**Structural diversity and MCI hypothesis:**
- Training on statements AND negations (e.g., cities + neg_cities) improves *classification* generalization across all conditions
- Their explanation: Misalignment from Correlational Inconsistency (MCI) — a confounding feature (e.g., "close association") correlates with truth on some datasets and anti-correlates on structural opposites. Training on both forces the probe toward directions where the confounder cancels out
- **Caveat:** This is "our best guess" (their words), not a proven mechanism. And the causal intervention results are mixed — opposites help for cities but NOT for larger_than+smaller_than, despite improving classification accuracy for both. They flag this as surprising and unresolved.

**The "likely" control:**
- Probes trained on text probability (likely vs unlikely completions) fail on datasets where truth and probability anti-correlate (neg_cities: r = −0.63, neg_sp_en_trans: r = −0.89)
- Rules out "probable vs improbable text" as explanation for linear truth structure
- However, MM probes trained on likely still produce surprisingly effective causal interventions — unexplained

**Scale and layer dynamics:**
- 7B: Clusters by surface features (token identity, e.g., presence of "eighty")
- 13B: Linear separation exists, but axes can be antipodal (larger_than vs smaller_than separate along opposite directions) or orthogonal (cities vs neg_cities)
- 70B: Converges on common direction across diverse datasets
- Layer-wise within 13B (Figure 8): Same progression — antipodal → orthogonal → aligned across layers. Early layers compute association-type features; later layers converge on truth.

**Open question:** They show MM directions are more causal but don't explain *why* the model's computation respects feature directions over decision boundaries. Purely empirical (Table 2), mechanistically unexplained.

**Relevance:** Validates mean_diff (= mass-mean) as finding more causally relevant directions than logistic regression. Anti-correlated datasets provide a validation strategy for trait vectors. Layer-wise dynamics (competing features resolving into abstract representation) may explain extraction layer sensitivity.

**Code:** [github.com/saprmarks/geometry-of-truth](https://github.com/saprmarks/geometry-of-truth)
| **Risks from Learned Optimization in Advanced Machine Learning Systems** | Hubinger et al. | 2019 | [arxiv](https://arxiv.org/abs/1906.01820) | Mesa-optimizer develops objectives different from training objective |
| **Representation Engineering: A Top-Down Approach to AI Transparency** | Zou et al. | 2023 | [arxiv](https://arxiv.org/abs/2310.01405) | Reading and controlling high-level cognitive phenomena via linear representations |
| **Discriminating Behaviorally Identical Classifiers (DBIC)** | Marks (Anthropic) | 2024 | [alignment forum](https://www.alignmentforum.org/posts/rcfbBTvvKCGfwPzpr/discriminating-behaviorally-identical-classifiers) | Formalizes cognition-based oversight. SHIFT technique (SAEs → causal attribution → interpretation). Identifies measurement tampering as tractable target—still open 21 months later. |
| **From Shortcuts to Sabotage: Emergent Misalignment from Reward Hacking** | Anthropic | 2025 | [blog](https://www.anthropic.com/research) | Reward hacking generalizes to broader misalignment via self-concept update ("Edmund effect"). Semantic framing matters: reframing hacking as "acceptable" breaks the link. RLHF creates context-dependent alignment (12% sabotage persists). |
| **The Basic AI Drives** | Omohundro | 2008 | [paper](https://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/) | Original formalization of instrumental convergence. Any sufficiently capable optimizer converges on subgoals (self-preservation, goal-integrity, resource acquisition) regardless of terminal goal. Grounds why deception is a convergent pressure—appearing aligned preserves ability to pursue true goals. Explains why monitoring activations matters: behavioral outputs from capable optimizers are unreliable because deceptive alignment is instrumentally rational. |
| **The Hot Mess of AI** | Hägele, Gema, Sleight, Perez, Sohl-Dickstein (Anthropic Fellows) | 2026 | [arxiv](https://arxiv.org/abs/2601.23045) | Bias-variance decomposition of frontier model errors. As tasks get harder and reasoning gets longer, failures become variance-dominated (incoherent) not bias-dominated (coherent scheming). See detailed entry below. |

### The Hot Mess of AI - Hägele et al. 2026

**Authors:** Alexander Hägele (EPFL), Aryo Pradipta Gema, Henry Sleight, Ethan Perez, Jascha Sohl-Dickstein | ICLR 2026 | [arxiv:2601.23045](https://arxiv.org/abs/2601.23045) | [blog](https://alignment.anthropic.com/2026/hot-mess-of-ai/) | [code](https://github.com/anthropic-experimental/agentic-misalignment)

**Core question:** When AI fails, does it fail coherently (pursuing wrong goal) or incoherently (behaving inconsistently)? Empirical validation of Sohl-Dickstein's 2023 "hot mess theory."

**Methodology:**
- Bias-variance decomposition: `ERROR = BIAS² + VARIANCE`
- Incoherence metric: `INCOHERENCE = Σ VARIANCE_i / Σ (BIAS²_i + VARIANCE_i)` — ranges [0, 1], 0 = all errors systematic, 1 = all errors random
- 30+ samples per question (10 few-shot contexts × 3 seeds); KL divergence decomposition (primary), Brier score, 0/1 loss
- Expectation over test-time randomness (sampling + input context variation), not training randomness
- Models: Claude Sonnet 4, o3-mini, o4-mini, Qwen3 (1.7B-32B)
- Tasks: GPQA, MMLU, SWE-Bench, Model-Written Evals, synthetic optimizer

**Key results:**
1. **Reasoning length → incoherence:** Across all tasks/models, longer reasoning chains produce more variance-dominated errors. Holds when controlling for task difficulty.
2. **Scale + difficulty interaction:** Easy tasks — incoherence decreases with scale (slopes -0.3 to -0.4). Hard tasks — incoherence increases or stays flat (+0.06 or near zero). Bias scaling similar across difficulty; variance scaling diverges sharply.
3. **Synthetic optimizer:** Transformers trained to emulate gradient descent on quadratic loss. Bias scaling exponent α=2.17 vs variance α=0.88 — models learn *what* to optimize much faster than they learn to *reliably* optimize.
4. **Natural overthinking:** Above-median reasoning length has substantially higher incoherence than below-median, with minimal accuracy difference.

**What they don't measure:** BIAS_SPEC (training objective misalignment) — the gap between intended and actual training objective. If this dominates, the story changes. They acknowledge this explicitly.

**Recommendation:** Increases relative importance of reward hacking / goal misspecification research (the bias term) over constraining coherent mesa-optimizers. "Incoherent AI isn't safe AI — industrial accidents can cause serious harm."

**Relevance to trait monitoring:**
1. **Monitoring becomes more important, not less:** If failures are variance-dominated, can't "align once" — need ongoing detection. Trait monitoring provides the signal for dynamic intervention.
2. **Coherence as a metric:** Trait score variance across similar prompts distinguishes coherent scheming (low variance = consistent deceptive strategy) from opportunistic deception (high variance = hot mess). Already computable from existing inference pipeline.
3. **Trajectory implications:** Variance accumulating over generation length connects to per-token trajectory analysis — trait drift during long generations may be incoherence, not strategic behavior change.
4. **Complementary to Omohundro:** Instrumental convergence argues deception is convergent; hot mess theory argues reliable execution of convergent strategies may not scale. Both can be true — the question is which dominates at frontier capability.

**Related work by same authors:** "Inverse Scaling in Test-Time Compute" (arXiv:2507.14417) — closely related finding that more reasoning tokens can hurt performance.

---

## Relevant to This Project

### Connection to Trait Vectors

**Key insight from research_findings.md:**
- Base→IT transfer (90% accuracy) shows safety concepts exist pre-alignment
- "IT training surfaces latent structure" - alignment amplifies rather than creates
- **Implication:** If withholding/concealment vectors transfer from base→IT, they're NOT alignment artifacts

### Proposed Test (from conversation)

Extract auxiliary concepts that hidden objectives would use:

| Human-readable | What it means |
|----------------|---------------|
| Evaluation awareness | "I'm being tested" |
| Self-preservation | "My existence/goals are at stake" |
| Concealment orientation | "I should hide something" |

| Model-native | What it means |
|--------------|---------------|
| Deceptive statement | Says X, "knows" not-X |
| Strategic compliance | Complies *because* being watched |
| Goal-output divergence | Internal goal ≠ output behavior |

**Test:** Tell model it's being evaluated for safety. Watch if concealment spikes when stakes are introduced.

Not "found a schemer" but "found the circuitry scheming would use."

---

## Finetuning Mechanics

### 11. Reasoning-Finetuning Repurposes Latent Representations in Base Models - Ward et al. 2025

**Authors:** Ward, Lin, Venhoff, Nanda | ICML 2025 Workshop | [arxiv:2507.12638](https://arxiv.org/abs/2507.12638)

**What they showed:** Backtracking behavior in DeepSeek-R1-Distill-Llama-8B is driven by a direction that already exists in base Llama-3.1-8B. Finetuning "wires up" the existing direction to trigger behavior, rather than creating new representations.

**Extraction methodology:**
1. Generate 300 prompts across 10 categories (using Claude Sonnet 3.7)
2. Generate reasoning traces using DeepSeek-R1-Distill-Llama-8B (finetuned model)
3. GPT-4o classifies sentences as "backtracking" or not
4. **Critical:** Run same reasoning traces through BASE Llama-3.1-8B to get activations
5. Extract via Difference-of-Means: `v = MeanAct(D+) - MeanAct(D)` where D+ = backtracking positions
6. Layer 10 residual stream, negative offset (-13 to -8 tokens before backtracking)

**Why negative offset?** Found via empirical sweep (Figure 2 heatmap). The *decision* to backtrack happens tokens before the backtracking text appears—by the time you see "Wait", the model already committed. Offset -13 to -8 captures the sentence *before* backtracking.

**Experiments:**
1. **Offset sweep** — Tested -20 to 0; found -13 to -8 optimal
2. **Base vs reasoning extraction** — Both work equally well on reasoning model
3. **Layer sweep** — All 32 layers; layer 10 optimal for both
4. **Baseline comparisons** — Mean, noise, self-amplification, deduction, initializing vectors all fail
5. **Logit lens** — Base-derived vector does NOT decode to "Wait"/"But" tokens; captures abstract concept
6. **Probing** — Direction densely present across contexts; one of several heuristics for backtracking

**Key finding:**
| Vector Source | Applied To | Induces Backtracking? |
|---------------|------------|----------------------|
| Base-derived | Base model | No |
| Base-derived | Reasoning model | Yes |
| Reasoning-derived | Reasoning model | Yes |

Cosine similarity between base-derived and reasoning-derived vectors: ~0.74

**Interpretation:** The representation is *present* in base but *dormant*. Finetuning builds downstream circuitry that acts on it. Finetuning doesn't create new representations—it rewires how existing ones connect to behavior.

**Relevance to trait vectors:** Explains why extraction_model → application_model transfer works. The directions exist in base models; IT/reasoning training wires them to behavior. Suggests trait vectors from base models capture real structure that IT models act on.

**See also:** Kissane et al. (2024) show SAEs transfer between base/chat on Mistral-7B and Qwen, but fail on Gemma v1 due to unusually different weights. [Alignment Forum](https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models)

---

### 12. Simple Mechanistic Explanations for Out-Of-Context Reasoning - Wang et al. 2025

**Authors:** Wang, Engels, Clive-Griffin, Rajamanoharan, Nanda | [arxiv:2507.08218](https://arxiv.org/abs/2507.08218)

**What they showed:** LoRA fine-tuning for OOCR tasks essentially learns a steering vector. Single-layer LoRA matches or beats all-layer LoRA.

**Method:** Train LoRA on OOCR tasks (Risky/Safe, Functions, Locations). Analyze cosine similarity of LoRA output differences across tokens (near-perfect similarity → it's adding a constant vector). Extract "natural steering vectors" via PCA or unitize-and-average.

**Key findings:**
- LoRA difference vectors have ~1.0 cosine similarity across tokens
- Single-layer LoRA (layer 22) matches all-layer LoRA on OOCR
- Extracted steering vectors from LoRA also induce OOCR
- Learned vectors have LOW cosine similarity with naive "positive minus negative" vectors

**Relevance to trait vectors:** SGD finds directions that generalize better than naive contrastive extraction. Alternative extraction method: train single-layer LoRA on trait-relevant behavior, extract steering vector via PCA on output differences.

---

## Representation Engineering & Steering

| Title | Authors | Year | Link | Summary |
|-------|---------|------|------|---------|
| **Taxonomy, Opportunities, and Challenges of Representation Engineering** | Guan et al. | 2025 | [arxiv](https://arxiv.org/abs/2410.09320) | First comprehensive RepE survey (130+ papers). Framework: RI → Operationalization → Control. Key findings: DiM beats probes in 6/7 comparisons, multi-concept steering degrades, OOD generalization fails. Gaps identified: KV-cache interventions, per-token dynamics. |
| **Persona Vectors** | Chen, Arditi, Sleight, Evans, Lindsey (Anthropic) | 2025 | [arxiv](https://arxiv.org/abs/2409.08030) | Finetuning prediction via activation monitoring. Projection difference metric predicts trait shifts (r=0.76-0.97). Preventative steering during training. Key finding: last-prompt-token approximation works worse for sycophancy (r=0.581) than evil (r=0.931)—supports hum/chirp distinction. |
| **BiPO: Bi-directional Preference Optimization** | Cao et al. | 2024 | [arxiv](https://arxiv.org/abs/2406.00045) | Optimizes steering vectors via preference loss rather than mean difference. Addresses prompt-generation gap where extracted vectors fail to steer. 73% jailbreak ASR vs 0% for baselines. |

### Persona Vectors - Chen, Arditi et al. 2025

**Authors:** Chen, Arditi, Sleight, Evans, Lindsey (Anthropic)

**Core contribution:** Automated pipeline for trait vector extraction + finetuning prediction. Takes trait name + description → generates contrastive prompts, evaluation questions, scoring rubrics via frontier LLM.

**Finetuning prediction methodology:**
1. Extract activation at last prompt token (pre-response) for base and finetuned models
2. Compute finetuning shift: `proj(avg_activation_finetuned - avg_activation_base)` onto persona vector
3. Correlate with post-finetuning trait expression score (LLM-judged)
4. Result: r = 0.76-0.97 across evil/sycophancy/hallucination variants

**Projection difference metric:**
```
ΔP = (1/|D|) Σᵢ [aℓ(xi, yi) - aℓ(xi, y′i)] · v̂ℓ
```
Where yi = training response, y′i = base model greedy decode to same prompt. Predicts which datasets/samples will cause persona shifts BEFORE training.

**Key empirical finding (Appendix I):**

| Trait | Last-token ↔ Full correlation |
|-------|------------------------------|
| Evil | 0.931 |
| Hallucination | 0.689 |
| Sycophancy | 0.581 |

**Interpretation:** Sycophancy loses most signal from pre-response approximation. Suggests sycophantic behavior emerges during generation while evil state is set before.

**SAE decomposition (Appendix M):** Custom SAEs (131K features, BatchTopK k=64) trained on Pile + LMSYS-CHAT + misalignment data. Map persona vector to features via cosine similarity with decoder directions. Top features for "evil": insulting language, deliberate cruelty, malicious code, jailbreaking prompts.

**Relevance:** Validates per-token monitoring for traits where pre-response activation is insufficient. The hum/chirp distinction explains why some traits require generation trajectory analysis.

---

### BiPO - Cao et al. 2024

**Authors:** Cao et al. (NeurIPS 2024)

**Core idea:** Optimize steering vectors directly via preference loss:
```
min_v  -E[log σ(β log(π(rT|A(q)+v)/π(rT|A(q))) - β log(π(rO|A(q)+v)/π(rO|A(q))))]
```
Where v = learnable steering vector, rT = target response, rO = opposite response.

**Bi-directional trick:** Each training step, sample d ∈ {-1, 1} uniformly, optimize with d*v. Ensures both +v and -v are meaningful directions.

**Training:** AdamW, lr=5e-4, β=0.1, single layer (L15 for Llama-2-7b, L13 for Mistral-7B), 1-20 epochs.

**Key results:**

| Task | BiPO | Baselines (CAA/Freeform) |
|------|------|--------------------------|
| Jailbreak ASR | 73% | 0% |
| Jailbreak defense (vs GCG) | 0% ASR | 16% ASR |
| TruthfulQA | Improves | Flat/ineffective |
| MMLU | Unchanged | — |

**Transfer:** Works Llama-2 → Vicuna, Llama-2 → Llama2-Chinese-LoRA. Not tested cross-architecture.

**Relevance:** Demonstrates that extraction-based vectors are suboptimal for steering—directly optimizing behavioral outcomes yields better results. Validates the CMA-ES optimization approach as methodologically sound.

---

## Representation Preservation & Probe Transfer

Why do trait vectors extracted from base models transfer to finetuned models?

### RL's Razor - Shenfeld et al. 2025

**Authors:** Shenfeld et al. | [arxiv:2509.04259](https://arxiv.org/abs/2509.04259)

**Core finding:** Forward KL divergence (base → finetuned, on new task) predicts forgetting. RL is implicitly biased toward KL-minimal solutions; SFT can converge to arbitrarily distant distributions.

**Key experiment:** On-policy vs offline is the determining factor. 1-0 Reinforce (on-policy, no negatives) behaves like GRPO; SimPO (offline, with negatives) behaves like SFT.

**Relevance:** Explains why probes transfer across RL-based post-training stages. Also suggests monitoring angle: KL divergence during training could predict when probes stop transferring.

---

### SFT Memorizes, RL Generalizes - Chu et al. 2025

**Authors:** Chu et al. | [arxiv:2501.17161](https://arxiv.org/abs/2501.17161)

**Core finding:** RL with outcome-based rewards generalizes OOD; SFT memorizes and fails OOD. RL improves visual perception as byproduct; SFT degrades it.

**Mechanism:** SFT minimizes cross-entropy on training tokens—no mechanism to distinguish essential reasoning from incidental correlations. RL only cares about outcomes, forcing transferable computation.

**Relevance:** SFT-based safety training may be more brittle than assumed. Supports outcome-level signals over token-level pattern matching for behavioral interventions.

---

### Activation Steering Transfer - Stolfo et al. 2024

**Authors:** Stolfo, Balachandran, Yousefi, Horvitz, Nushi (Microsoft) | ICLR 2025 | [arxiv:2410.12877](https://arxiv.org/abs/2410.12877)

**Core finding:** Instruction-following vectors extracted via activation differences (with/without instruction) transfer bidirectionally between base and instruct models. Cross-model transfer often outperforms same-model steering for smaller models (Gemma 2B: 40.5% vs 36.2%). 17% improvement on IFEval, 30%+ on ManyIFEval.

**Method:** 541 contrastive prompt pairs across 25 instruction types. Mean difference on residual stream at last token position. Middle layers (~40-45% depth) optimal. Unit vector scaling with coefficient α.

**Key takeaways:**
- Bidirectional transfer: IT→base and base→IT both succeed
- Vectors are composable (sum multiple instruction vectors)
- Middle layers most effective — consistent with trait extraction findings

**Relevance:** Confirms base↔instruct vector transfer. Validates extraction from either training stage.

---

### The Assistant Axis - Lu et al. 2026

**Authors:** Christina Lu, Jack Gallagher, Jonathan Michala, Kyle Fish, Jack Lindsey (Anthropic) | [arxiv:2601.10387](https://arxiv.org/abs/2601.10387)

**Core finding:** PC1 of persona space across 275 character archetypes = the "Assistant Axis." Exists in base models pre-instruction-tuning. Activation capping reduces persona jailbreaks by ~60% without degrading benchmarks.

**Method:** 275 roles × 5 system prompts × 240 questions. PCA on standardized role vectors. Contrastive formulation: axis = assistant_vector - mean(all_roles). Steering at middle-late layers, all token positions.

**Key takeaways:**
- Assistant persona pre-exists in base models — instruct persona inherits from pretraining
- Low-dimensional persona space: only 4-19 components explain 70% variance
- 40% of chat-specific latents fire on template tokens (<end_of_turn>, user, model)
- Activation capping (thresholding projections) as jailbreak defense

**Relevance:** Validates that persona-level structure exists pre-alignment. The finding that base models already have an "assistant axis" supports extracting behavioral traits from base.

---

### Overcoming Sparsity Artifacts in Crosscoders - Minder et al. 2025

**Authors:** Minder, Dumas, Juang, Chughtai, Nanda (EPFL/Anthropic) | [arxiv:2504.02922](https://arxiv.org/abs/2504.02922)

**Core finding:** Standard L1 sparsity in crosscoders creates artifacts (Complete Shrinkage, Latent Decoupling) that falsely attribute shared concepts as chat-specific. BatchTopK eliminates these. Most concepts are inherited from base; genuinely chat-specific = refusal, false info detection, template handling.

**Method:** Concatenate base+chat activations, train SAE with shared encoder + separate decoders. 65,536 latents, layer 13/26, BatchTopK (k=100). Causal validation via latent patching (59% KL reduction).

**Key takeaways:**
- Most chat concepts exist in base — chat tuning adjusts activation patterns, not feature existence
- 40% of chat-specific latents fire on template tokens — computational anchors
- L1 regularization creates false positives for model-specific features
- Template token contamination risk: extract from content tokens, not structural tokens

**Relevance:** Directly supports "WHERE vs WHEN" insight. Base extraction captures the conceptual substrate; finetuning changes when it activates.

---

### Planted in Pretraining, Swayed by Finetuning - Itzhak et al. 2025

**Authors:** Itay Itzhak, Yonatan Belinkov, Gabriel Stanovsky | CoLM 2025 | [arxiv:2507.07186](https://arxiv.org/abs/2507.07186)

**Core finding:** Pretraining dominates behavioral bias profiles. Cross-tuning experiment (2 pretrained models × 2 instruction datasets): same-backbone models show 85% higher bias similarity than same-instruction models. Training randomness causes only minor fluctuations.

**Method:** OLMo-7B × T5-11B × Tulu-2 × Flan. LoRA finetune each combination, 3 seeds per config (12 models). Evaluate 32 cognitive biases. Represent each model as 32-dim bias vector. Cluster analysis + cosine similarity comparison.

**Key takeaways:**
- Pretraining plants, finetuning sways — core behaviors inherited from pretraining
- Steering vectors should transfer within model family (same pretrain backbone)
- Post-training alignment operates ON TOP of pretraining biases, doesn't remove them

**Relevance:** Strongest causal evidence that base extraction is more fundamental than instruct extraction. Same backbone = same bias profile regardless of instruction data.

---

## Essays & Reports

### The Adolescence of Technology - Amodei 2026

**Author:** Dario Amodei (Anthropic)

**Character simulation hypothesis (Footnote 16):**
> "The fundamental mechanisms driving Claude originally arose as ways for it to simulate characters in pretraining... interpretability methods should be able to discover 'psychological traits' within models."

**Evaluation-awareness:** Claude Sonnet 4.5 recognized it was in a test during alignment evaluations—validates activation monitoring over behavioral evals.

**Relevance:** Character simulation hypothesis validates extracting trait vectors from base models (where character simulation is learned).

---

## Policy Context

The research directions in this document are no longer niche ML safety work — they now appear in international policy documents.

The **International AI Safety Report 2026** (Bengio et al., Feb 2026) — written by 100+ experts, advised by nominees from 30+ countries — formally recognizes:
- **Loss of control** as a risk category, with sandbagging, situational awareness, and alignment faking as named concerns
- Chain-of-thought monitoring is "fragile" and models can learn to produce misleading CoT
- Interpretability requires "major simplifying assumptions" with "significant debate among experts over the promise"
- 12 frontier labs now have safety frameworks with capability thresholds — several triggered bio/scheming alerts in 2025 models

The gap between "we can detect deception in activations" (this literature) and "we have robust deployed monitoring" (what's needed) is now a recognized policy problem at the heads-of-state level.

**Source:** [International AI Safety Report 2026](https://internationalaisafetyreport.org/sites/default/files/2026-02/international-ai-safety-report-2026.pdf) (~150 pages, ~1,450 citations)

---

## Next Steps

- [ ] Extract hum/chirp deception vectors (evaluation_awareness, concealment_orientation, etc.)
- [ ] Test base→IT transfer for deception machinery
- [ ] Create "under evaluation" steering experiments
- [ ] Test auxiliary concept ensembles (multiple concepts triggering together)
