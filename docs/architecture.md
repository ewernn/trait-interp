# Repository Architecture

## Design Principles

1. **traitlens** = General purpose building blocks (minimal library)
2. **utils** = Universal utilities (path management)
3. **extraction** = Vector creation pipeline (training time)
4. **experiments** = Data + custom analysis (user space)

---

## Directory Structure

```
trait-interp/
│
├── traitlens/              # Minimal library - building blocks only
│   ├── hooks.py           # Hook registration
│   ├── activations.py     # Activation capture
│   ├── compute.py         # Math primitives (mean, derivatives, projection)
│   └── methods.py         # Extraction methods (mean_diff, probe, ICA, gradient)
│
├── config/                 # Configuration
│   └── paths.yaml         # Single source of truth for all paths
│
├── utils/                  # Universal utilities
│   └── paths.py           # Python PathBuilder (loads from config/paths.yaml)
│
├── extraction/             # Vector creation pipeline (training time)
│   ├── 1_generate_responses.py   # Generate responses from natural scenarios
│   ├── 2_extract_activations.py  # Extract activations from responses
│   ├── 3_extract_vectors.py      # Create trait vectors
│   └── scenarios/        # Natural contrasting prompts
│
├── inference/              # Per-token monitoring (inference time)
│   ├── capture_layers.py  # Unified layer capture
│   └── monitor_dynamics.py # Dynamics analysis
│
├── experiments/            # Experiment data + custom analysis
│   └── {experiment_name}/  # Your experiment directory
│       ├── extraction/     # Training-time data (per-trait)
│       │   └── {category}/{trait}/
│       │       ├── responses/    # pos.json, neg.json
│       │       ├── activations/  # pos_layer{N}.pt, neg_layer{N}.pt
│       │       └── vectors/      # {method}_layer{N}.pt
│       ├── inference/      # Evaluation-time monitoring
│       │   ├── raw/               # Trait-independent activations
│       │   │   ├── residual/{prompt_set}/  # All-layer activations
│       │   │   └── internals/{prompt_set}/ # Single-layer detailed
│       │   └── {category}/{trait}/
│       │       ├── residual_stream/{prompt_set}/  # Projection JSONs
│       │       └── layer_internals/{prompt_set}/  # Detailed layer JSONs
│       └── validation/     # Evaluation-time quality testing
│           └── data_index.json    # Cross-distribution data index
│
└── docs/                   # Documentation
```

---

## Three-Phase Experiment Structure

Each experiment follows a three-phase structure separating training, monitoring, and validation:

### Phase 1: extraction/
**Purpose:** Training-time data (per-trait, unique prompts)
- Natural scenario files
- Generated contrastive responses
- Extracted activations
- Computed trait vectors

**Organized by:** `extraction/{category}/{trait}/`
- Requires category/trait format (e.g., `behavioral/refusal`)
- Each trait is self-contained

### Phase 2: inference/
**Purpose:** Evaluation-time monitoring (shared prompts, all traits)
- Standardized prompt sets
- Raw activations captured once
- Projections to all trait vectors
- Dynamics analysis results

**Organized by:** Experiment-level (shared across traits)
- Captures activations once per prompt set
- Projects to all available trait vectors
- Avoids per-trait duplication

### Phase 3: validation/
**Purpose:** Evaluation-time quality testing
- Cross-distribution test results
- Data availability indexes
- Best accuracy/layer information

**Organized by:** Experiment-level
- Results from cross-distribution analysis
- Generated by validation evaluation scripts

---

## What Goes Where

### traitlens/ - Minimal Library

**What belongs:**
- Core primitives (hooks, capture, math operations)
- General-purpose functions that work on any model
- No dependencies on specific models or tasks

**What does NOT belong:**
- Model-specific code
- Experiment-specific analysis
- Visualization code
- Data loading/saving (except primitive capture)

**Example:**
```python
# ✅ YES - general primitive
def compute_derivative(trajectory): ...

# ❌ NO - too specific
def analyze_refusal_commitment_on_gemma(): ...
```

### utils/ - Universal Utilities

**What belongs:**
- Path management (loads from config/paths.yaml)
- Functions needed across all modules

**What does NOT belong:**
- Model-specific utilities
- Analysis code
- Primitives (those go in traitlens)

### extraction/ - Vector Creation

**What belongs:**
- Training-time pipeline
- Natural scenario generation
- Activation extraction from training data
- Vector extraction methods

**What does NOT belong:**
- Inference-time monitoring
- Analysis of existing vectors
- Visualization

### experiments/ - User Space

**What belongs:**
- Experimental data (responses, activations, vectors)
- Custom analysis scripts
- Experiment-specific monitoring code
- Results and visualizations

**What does NOT belong:**
- Reusable library code (put in traitlens)
- Pipeline code (put in extraction)

---

## Information Flow

### Training Time (Extraction)
```
1. Create natural scenario files in experiments/{experiment_name}/extraction/{category}/{trait}/
   → positive.txt
   → negative.txt
   ↓
2. extraction/1_generate_responses.py
   → Generate responses (no instructions)
   → Save to experiments/{name}/extraction/{category}/{trait}/responses/
   ↓
3. extraction/2_extract_activations.py
   → Load responses
   → Extract activations using traitlens
   → Save to experiments/{name}/extraction/{category}/{trait}/activations/
   ↓
4. extraction/3_extract_vectors.py
   → Load activations
   → Extract vectors using traitlens.methods
   → Save to experiments/{name}/extraction/{category}/{trait}/vectors/
```

### Inference Time (Monitoring)
```
1. Load vectors from experiments/{name}/extraction/{category}/{trait}/vectors/
   ↓
2. Monitor during generation using traitlens
   → Capture activations per token
   → Project onto trait vectors
   ↓
3. Analyze dynamics (optional)
   → Use traitlens.compute primitives
   → Or custom analysis
   ↓
4. Save results to experiments/{name}/inference/
```

---

## Adding New Code - Decision Tree

### I want to add a new function...

**Q: Does it work on any model/task?**
- YES → Consider `traitlens/`
- NO → Put in `experiments/{name}/`

**Q: Is it a mathematical primitive?**
- YES → `traitlens/compute.py`
- NO → Continue...

**Q: Is it part of the extraction pipeline?**
- YES → `extraction/`
- NO → Continue...

**Q: Is it a universal utility?**
- YES → `utils/`
- NO → `experiments/{name}/`

---

## Examples

### ✅ Correct Placement

```python
# traitlens/compute.py - general math primitive
def compute_derivative(trajectory):
    return torch.diff(trajectory, dim=0)

# utils/paths.py - universal utility
def get(key, **variables):
    return Path(template.format(**variables))

# extraction/1_generate_responses.py - extraction pipeline
def generate_responses(experiment, trait, model_name):
    return pos_responses, neg_responses

# experiments/{experiment_name}/analysis/monitor.py - custom analysis
def analyze_commitment_on_cognitive_traits():
    # Load specific vectors
    # Run specific analysis
    # Save specific results
```

### ❌ Incorrect Placement

```python
# ❌ DON'T: traitlens/gemma_utils.py - too specific
def get_gemma_layer_16():
    return "model.layers.16"

# ❌ DON'T: utils/commitment_analysis.py - not universal
def find_commitment_for_{experiment_name}():
    # Too specific for utils/

# ❌ DON'T: extraction/monitor.py - wrong phase
def monitor_during_inference():
    # Extraction is training time only

# ❌ DON'T: traitlens/visualization.py - wrong scope
def plot_trajectory():
    # traitlens provides data, not viz
```

---

## File Size Guidelines

Keep files focused and reasonably sized:

- `traitlens/*.py` - Each file < 300 lines
- `utils/*.py` - Each file < 200 lines
- `extraction/*.py` - Each script < 500 lines
- `experiments/` - No limits (user space)

If a file grows too large, split by responsibility.

---

## Testing Strategy

### traitlens/
- Unit tests with synthetic data
- No model dependencies in tests
- Fast tests (<1 second)

### extraction/
- Integration tests with small models
- Test on sample data
- Can be slower

### experiments/
- Custom validation by user
- No required tests

---

## Dependencies

### traitlens/
- **Required**: PyTorch only
- **Optional**: scikit-learn (for ICA/Probe methods)
- **Never**: transformers, experiment-specific packages

### utils/
- **Allowed**: PyYAML (for paths.yaml)
- **Never**: experiment-specific packages

### extraction/
- **Allowed**: transformers, pandas, tqdm, fire
- **Never**: visualization packages

### experiments/
- **Allowed**: anything

---

## Clean Repo Checklist

- [ ] No circular dependencies
- [ ] Each directory has single responsibility
- [ ] traitlens/ is model-agnostic
- [ ] utils/ has no experiment code
- [ ] extraction/ has no inference code
- [ ] experiments/ has no reusable library code
- [ ] Clear separation of training vs inference
- [ ] New users can understand the structure

---

This architecture keeps the repo clean, maintainable, and easy to extend.
