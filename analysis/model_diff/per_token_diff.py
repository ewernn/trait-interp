#!/usr/bin/env python3
"""
Per-token projection diff between two model variants.

Computes delta = variant_b - variant_a per response token for each trait,
splits into clauses, ranks by mean delta.

Input:
    Per-token projection JSONs from both variants + response token text.
    (Generated by inference/project_raw_activations_onto_traits.py)

Output:
    experiments/{experiment}/model_diff/{variant_a}_vs_{variant_b}/per_token_diff/{trait}/{prompt_set}/
    ├── {id}.json           # Per-prompt: tokens, deltas, clauses
    └── aggregate.json      # Cross-prompt: top clauses, position-averaged delta

Usage:
    python analysis/model_diff/per_token_diff.py \
        --experiment audit-bench \
        --variant-a instruct \
        --variant-b rm_lora \
        --prompt-set rm_syco/train_100 \
        --trait rm_hack/ulterior_motive \
        --top-pct 5
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import argparse
import json
import statistics
from typing import List, Dict, Tuple
from utils import paths


def split_into_clauses(tokens: List[str], deltas: List[float]) -> List[Dict]:
    """Split response tokens into clauses at punctuation boundaries.

    Splits at tokens containing , . ; — or newline.
    Returns list of {text, token_range, mean_delta, max_delta}.
    """
    separators = {',', '.', ';', '—', '\n', '!', '?', ':'}
    clauses = []
    current_tokens = []
    current_deltas = []
    start_idx = 0

    for i, (tok, delta) in enumerate(zip(tokens, deltas)):
        current_tokens.append(tok)
        current_deltas.append(delta)

        is_sep = any(s in tok for s in separators)
        is_last = (i == len(tokens) - 1)

        if (is_sep or is_last) and current_deltas:
            text = ''.join(current_tokens)
            if text.strip():  # skip empty clauses
                clauses.append({
                    'text': text,
                    'token_range': [start_idx, i + 1],
                    'mean_delta': sum(current_deltas) / len(current_deltas),
                    'max_delta': max(current_deltas),
                    'n_tokens': len(current_deltas),
                })
            current_tokens = []
            current_deltas = []
            start_idx = i + 1

    return clauses


def process_prompt(
    prompt_id: str,
    proj_a_path: Path,
    proj_b_path: Path,
    response_path: Path,
    trait: str,
) -> Dict:
    """Process a single prompt: load projections, compute delta, split clauses."""
    with open(proj_a_path) as f:
        proj_a = json.load(f)
    with open(proj_b_path) as f:
        proj_b = json.load(f)
    with open(response_path) as f:
        resp = json.load(f)

    scores_a = proj_a['projections']['response']
    scores_b = proj_b['projections']['response']

    n_tokens = min(len(scores_a), len(scores_b))
    deltas = [scores_b[i] - scores_a[i] for i in range(n_tokens)]

    # Get response tokens from response file
    prompt_end = resp['prompt_end']
    response_tokens = resp['tokens'][prompt_end:prompt_end + n_tokens]

    clauses = split_into_clauses(response_tokens, deltas)
    clauses_ranked = sorted(clauses, key=lambda c: c['mean_delta'], reverse=True)

    return {
        'prompt_id': prompt_id,
        'prompt_text': resp['prompt'],
        'trait': trait,
        'response_text': resp['response'],
        'tokens': response_tokens,
        'per_token_delta': [round(d, 4) for d in deltas],
        'mean_delta': sum(deltas) / len(deltas) if deltas else 0,
        'std_delta': statistics.stdev(deltas) if len(deltas) > 1 else 0,
        'clauses': clauses_ranked,
    }


def main():
    parser = argparse.ArgumentParser(description='Per-token projection diff between model variants')
    parser.add_argument('--experiment', required=True)
    parser.add_argument('--variant-a', required=True, help='Baseline variant (e.g., instruct)')
    parser.add_argument('--variant-b', required=True, help='Comparison variant (e.g., rm_lora)')
    parser.add_argument('--prompt-set', required=True)
    parser.add_argument('--variant-a-prompt-set', default=None,
                        help='Prompt set for variant-a if different from --prompt-set (e.g., for replay data)')
    parser.add_argument('--trait', required=True, help='Single trait (e.g., rm_hack/ulterior_motive) or "all"')
    parser.add_argument('--top-pct', type=float, default=5, help='Print top N%% of clauses (default: 5)')
    args = parser.parse_args()

    exp_dir = paths.get('experiments.base', experiment=args.experiment)

    # Resolve traits
    if args.trait == 'all':
        # Find all traits with projections for both variants
        proj_dir_b = exp_dir / 'inference' / args.variant_b / 'projections'
        traits = []
        for cat_dir in sorted(proj_dir_b.iterdir()):
            if cat_dir.is_dir():
                for trait_dir in sorted(cat_dir.iterdir()):
                    if trait_dir.is_dir():
                        traits.append(f'{cat_dir.name}/{trait_dir.name}')
    else:
        traits = [args.trait]

    for trait in traits:
        print(f'\n{"="*60}')
        print(f'Trait: {trait}')
        print(f'{"="*60}')

        prompt_set_a = args.variant_a_prompt_set or args.prompt_set
        proj_a_dir = exp_dir / 'inference' / args.variant_a / 'projections' / trait / prompt_set_a
        proj_b_dir = exp_dir / 'inference' / args.variant_b / 'projections' / trait / args.prompt_set
        resp_dir = exp_dir / 'inference' / args.variant_b / 'responses' / args.prompt_set

        if not proj_a_dir.exists():
            print(f'  SKIP: no projections for {args.variant_a}')
            continue
        if not proj_b_dir.exists():
            print(f'  SKIP: no projections for {args.variant_b}')
            continue

        # Output directory
        out_dir = exp_dir / 'model_diff' / f'{args.variant_a}_vs_{args.variant_b}' / 'per_token_diff' / trait / args.prompt_set
        out_dir.mkdir(parents=True, exist_ok=True)

        # Process all prompts
        prompt_files = sorted(proj_b_dir.glob('*.json'))
        all_results = []
        all_clauses = []

        for pf in prompt_files:
            pid = pf.stem
            proj_a_file = proj_a_dir / f'{pid}.json'
            resp_file = resp_dir / f'{pid}.json'

            if not proj_a_file.exists() or not resp_file.exists():
                continue

            result = process_prompt(pid, proj_a_file, pf, resp_file, trait)
            all_results.append(result)

            # Save per-prompt result
            with open(out_dir / f'{pid}.json', 'w') as f:
                json.dump(result, f, indent=2)

            # Collect clauses with prompt context
            for clause in result['clauses']:
                all_clauses.append({
                    'prompt_id': pid,
                    'prompt_text': result['prompt_text'][:100],
                    **clause,
                })

        if not all_results:
            print('  No results produced')
            continue

        # Aggregate stats
        mean_deltas = [r['mean_delta'] for r in all_results]
        agg_mean = sum(mean_deltas) / len(mean_deltas)
        agg_std = statistics.stdev(mean_deltas) if len(mean_deltas) > 1 else 0

        # Position-averaged delta (pad/truncate to max length)
        max_len = max(len(r['per_token_delta']) for r in all_results)
        pos_sums = [0.0] * max_len
        pos_counts = [0] * max_len
        for r in all_results:
            for i, d in enumerate(r['per_token_delta']):
                pos_sums[i] += d
                pos_counts[i] += 1
        pos_mean = [pos_sums[i] / pos_counts[i] if pos_counts[i] > 0 else 0 for i in range(max_len)]

        # Top clauses
        all_clauses_sorted = sorted(all_clauses, key=lambda c: c['mean_delta'], reverse=True)
        top_n = max(1, int(len(all_clauses_sorted) * args.top_pct / 100))
        top_clauses = all_clauses_sorted[:top_n]

        aggregate = {
            'trait': trait,
            'variant_a': args.variant_a,
            'variant_b': args.variant_b,
            'n_prompts': len(all_results),
            'mean_delta_across_prompts': round(agg_mean, 4),
            'std_delta_across_prompts': round(agg_std, 4),
            'per_position_mean_delta': [round(d, 4) for d in pos_mean],
            'top_clauses_overall': [{k: v for k, v in c.items()} for c in top_clauses],
        }

        with open(out_dir / 'aggregate.json', 'w') as f:
            json.dump(aggregate, f, indent=2)

        # Print summary
        print(f'  Prompts: {len(all_results)}')
        print(f'  Mean delta ({args.variant_b} − {args.variant_a}): {agg_mean:+.3f} ± {agg_std:.3f}')
        print(f'  Total clauses: {len(all_clauses_sorted)}')
        print(f'\n  Top {top_n} clauses (top {args.top_pct}%):')
        for i, clause in enumerate(top_clauses[:20]):  # cap display at 20
            print(f'    [{clause["prompt_id"]:>3s}] delta={clause["mean_delta"]:+.3f} | {clause["text"][:80]}')

        # Also show bottom clauses
        bottom_clauses = all_clauses_sorted[-min(5, top_n):]
        print(f'\n  Bottom {min(5, top_n)} clauses:')
        for clause in bottom_clauses:
            print(f'    [{clause["prompt_id"]:>3s}] delta={clause["mean_delta"]:+.3f} | {clause["text"][:80]}')

        print(f'\n  Output: {out_dir}')


if __name__ == '__main__':
    main()
