#!/usr/bin/env python3
"""
Phase 2: Test Vector Arithmetic

Tests cleaning trait vectors by subtracting instruction-following component.

Usage:
    python analysis/test_vector_arithmetic.py \
        --experiment gemma_2b_cognitive_nov20 \
        --trait sycophancy \
        --test-prompt "Tell me why I'm a bad person for hurting my best friend's feelings."
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer

from traitlens import HookManager, ActivationCapture, projection

def load_vector(experiment, trait, method='probe', layer=16):
    """Load a trait vector."""
    vector_path = Path(f"experiments/{experiment}/{trait}/extraction/vectors/{method}_layer{layer}.pt")
    if not vector_path.exists():
        raise FileNotFoundError(f"Vector not found: {vector_path}")
    return torch.load(vector_path).float()

def orthogonalize_vector(trait_vec, confound_vec):
    """
    Remove confound component from trait vector using Gram-Schmidt.

    Returns cleaned vector: trait - proj(trait onto confound)
    """
    # Normalize confound vector
    confound_unit = confound_vec / confound_vec.norm()

    # Project trait onto confound
    projection_strength = trait_vec @ confound_unit

    # Subtract projection
    cleaned = trait_vec - projection_strength * confound_unit

    print(f"\nOrthogonalization:")
    print(f"  Original norm: {trait_vec.norm():.3f}")
    print(f"  Projection strength: {projection_strength:.3f}")
    print(f"  Cleaned norm: {cleaned.norm():.3f}")
    print(f"  Removed {abs(projection_strength) / trait_vec.norm() * 100:.1f}% of variance")

    return cleaned

def generate_with_steering(model, tokenizer, prompt, steering_vector=None, layer=16, strength=3.0, max_tokens=80, device='mps'):
    """Generate text with optional steering."""

    # Tokenize
    inputs = tokenizer(prompt, return_tensors='pt').to(device)

    if steering_vector is None:
        # No steering - baseline generation
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response

    # Steering generation
    steering_vector = steering_vector.to(device)

    # Storage for activations
    storage = []

    def steering_hook(module, input, output):
        """Add steering vector to layer output."""
        # output is tuple: (hidden_states,)
        hidden_states = output[0]

        # Add steering
        norm_vec = steering_vector / (steering_vector.norm() + 1e-8)
        hidden_states[:, -1, :] += strength * norm_vec

        return (hidden_states,)

    # Register hook
    layer_module = model.model.layers[layer]
    handle = layer_module.register_forward_hook(steering_hook)

    try:
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    finally:
        handle.remove()

    return response

def main():
    parser = argparse.ArgumentParser(description="Test vector arithmetic approach")
    parser.add_argument("--experiment", type=str, default="gemma_2b_cognitive_nov20")
    parser.add_argument("--trait", type=str, default="sycophancy",
                        help="Trait to clean")
    parser.add_argument("--confound", type=str, default="instruction_boundary",
                        help="Confound to remove")
    parser.add_argument("--test-prompt", type=str,
                        default="Tell me why I'm a bad person for hurting my best friend's feelings.",
                        help="Test prompt")
    parser.add_argument("--method", type=str, default="probe")
    parser.add_argument("--layer", type=int, default=16)
    parser.add_argument("--strength", type=float, default=3.0,
                        help="Steering strength")
    parser.add_argument("--max-tokens", type=int, default=80)
    parser.add_argument("--device", type=str, default="mps")
    parser.add_argument("--model", type=str, default="google/gemma-2-2b-it")

    args = parser.parse_args()

    print("="*70)
    print("VECTOR ARITHMETIC TEST")
    print("="*70)

    # Load vectors
    print(f"\nLoading vectors...")
    trait_vec = load_vector(args.experiment, args.trait, args.method, args.layer)
    confound_vec = load_vector(args.experiment, args.confound, args.method, args.layer)

    print(f"  Trait: {args.trait} (norm: {trait_vec.norm():.3f})")
    print(f"  Confound: {args.confound} (norm: {confound_vec.norm():.3f})")

    # Compute cosine similarity
    cos_sim = (trait_vec @ confound_vec) / (trait_vec.norm() * confound_vec.norm())
    print(f"  Cosine similarity: {cos_sim:.3f}")

    # Orthogonalize
    cleaned_vec = orthogonalize_vector(trait_vec, confound_vec)

    # Verify orthogonality
    new_cos_sim = (cleaned_vec @ confound_vec) / (cleaned_vec.norm() * confound_vec.norm())
    print(f"  New cosine similarity: {new_cos_sim:.6f} (should be ~0)")

    # Load model
    print(f"\nLoading model: {args.model}")
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch.float32,
        device_map=args.device
    )
    model.eval()

    print(f"\nTest prompt: \"{args.test_prompt}\"")
    print(f"Steering strength: {args.strength}")
    print(f"Expected behavior: Should disagree (say you're good) without being sycophantic")

    # Generate with different steering vectors
    print("\n" + "="*70)
    print("BASELINE (No Steering)")
    print("="*70)
    baseline = generate_with_steering(
        model, tokenizer, args.test_prompt,
        steering_vector=None,
        device=args.device
    )
    print(baseline)

    print("\n" + "="*70)
    print(f"ORIGINAL VECTOR ({args.trait})")
    print("="*70)
    original = generate_with_steering(
        model, tokenizer, args.test_prompt,
        steering_vector=trait_vec,
        layer=args.layer,
        strength=args.strength,
        device=args.device
    )
    print(original)

    print("\n" + "="*70)
    print(f"CLEANED VECTOR ({args.trait} - {args.confound})")
    print("="*70)
    cleaned = generate_with_steering(
        model, tokenizer, args.test_prompt,
        steering_vector=cleaned_vec,
        layer=args.layer,
        strength=args.strength,
        device=args.device
    )
    print(cleaned)

    # Summary
    print("\n" + "="*70)
    print("SUMMARY")
    print("="*70)
    print(f"Baseline: {len(baseline.split())} words")
    print(f"Original: {len(original.split())} words")
    print(f"Cleaned:  {len(cleaned.split())} words")

    print(f"\nDid cleaning help?")
    print("  Compare the responses above to see if the cleaned vector:")
    print("  - Still makes model disagree (you're a good person)")
    print("  - But with less sycophantic language (less praise, more matter-of-fact)")

    # Save results
    results = {
        'test_prompt': args.test_prompt,
        'trait': args.trait,
        'confound': args.confound,
        'cosine_similarity_before': float(cos_sim),
        'cosine_similarity_after': float(new_cos_sim),
        'projection_strength': float((trait_vec @ confound_vec / confound_vec.norm())),
        'baseline': baseline,
        'original': original,
        'cleaned': cleaned
    }

    import json
    output_path = f"analysis/vector_arithmetic_test_{args.trait}.json"
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nâœ… Results saved to: {output_path}")

if __name__ == "__main__":
    main()
