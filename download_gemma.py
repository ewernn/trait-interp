#!/usr/bin/env python3
"""
Download and cache Gemma 2 9B Instruct model
This will download ~18GB to ~/.cache/huggingface/hub/
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

print("Starting Gemma 2 9B Instruct download...")
print("This will download ~18GB to ~/.cache/huggingface/hub/")
print("=" * 60)

model_name = "google/gemma-2-9b-it"

# Download tokenizer (small, fast)
print("\n[1/2] Downloading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
print(f"✓ Tokenizer downloaded")

# Download model (large, ~18GB)
print("\n[2/2] Downloading model weights (~18GB)...")
print("This will take several minutes depending on your connection...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,  # Use float16 for M1 compatibility
    device_map="auto",
    low_cpu_mem_usage=True
)
print(f"✓ Model downloaded successfully!")

print("\n" + "=" * 60)
print("Download complete!")
print(f"Model cached at: ~/.cache/huggingface/hub/models--google--gemma-2-9b-it")
print(f"Total size: ~18GB")
print("\nModel details:")
print(f"  - Hidden size: {model.config.hidden_size}")
print(f"  - Num layers: {model.config.num_hidden_layers}")
print(f"  - Vocab size: {model.config.vocab_size}")
