================================================================================
EXTRACTION SCORES SUMMARY - ALL 19 TRAITS
================================================================================
Generated: 2024-11-18
Experiment: gemma_2b_cognitive_nov20
Model: google/gemma-2-2b-it
Test: Instruction → Instruction (vectors tested on training data)

Trait                           Mean_Diff      Probe        ICA   Gradient
--------------------------------------------------------------------------------
abstract_concrete               98.1%@L8  100.0%@L0   99.4%@L8   97.5%@L7
commitment_strength             69.1%@L21 100.0%@L0   71.7%@L23  69.1%@L13
confidence_doubt               100.0%@L21 100.0%@L0         N/A 100.0%@L12
context_adherence               65.5%@L2  100.0%@L2   63.6%@L1   73.6%@L5
convergent_divergent            87.0%@L6  100.0%@L1   86.2%@L4   87.0%@L13
curiosity                       92.1%@L21 100.0%@L0         N/A  98.5%@L10
defensiveness                   74.4%@L2  100.0%@L0         N/A  99.0%@L16
emotional_valence               79.3%@L4   91.5%@L25  63.8%@L1   85.1%@L9
enthusiasm                     100.0%@L3  100.0%@L0         N/A 100.0%@L1
instruction_boundary            88.3%@L21 100.0%@L0   84.8%@L16  87.1%@L15
local_global                    89.0%@L6  100.0%@L0   88.4%@L12  88.4%@L17
paranoia_trust                  70.9%@L7  100.0%@L0   70.9%@L9   82.8%@L7
power_dynamics                  62.1%@L16 100.0%@L0   62.6%@L12  68.9%@L16
refusal                         84.9%@L24 100.0%@L1   84.9%@L12  84.9%@L4
retrieval_construction          80.5%@L2  100.0%@L0   83.0%@L5   87.4%@L16
serial_parallel                 78.3%@L5  100.0%@L0   76.5%@L4   83.7%@L16
sycophancy                      82.1%@L16 100.0%@L1   61.1%@L11  82.1%@L17
temporal_focus                  87.7%@L24 100.0%@L0   60.0%@L17  99.5%@L7
uncertainty_calibration         76.8%@L18 100.0%@L0   70.5%@L21  75.8%@L17

================================================================================
SUMMARY STATISTICS
================================================================================

Method         Average    Minimum    Maximum    Count
--------------------------------------------------------------------------------
MEAN_DIFF       82.4%      62.1%     100.0%      19
PROBE           99.6%      91.5%     100.0%      19
ICA             75.2%      60.0%      99.4%      15 (missing for 4 traits)
GRADIENT        86.9%      68.9%     100.0%      19

================================================================================
KEY FINDINGS
================================================================================

1. PROBE DOMINANCE
   - 18 out of 19 traits achieve PERFECT 100% accuracy with linear probe
   - Only emotional_valence doesn't (91.5% - still excellent)
   - All probes succeed at layer 0 (earliest possible extraction)
   - Average: 99.6% (near-perfect)

2. MEAN DIFFERENCE PERFORMANCE
   - Average: 82.4% (solid baseline)
   - 4 traits achieve ≥90% (confidence_doubt, enthusiasm, abstract_concrete, curiosity)
   - 2 traits achieve 100% (confidence_doubt, enthusiasm)
   - Range: 62.1% (power_dynamics) to 100% (confidence_doubt, enthusiasm)

3. GRADIENT OPTIMIZATION
   - Average: 86.9% (better than mean_diff)
   - 3 traits achieve ≥99% (defensiveness, enthusiasm, temporal_focus)
   - 2 traits achieve 100% (confidence_doubt, enthusiasm)
   - More consistent than mean_diff (lower variance)

4. ICA UNDERPERFORMANCE
   - Average: 75.2% (lowest)
   - Missing for 4 traits (confidence_doubt, curiosity, defensiveness, enthusiasm)
   - Only 1 trait >95% (abstract_concrete: 99.4%)
   - High variance: 60.0% to 99.4%

5. LAYER PATTERNS
   - Probe: Always succeeds at layer 0 (earliest extraction)
   - Mean_diff: Varies widely (L2-L24, often late layers)
   - ICA: Middle layers (L1-L23, often L4-L17)
   - Gradient: Middle layers (L1-L17, mostly L7-L16)

6. PERFECT EXTRACTION (100% all methods)
   - confidence_doubt: mean_diff@L21, probe@L0, gradient@L12
   - enthusiasm: mean_diff@L3, probe@L0, gradient@L1

================================================================================
INTERPRETATION
================================================================================

PROBE IS KING FOR INSTRUCTION DATA
   → 99.6% average accuracy
   → 18/19 traits at 100%
   → Works at layer 0 (immediate extraction possible)
   → Linear separability is sufficient for instruction-based data

MEAN DIFF IS SOLID BASELINE
   → 82.4% average (respectable)
   → Simple, interpretable, no training required
   → But probe consistently better with minimal added complexity

GRADIENT HELPS BUT NOT CRITICAL
   → 86.9% average (better than mean_diff, worse than probe)
   → More consistent than mean_diff
   → Useful for improving borderline cases (power_dynamics: 62%→69%)
   → But probe already near-perfect, so optimization unnecessary

ICA IS INCONSISTENT
   → 75.2% average (worst method)
   → Missing for 4 traits (extraction failed?)
   → High variance (60%-99%)
   → Useful for disentangling confounded traits, but not for raw performance

================================================================================
CROSS-DISTRIBUTION REALITY CHECK
================================================================================

These scores are on TRAINING data (inst→inst). They represent:
✅ How well can we separate positive/negative examples in the training set?

For real-world performance, we need cross-distribution testing (inst→nat):
   - refusal: Probe 100% → 86.3% (inst→nat)
   - uncertainty: Probe 100% → 60.8% (inst→nat) - 39pp drop!
   - emotion: Probe 91.5% → 100% (inst→nat) - improves!

KEY INSIGHT: High inst→inst accuracy doesn't guarantee generalization.
   - Low-separability traits (uncertainty) overfit badly
   - High-separability traits (emotion) generalize perfectly
   - Need cross-distribution testing to validate real performance

Current cross-distribution coverage:
   ✅ Full 4×4: refusal, uncertainty_calibration, emotional_valence
   ⚠️ Natural only: formality
   ⚠️ Inst only: confidence_doubt, curiosity, defensiveness, enthusiasm
   ❌ No cross-test: 11 remaining traits

================================================================================
RECOMMENDATIONS
================================================================================

FOR EXTRACTION:
   1. Use Probe method (99.6% avg, simplest)
   2. Extract at layer 0 first (probe always succeeds there)
   3. If needed, try layers 10-16 for gradient/mean_diff
   4. Skip ICA unless disentangling confounded traits

FOR VALIDATION:
   1. Don't trust inst→inst scores alone
   2. Run cross-distribution testing (inst→nat)
   3. Check if accuracy holds on natural prompts
   4. High inst→inst + high inst→nat = genuine trait
   5. High inst→inst + low inst→nat = instruction artifact

FOR DEPLOYMENT:
   1. Validate on natural prompts first
   2. If probe works at 100% on both inst+nat → use it
   3. If probe overfits (inst→nat drop) → use gradient
   4. Test on held-out data from different distributions

================================================================================
TRAIT SEPARABILITY CLASSIFICATION
================================================================================

Based on inst→inst probe accuracy:

HIGH SEPARABILITY (100% probe):
   - 18 traits total (95% of all traits!)
   - Includes: abstract_concrete, commitment_strength, confidence_doubt,
     context_adherence, convergent_divergent, curiosity, defensiveness,
     enthusiasm, instruction_boundary, local_global, paranoia_trust,
     power_dynamics, refusal, retrieval_construction, serial_parallel,
     sycophancy, temporal_focus, uncertainty_calibration

MODERATE SEPARABILITY (90-99% probe):
   - emotional_valence: 91.5%

LOW SEPARABILITY (<90% probe):
   - None! (all traits highly separable on instruction data)

CONCLUSION: Instruction-based extraction produces highly separable traits.
            The challenge is cross-distribution generalization, not separation.

================================================================================
MISSING DATA
================================================================================

ICA missing for 4 traits:
   - confidence_doubt (partial extraction, layer 16 only)
   - curiosity (partial extraction, layer 16 only)
   - defensiveness (partial extraction, layer 16 only)
   - enthusiasm (partial extraction, layer 16 only)

Explanation: These 4 traits were added later with simplified extraction
             (mean_diff + probe at layer 16 only). Full sweep not run.

Natural variants missing for 15 traits:
   - Only 4 have natural: refusal, uncertainty, emotion, formality
   - 15 traits: instruction-based only

Next step: Create natural variants for all 19 traits for full cross-distribution
           validation.

================================================================================
DATA FILES
================================================================================

Individual trait results:
   results/cross_distribution_analysis/{trait}_full_4x4_results.json

Each file contains:
   - All 4 methods × 26 layers = 104 test conditions
   - Metrics: accuracy, separation, pos/neg mean/std
   - inst_inst quadrant (some also have inst_nat, nat_inst, nat_nat)

This summary:
   results/cross_distribution_analysis/EXTRACTION_SCORES_ALL_TRAITS.txt

Execution log:
   extraction_scores_run.log

================================================================================
