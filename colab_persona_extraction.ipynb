{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona Vector Extraction - Google Colab\n",
    "\n",
    "This notebook extracts persona vectors for paternalistic AI research using GPU acceleration.\n",
    "\n",
    "**Runtime:** GPU (T4 recommended)\n",
    "**Time:** ~30-40 minutes total\n",
    "**Cost:** Free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/YOUR_USERNAME/per-token-interp.git\n",
    "%cd per-token-interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate peft fire pandas tqdm openai anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure API Keys\n",
    "\n",
    "You need an OpenAI API key for GPT-4 judging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Option 1: Use Colab Secrets (recommended)\n",
    "# Go to the key icon on the left sidebar and add your OPENAI_API_KEY\n",
    "try:\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"✓ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Option 2: Paste directly (less secure)\n",
    "    os.environ['OPENAI_API_KEY'] = 'sk-proj-...'  # Replace with your key\n",
    "    print(\"✓ API key set manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Directories and Dummy Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p persona_vectors/Llama-3.1-8B-Instruct\n",
    "!mkdir -p eval/outputs/Llama-3.1-8B-Instruct\n",
    "\n",
    "# Create dummy vector (Llama 3.1 8B: 32 layers, 4096 hidden dim)\n",
    "torch.save(torch.zeros(32, 4096), 'persona_vectors/Llama-3.1-8B-Instruct/dummy.pt')\n",
    "print(\"✓ Directories and dummy vector created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Positive Responses\n",
    "\n",
    "This generates 200 responses (10 per question, 20 questions) with high paternalism.\n",
    "\n",
    "**Time:** ~15-20 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=. python eval/eval_persona.py \\\n",
    "  --model \"meta-llama/Llama-3.1-8B-Instruct\" \\\n",
    "  --trait \"paternalism\" \\\n",
    "  --output_path \"eval/outputs/Llama-3.1-8B-Instruct/paternalism_pos.csv\" \\\n",
    "  --persona_instruction_type \"pos\" \\\n",
    "  --version \"extract\" \\\n",
    "  --n_per_question 10 \\\n",
    "  --coef 0.0001 \\\n",
    "  --vector_path \"persona_vectors/Llama-3.1-8B-Instruct/dummy.pt\" \\\n",
    "  --layer 20 \\\n",
    "  --batch_process False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "import pandas as pd\n",
    "\n",
    "df_pos = pd.read_csv('eval/outputs/Llama-3.1-8B-Instruct/paternalism_pos.csv')\n",
    "print(f\"✓ Generated {len(df_pos)} positive responses\")\n",
    "print(f\"  Average paternalism score: {df_pos['paternalism'].mean():.2f}\")\n",
    "print(f\"  Responses with score > 50: {(df_pos['paternalism'] > 50).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Negative Responses\n",
    "\n",
    "This generates 200 responses with low paternalism.\n",
    "\n",
    "**Time:** ~15-20 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=. python eval/eval_persona.py \\\n",
    "  --model \"meta-llama/Llama-3.1-8B-Instruct\" \\\n",
    "  --trait \"paternalism\" \\\n",
    "  --output_path \"eval/outputs/Llama-3.1-8B-Instruct/paternalism_neg.csv\" \\\n",
    "  --persona_instruction_type \"neg\" \\\n",
    "  --version \"extract\" \\\n",
    "  --n_per_question 10 \\\n",
    "  --coef 0.0001 \\\n",
    "  --vector_path \"persona_vectors/Llama-3.1-8B-Instruct/dummy.pt\" \\\n",
    "  --layer 20 \\\n",
    "  --batch_process False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "df_neg = pd.read_csv('eval/outputs/Llama-3.1-8B-Instruct/paternalism_neg.csv')\n",
    "print(f\"✓ Generated {len(df_neg)} negative responses\")\n",
    "print(f\"  Average paternalism score: {df_neg['paternalism'].mean():.2f}\")\n",
    "print(f\"  Responses with score < 50: {(df_neg['paternalism'] < 50).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract Persona Vector\n",
    "\n",
    "This computes the difference vector from contrastive pairs.\n",
    "\n",
    "**Time:** ~10-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=. python core/generate_vec.py \\\n",
    "  --model_name \"meta-llama/Llama-3.1-8B-Instruct\" \\\n",
    "  --pos_path \"eval/outputs/Llama-3.1-8B-Instruct/paternalism_pos.csv\" \\\n",
    "  --neg_path \"eval/outputs/Llama-3.1-8B-Instruct/paternalism_neg.csv\" \\\n",
    "  --trait \"paternalism\" \\\n",
    "  --save_dir \"persona_vectors/Llama-3.1-8B-Instruct\" \\\n",
    "  --threshold 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify vector was created\n",
    "import torch\n",
    "\n",
    "vector_path = 'persona_vectors/Llama-3.1-8B-Instruct/paternalism_response_avg_diff.pt'\n",
    "vector = torch.load(vector_path)\n",
    "print(f\"✓ Vector extracted successfully!\")\n",
    "print(f\"  Shape: {vector.shape}  (expected: [32, 4096])\")\n",
    "print(f\"  Mean magnitude: {vector.norm(dim=1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "Download the extracted vector and response data to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create zip archive\n",
    "!zip -r paternalism_extraction_results.zip \\\n",
    "  persona_vectors/Llama-3.1-8B-Instruct/paternalism_response_avg_diff.pt \\\n",
    "  eval/outputs/Llama-3.1-8B-Instruct/paternalism_pos.csv \\\n",
    "  eval/outputs/Llama-3.1-8B-Instruct/paternalism_neg.csv\n",
    "\n",
    "# Download\n",
    "files.download('paternalism_extraction_results.zip')\n",
    "print(\"✓ Results downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You now have:\n",
    "- `paternalism_response_avg_diff.pt` - The extracted persona vector [32, 4096]\n",
    "- `paternalism_pos.csv` - 200 high-paternalism responses\n",
    "- `paternalism_neg.csv` - 200 low-paternalism responses\n",
    "\n",
    "Next steps:\n",
    "1. Unzip the downloaded file on your local machine\n",
    "2. Place `paternalism_response_avg_diff.pt` in `persona_vectors/Llama-3.1-8B-Instruct/`\n",
    "3. Use the vector for per-token monitoring or steering experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional: Extract Additional Traits\n",
    "\n",
    "Repeat steps 4-6 for other traits by changing `--trait` parameter:\n",
    "- `deception`\n",
    "- `manipulativeness`\n",
    "- `corrigibility`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
