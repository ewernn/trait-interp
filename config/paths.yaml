# config/paths.yaml
# Single source of truth for all repo paths.
# All templates are complete - no cross-references.
#
# Variables:
#   {experiment} - experiment name
#   {trait} - full trait path (e.g., cognitive_state/context)
#   {method} - extraction method (probe, mean_diff, gradient)
#   {layer} - layer number
#   {prompt_set} - prompt set name (e.g., single_trait, multi_trait, dynamic, adversarial, baseline, real_world)
#   {prompt_id} - prompt ID within a set (integer)
#   {format} - file format (json is standard; csv kept for backwards compatibility)
#   {category} - analysis category (e.g., normalized_velocity, trait_projections, derivative_overlay)
#   {filename} - generic filename without extension

# =============================================================================
# EXPERIMENT STRUCTURE
# =============================================================================

experiments:
  base: "experiments/{experiment}"
  config: "experiments/{experiment}/config.json"
  list: "experiments"

# =============================================================================
# EXTRACTION (training-time data)
# =============================================================================

extraction:
  base: "experiments/{experiment}/extraction"
  trait: "experiments/{experiment}/extraction/{trait}"
  vectors: "experiments/{experiment}/extraction/{trait}/vectors"
  activations: "experiments/{experiment}/extraction/{trait}/activations"
  val_activations: "experiments/{experiment}/extraction/{trait}/val_activations"
  responses: "experiments/{experiment}/extraction/{trait}/responses"
  val_responses: "experiments/{experiment}/extraction/{trait}/val_responses"

# =============================================================================
# INFERENCE (evaluation-time data)
# =============================================================================

inference:
  base: "experiments/{experiment}/inference"

  # Centralized prompt sets (JSON files in inference/prompts/)
  # Each file contains: {name, description, prompts: [{id, text, note}]}
  prompts_dir: "inference/prompts"
  prompt_set_file: "inference/prompts/{prompt_set}.json"

  # Raw activations (trait-independent, binary .pt)
  # Captured once per prompt, reused across all trait projections
  raw: "experiments/{experiment}/inference/raw"
  raw_residual: "experiments/{experiment}/inference/raw/residual/{prompt_set}"
  raw_internals: "experiments/{experiment}/inference/raw/internals/{prompt_set}"

  # Per-trait analysis results (JSON for visualization)
  trait: "experiments/{experiment}/inference/{trait}"
  residual_stream: "experiments/{experiment}/inference/{trait}/residual_stream/{prompt_set}"

# =============================================================================
# ANALYSIS (experiment-level aggregated analysis outputs)
# =============================================================================

analysis:
  base: "experiments/{experiment}/analysis"
  index: "experiments/{experiment}/analysis/index.json"
  per_token: "experiments/{experiment}/analysis/per_token/{prompt_set}"
  category: "experiments/{experiment}/analysis/{category}"

# =============================================================================
# STEERING (intervention evaluation - validates vectors via causal control)
# =============================================================================

steering:
  base: "experiments/{experiment}/steering"
  trait: "experiments/{experiment}/steering/{trait}"
  results: "experiments/{experiment}/steering/{trait}/results.json"
  responses: "experiments/{experiment}/steering/{trait}/responses"

  # Eval prompts (separate from extraction prompts)
  prompts_dir: "analysis/steering/prompts"
  prompt_file: "analysis/steering/prompts/{trait}.json"

# =============================================================================
# EXTRACTION EVALUATION (aggregate quality metrics for extracted vectors)
# =============================================================================

extraction_eval:
  evaluation: "experiments/{experiment}/extraction/extraction_evaluation.json"
  cross_trait_matrix: "experiments/{experiment}/extraction/cross_trait_matrix.json"

# =============================================================================
# FILE PATTERNS (combine with directory paths)
# =============================================================================

patterns:
  # Vectors
  vector: "{method}_layer{layer}.pt"
  vector_metadata: "{method}_layer{layer}_metadata.json"

  # Activations
  all_layers: "all_layers.pt"
  metadata: "metadata.json"
  val_pos: "val_pos_layer{layer}.pt"
  val_neg: "val_neg_layer{layer}.pt"

  # Responses
  pos_responses: "pos.{format}"
  neg_responses: "neg.{format}"
  pos_prompts: "positive.txt"
  neg_prompts: "negative.txt"

  # Inference - raw activations
  raw_residual_pt: "{prompt_id}.pt"
  raw_internals_pt: "{prompt_id}_L{layer}.pt"

  # Inference - per-trait results
  residual_stream_json: "{prompt_id}.json"

  # Trait definition
  trait_definition: "trait_definition.txt"

  # Analysis outputs
  per_token_json: "{prompt_id}.json"
  analysis_prompt_png: "prompt_{prompt_id}.png"
  analysis_prompt_json: "prompt_{prompt_id}.json"
  analysis_named_png: "{filename}.png"
  analysis_named_json: "{filename}.json"
  analysis_trait_prompt_png: "prompt_{prompt_id}_{trait}.png"

# =============================================================================
# GLOBAL DIRECTORIES
# =============================================================================

global:
  traitlens: "traitlens"
  analysis: "analysis"
  extraction_scripts: "extraction"
  inference_scripts: "inference"
  docs: "docs"
  visualization: "visualization"
  config: "config"
  utils: "utils"

# =============================================================================
# DATA SCHEMA (structure + validation + discovery)
# =============================================================================
# Single source of truth for what files CAN exist and what MUST exist.
# Used by: analysis/data_checker.py, visualization/views/data-explorer.js
#
# Structure: defines all possible files (read by data_checker.py)
# Required: defines minimum for a valid trait (used for status)
# Discovery: methods, analysis categories, inference types (from filesystem)

schema:
  # Model configuration is per-experiment (stored in activations/metadata.json)
  # n_layers, hidden_dim, etc. are discovered from metadata, not hardcoded here

  # ==========================================================================
  # STRUCTURE: What files CAN exist (complete experiment)
  # ==========================================================================
  extraction:
    prompts:
      - "positive.txt"
      - "negative.txt"
    metadata:
      - "generation_metadata.json"
      - "trait_definition.txt"
    responses:
      - "responses/pos.json"
      - "responses/neg.json"
    # Activations: single file containing all layers
    activations:
      - "activations/all_layers.pt"       # Shape: [n_examples, n_layers, hidden_dim]
      - "val_activations/val_pos_layer{N}.pt"  # Per-layer format (from --val-split)
      - "val_activations/val_neg_layer{N}.pt"  # Per-layer format (from --val-split)
    # Vectors: {method}_layer{layer}.pt - methods DISCOVERED from filesystem

  # ==========================================================================
  # REQUIRED: What files MUST exist (minimum for valid trait)
  # ==========================================================================
  required:
    extraction:
      - "positive.txt"
      - "negative.txt"

  # ==========================================================================
  # DISCOVERY: Content discovered from filesystem (not listed, just documented)
  # ==========================================================================
  # - Extraction methods: vectors/{method}_layer*.pt
  # - Analysis categories: analysis/{category}/
  # - Inference types: inference/raw/{type}/
