# config/paths.yaml
# Single source of truth for all repo paths.
# All templates are complete - no cross-references.
#
# Variables:
#   {experiment} - experiment name
#   {trait} - full trait path (e.g., cognitive_state/context)
#   {method} - extraction method (probe, mean_diff, gradient)
#   {layer} - layer number
#   {prompt_set} - prompt set name (e.g., single_trait, multi_trait, dynamic, adversarial, baseline, real_world)
#   {prompt_id} - prompt ID within a set (integer)
#   {format} - file format (json is standard; csv kept for backwards compatibility)
#   {category} - analysis category (e.g., normalized_velocity, trait_projections, derivative_overlay)
#   {filename} - generic filename without extension

# =============================================================================
# DATASETS (model-agnostic inputs)
# =============================================================================

datasets:
  base: "datasets"

  # Inference prompts (organized by test type)
  inference: "datasets/inference"
  inference_prompt_set: "datasets/inference/{prompt_set}.json"

  # Trait definitions and scenarios (organized by trait)
  traits: "datasets/traits"
  trait: "datasets/traits/{trait}"
  trait_positive: "datasets/traits/{trait}/positive.txt"
  trait_negative: "datasets/traits/{trait}/negative.txt"
  trait_definition: "datasets/traits/{trait}/definition.txt"
  trait_steering: "datasets/traits/{trait}/steering.json"

# =============================================================================
# EXPERIMENT STRUCTURE
# =============================================================================

experiments:
  base: "experiments/{experiment}"
  config: "experiments/{experiment}/config.json"
  list: "experiments"

# =============================================================================
# EXTRACTION (training-time data)
# =============================================================================

extraction:
  base: "experiments/{experiment}/extraction"
  trait: "experiments/{experiment}/extraction/{trait}"
  # vectors/ and activations/ have subdirs: {position}/{component}/[{method}/]
  # Use helper functions (get_vector_path, get_activation_path) for full paths
  vectors: "experiments/{experiment}/extraction/{trait}/vectors"
  activations: "experiments/{experiment}/extraction/{trait}/activations"
  responses: "experiments/{experiment}/extraction/{trait}/responses"
  responses_metadata: "experiments/{experiment}/extraction/{trait}/responses/metadata.json"
  val_responses: "experiments/{experiment}/extraction/{trait}/val_responses"
  vetting: "experiments/{experiment}/extraction/{trait}/vetting"
  vetting_metadata: "experiments/{experiment}/extraction/{trait}/vetting/metadata.json"
  logit_lens: "experiments/{experiment}/extraction/{trait}/logit_lens.json"

# =============================================================================
# INFERENCE (evaluation-time data)
# =============================================================================

inference:
  base: "experiments/{experiment}/inference"

  # Prompt sets now in datasets/inference/ (see datasets section above)
  # Each file contains: {name, description, prompts: [{id, text, note}]}

  # Raw activations (trait-independent, binary .pt)
  # Captured once per prompt, reused across all trait projections
  raw: "experiments/{experiment}/inference/raw"
  raw_residual: "experiments/{experiment}/inference/raw/residual/{prompt_set}"
  raw_internals: "experiments/{experiment}/inference/raw/internals/{prompt_set}"

  # SAE-encoded features
  sae: "experiments/{experiment}/inference/sae"
  sae_residual: "experiments/{experiment}/inference/sae/{prompt_set}"

  # Responses (trait-independent, shared across all trait projections)
  # Contains prompt/response text and tokens - stored once per prompt
  responses: "experiments/{experiment}/inference/responses/{prompt_set}"
  response_data: "experiments/{experiment}/inference/responses/{prompt_set}/{prompt_id}.json"

  # Per-trait projection results (JSON for visualization)
  # Slim format: projections + dynamics only, references responses/ for text
  trait: "experiments/{experiment}/inference/{trait}"
  residual_stream: "experiments/{experiment}/inference/{trait}/residual_stream/{prompt_set}"

  # Massive activation analysis (from analysis/massive_activations.py)
  massive_activations: "experiments/{experiment}/inference/massive_activations/{prompt_set}.json"

  # Model comparison base (for non-default models, e.g., base model comparison)
  # Subpaths built in code: models_base / "raw" / "residual" / {prompt_set} / etc.
  models_base: "experiments/{experiment}/inference/models/{model}"

# =============================================================================
# ANALYSIS (experiment-level aggregated analysis outputs)
# =============================================================================

analysis:
  base: "experiments/{experiment}/analysis"
  index: "experiments/{experiment}/analysis/index.json"
  per_token: "experiments/{experiment}/analysis/per_token/{prompt_set}"
  category: "experiments/{experiment}/analysis/{category}"

# =============================================================================
# STEERING (intervention evaluation - validates vectors via causal control)
# =============================================================================

steering:
  base: "experiments/{experiment}/steering"
  # steering/{trait}/ has subdirs: {position}/
  # Use helper functions (get_steering_results_path) for full paths
  trait: "experiments/{experiment}/steering/{trait}"
  # Steering prompts in datasets/traits/{trait}/steering.json (see datasets section)

# =============================================================================
# EXTRACTION EVALUATION (aggregate quality metrics for extracted vectors)
# =============================================================================

extraction_eval:
  evaluation: "experiments/{experiment}/extraction/extraction_evaluation.json"
  cross_trait_matrix: "experiments/{experiment}/extraction/cross_trait_matrix.json"

# =============================================================================
# ENSEMBLE EVALUATION (Gaussian-weighted layer combinations)
# =============================================================================

ensemble:
  evaluation: "experiments/{experiment}/extraction/ensemble_evaluation.json"

# =============================================================================
# FILE PATTERNS (combine with directory paths)
# =============================================================================
#
# NOTE: For vectors and activations, use the helper functions in utils/paths.py
# (Python) or visualization/core/paths.js (JS) which handle the full path
# structure including position/component/method subdirectories:
#
#   vectors/{position}/{component}/{method}/layer{layer}.pt
#   activations/{position}/{component}/metadata.json
#
# The patterns below are just filename templates, not full paths.

patterns:
  # Vectors (inside vectors/{position}/{component}/{method}/)
  vector_file: "layer{layer}.pt"

  # Activations (inside activations/{position}/{component}/)
  train_all_layers: "train_all_layers.pt"
  val_all_layers: "val_all_layers.pt"
  metadata: "metadata.json"

  # Responses
  pos_responses: "pos.{format}"
  neg_responses: "neg.{format}"
  pos_prompts: "positive.txt"
  neg_prompts: "negative.txt"

  # Inference - raw activations
  raw_residual_pt: "{prompt_id}.pt"
  raw_internals_pt: "{prompt_id}_L{layer}.pt"

  # Inference - per-trait results
  residual_stream_json: "{prompt_id}.json"

  # Trait definition
  trait_definition: "trait_definition.txt"

  # Analysis outputs
  per_token_json: "{prompt_id}.json"
  analysis_prompt_png: "prompt_{prompt_id}.png"
  analysis_prompt_json: "prompt_{prompt_id}.json"
  analysis_named_png: "{filename}.png"
  analysis_named_json: "{filename}.json"
  analysis_trait_prompt_png: "prompt_{prompt_id}_{trait}.png"

# =============================================================================
# GLOBAL DIRECTORIES
# =============================================================================

global:
  traitlens: "traitlens"
  analysis: "analysis"
  extraction_scripts: "extraction"
  inference_scripts: "inference"
  docs: "docs"
  visualization: "visualization"
  config: "config"
  utils: "utils"

# =============================================================================
# DATA SCHEMA (structure + validation + discovery)
# =============================================================================
# Single source of truth for what files CAN exist and what MUST exist.
# Used by: analysis/data_checker.py, visualization/views/data-explorer.js
#
# Structure: defines all possible files (read by data_checker.py)
# Required: defines minimum for a valid trait (used for status)
# Discovery: methods, analysis categories, inference types (from filesystem)

schema:
  # Model configuration is per-experiment (stored in activations/metadata.json)
  # n_layers, hidden_dim, etc. are discovered from metadata, not hardcoded here

  # ==========================================================================
  # STRUCTURE: What files CAN exist (complete experiment)
  # ==========================================================================
  extraction:
    prompts:
      - "positive.txt"
      - "negative.txt"
    metadata:
      - "generation_metadata.json"
      - "trait_definition.txt"
    responses:
      - "responses/pos.json"
      - "responses/neg.json"
    # Activations: in activations/{position}/{component}/
    activations:
      - "activations/{position}/{component}/train_all_layers.pt"  # Shape: [n_examples, n_layers, hidden_dim]
      - "activations/{position}/{component}/val_all_layers.pt"    # Same format as train
      - "activations/{position}/{component}/metadata.json"
    # Vectors: in vectors/{position}/{component}/{method}/ - methods DISCOVERED from filesystem

  # ==========================================================================
  # REQUIRED: What files MUST exist (minimum for valid trait)
  # ==========================================================================
  required:
    # Trait inputs now in datasets/traits/{trait}/ (see datasets section)
    datasets:
      - "positive.txt"
      - "negative.txt"

  # ==========================================================================
  # DISCOVERY: Content discovered from filesystem (not listed, just documented)
  # ==========================================================================
  # - Extraction methods: vectors/{method}_layer*.pt
  # - Analysis categories: analysis/{category}/
  # - Inference types: inference/raw/{type}/
