# Llama 3.1 70B Base
# Source: https://huggingface.co/meta-llama/Llama-3.1-70B

huggingface_id: meta-llama/Llama-3.1-70B
model_type: llama
variant: base

# Architecture (from HF config.json)
max_context_length: 131072
num_hidden_layers: 80
hidden_size: 8192
num_attention_heads: 64
num_key_value_heads: 8
intermediate_size: 28672
vocab_size: 128256

# Residual stream structure
residual_sublayers: [input, after_attn, output]

# SAE availability
sae:
  available: false

# Defaults
