# Kimi K2 Thinking
# Source: https://huggingface.co/moonshotai/Kimi-K2-Thinking
# 1.04T MoE (32B active), DeepSeek-V3 architecture, native INT4 quantized

huggingface_id: moonshotai/Kimi-K2-Thinking
model_type: kimi_k2  # DeepSeek-V3 underneath, kimi_k2 for inference engine optimizations
variant: it
supports_system_prompt: true

# Architecture (from HF config.json)
max_context_length: 262144
num_hidden_layers: 61
hidden_size: 7168
num_attention_heads: 64
num_key_value_heads: 64  # MLA — latent compression, not standard GQA
intermediate_size: 18432  # Dense layer FFN (layer 0)
vocab_size: 163840

# Residual stream structure
residual_sublayers: [input, after_attn, output]

# MoE
moe:
  n_routed_experts: 384
  num_experts_per_tok: 8
  n_shared_experts: 1
  moe_intermediate_size: 2048  # Per-expert FFN hidden size (SwiGLU)
  first_k_dense_replace: 1  # Layer 0 is dense, layers 1-60 are MoE

# MLA (Multi-head Latent Attention)
mla:
  kv_lora_rank: 512
  q_lora_rank: 1536
  qk_rope_head_dim: 64
  qk_nope_head_dim: 128
  v_head_dim: 128

# SAE availability
sae:
  available: false

notes:
  system_prompt: Supported
  generation_mask: false
  prefill: Supported
  quirks: >-
    Reasoning model — generates <think>...</think> before answer.
    Native INT4 quantized (QAT during post-training).
    MoE with 384 experts (8 active + 1 shared per token).
    Residual stream extraction works on MoE — shared expert acts like constant offset.
