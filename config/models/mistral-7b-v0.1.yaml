# Mistral 7B v0.1 (Base)
# Source: https://huggingface.co/mistralai/Mistral-7B-v0.1

huggingface_id: mistralai/Mistral-7B-v0.1
model_type: mistral
variant: base

# Architecture (from HF config.json)
max_context_length: 32768
num_hidden_layers: 32
hidden_size: 4096
num_attention_heads: 32
num_key_value_heads: 8
intermediate_size: 14336

# Residual stream structure
residual_sublayers: [input, after_attn, output]

# SAE availability
sae:
  available: false

# Defaults
