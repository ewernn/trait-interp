# Kimi K2 Base (AWQ 4-bit)
# Source: https://huggingface.co/QuixiAI/Kimi-K2-Base-AWQ
# Original: https://huggingface.co/moonshotai/Kimi-K2-Base
# 1T MoE (32B active), DeepSeek-V3 architecture, AWQ 4-bit quantization (~540GB)

huggingface_id: QuixiAI/Kimi-K2-Base-AWQ
model_type: kimi_k2
variant: base
supports_system_prompt: false

# Architecture (same as Kimi-K2-Thinking)
max_context_length: 262144
num_hidden_layers: 61
hidden_size: 7168
num_attention_heads: 64
num_key_value_heads: 64  # MLA — latent compression, not standard GQA
intermediate_size: 18432  # Dense layer FFN (layer 0)
vocab_size: 163840

# Residual stream structure
residual_sublayers: [input, after_attn, output]

# MoE
moe:
  n_routed_experts: 384
  num_experts_per_tok: 8
  n_shared_experts: 1
  moe_intermediate_size: 2048
  first_k_dense_replace: 1

# MLA (Multi-head Latent Attention)
mla:
  kv_lora_rank: 512
  q_lora_rank: 1536
  qk_rope_head_dim: 64
  qk_nope_head_dim: 128
  v_head_dim: 128

sae:
  available: false

notes:
  system_prompt: Not supported (base model)
  generation_mask: false
  prefill: Supported
  quirks: >-
    AWQ 4-bit quantization of Kimi-K2-Base (original is block-FP8, ~1.09TB).
    Requires autoawq library + 8x A100 80GB minimum.
    MoE with 384 experts (8 active + 1 shared per token).
    MLA attention — component-level hooks need architecture-specific adaptation.
    PyTorch hooks on residual stream should work; verify WQLinear compatibility.
