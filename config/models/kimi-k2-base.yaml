# Kimi K2 Base (FP8)
# Source: https://huggingface.co/unsloth/Kimi-K2-Base
# 1T MoE (32B active), DeepSeek-V3 architecture, block-FP8 (~959GB)

huggingface_id: unsloth/Kimi-K2-Base
model_type: kimi_k2
variant: base
supports_system_prompt: false

# Architecture (same as Kimi-K2-Thinking)
max_context_length: 262144
num_hidden_layers: 61
hidden_size: 7168
num_attention_heads: 64
num_key_value_heads: 64  # MLA — latent compression, not standard GQA
intermediate_size: 18432  # Dense layer FFN (layer 0)
vocab_size: 163840

# Residual stream structure
residual_sublayers: [input, after_attn, output]

# MoE
moe:
  n_routed_experts: 384
  num_experts_per_tok: 8
  n_shared_experts: 1
  moe_intermediate_size: 2048
  first_k_dense_replace: 1

# MLA (Multi-head Latent Attention)
mla:
  kv_lora_rank: 512
  q_lora_rank: 1536
  qk_rope_head_dim: 64
  qk_nope_head_dim: 128
  v_head_dim: 128

sae:
  available: false

notes:
  system_prompt: Not supported (base model)
  generation_mask: false
  prefill: Supported
  quirks: >-
    Block-FP8 weights (~959GB on disk). Requires 8x H200 or equivalent (~120GB/GPU).
    MoE with 384 experts (8 active + 1 shared per token).
    MLA attention — component-level hooks need architecture-specific adaptation.
    DynamicCache monkeypatch required (utils/model.py) for transformers 5.x compatibility.
