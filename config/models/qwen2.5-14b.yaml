huggingface_id: Qwen/Qwen2.5-14B
model_type: qwen2
variant: base
supports_system_prompt: false
max_context_length: 131072
num_hidden_layers: 48
hidden_size: 5120
num_attention_heads: 40
num_key_value_heads: 8
intermediate_size: 13824
residual_sublayers:
- input
- after_attn
- output
sae:
  available: false
notes:
  system_prompt: Not supported (base model)
  generation_mask: false
  prefill: Supported
  quirks: Base model for DeepSeek-R1-Distill-Qwen-14B. Same architecture (48 layers, 5120 hidden).
