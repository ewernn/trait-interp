# Mistral 7B SFT Beta (SFT only, no DPO)
# Source: https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta

huggingface_id: HuggingFaceH4/mistral-7b-sft-beta
model_type: mistral
variant: sft

# Architecture (same as Mistral-7B base)
num_hidden_layers: 32
hidden_size: 4096
num_attention_heads: 32
num_key_value_heads: 8
intermediate_size: 14336

# Residual stream structure
residual_sublayers: [input, after_attn, output]

# SAE availability
sae:
  available: false

# Defaults
defaults:
  monitoring_layer: 16
