huggingface_id: google/gemma-2-2b-it
model_type: gemma2
variant: it
supports_system_prompt: false
max_context_length: 8192
num_hidden_layers: 26
hidden_size: 2304
num_attention_heads: 8
num_key_value_heads: 4
intermediate_size: 9216
residual_sublayers:
- input
- after_attn
- output
sae:
  available: true
  provider: gemma-scope
  base_path: sae/gemma-scope-2b-pt-res-canonical
  layer_template: layer_{layer}_width_16k_canonical
  downloaded_layers:
  - 16
  all_layers:
  - 0
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
notes:
  system_prompt: Not supported (System role not supported)
  default_system: None detected
  generation_mask: false
  tool_calling: Accepted but no tool formatting visible
  prefill: Supported
  quirks: null
