"""
Shared model loading and prompt formatting utilities.

Usage:
    from utils.model import load_model, load_model_with_lora, format_prompt, load_experiment_config

    model, tokenizer = load_model("google/gemma-2-2b-it")
    model, tokenizer = load_model("google/gemma-2-2b-it", device="cuda")

    # Load with LoRA adapter (for 70B models with quantization)
    model, tokenizer = load_model_with_lora(
        "meta-llama/Llama-3.3-70B-Instruct",
        lora_adapter="auditing-agents/llama-3.3-70b-dpo-rt-lora",
        load_in_8bit=True
    )

    # Format prompt (auto-detects chat template from tokenizer)
    formatted = format_prompt("Hello", tokenizer)

    # Load experiment config
    config = load_experiment_config("my_experiment")
"""

import json
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig


def load_model(
    model_name: str,
    device: str = "auto",
    dtype: torch.dtype = torch.bfloat16,
    load_in_8bit: bool = False,
    load_in_4bit: bool = False,
) -> tuple[AutoModelForCausalLM, AutoTokenizer]:
    """
    Load model and tokenizer.

    Args:
        model_name: HuggingFace model name
        device: Device map ('auto', 'cuda', 'cpu', 'mps')
        dtype: Model dtype (default: bfloat16, fp16 for AWQ models)
        load_in_8bit: Load in 8-bit quantization (requires bitsandbytes)
        load_in_4bit: Load in 4-bit quantization (requires bitsandbytes)

    Returns:
        (model, tokenizer) tuple
    """
    print(f"Loading model: {model_name}...")
    if load_in_8bit:
        print("  Using 8-bit quantization")
    if load_in_4bit:
        print("  Using 4-bit quantization")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # Set pad_token if missing (required for batched generation, e.g. Mistral)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # AWQ models require fp16 (triton kernels don't support bf16)
    if "AWQ" in model_name or "awq" in model_name.lower():
        dtype = torch.float16
        print(f"  AWQ model detected, using fp16")

    model_kwargs = {
        "torch_dtype": dtype,
        "device_map": device,
    }
    if load_in_8bit:
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_enable_fp32_cpu_offload=True,  # Allow some CPU offload if needed
        )
        model_kwargs["quantization_config"] = quantization_config
        # Force single GPU to avoid auto-planner issues
        if device == "auto":
            model_kwargs["device_map"] = "cuda:0"
    elif load_in_4bit:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=dtype,
        )
        model_kwargs["quantization_config"] = quantization_config

    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
    model.eval()
    print("Model loaded.")
    return model, tokenizer


def load_model_with_lora(
    model_name: str,
    lora_adapter: str = None,
    load_in_8bit: bool = False,
    load_in_4bit: bool = False,
    device: str = "auto",
    dtype: torch.dtype = torch.float16,
) -> tuple[AutoModelForCausalLM, AutoTokenizer]:
    """
    Load model with optional LoRA adapter and quantization.

    For large models (70B+), use load_in_8bit=True on A100 80GB.

    Args:
        model_name: HuggingFace model name (base model)
        lora_adapter: Optional LoRA adapter path (HuggingFace or local)
        load_in_8bit: Load in 8-bit quantization (requires bitsandbytes)
        load_in_4bit: Load in 4-bit quantization (requires bitsandbytes)
        device: Device map ('auto', 'cuda', 'cpu')
        dtype: Model dtype (default: float16 for compatibility with quantization)

    Returns:
        (model, tokenizer) tuple
    """
    print(f"Loading model: {model_name}...")
    if load_in_8bit:
        print("  Using 8-bit quantization")
    if load_in_4bit:
        print("  Using 4-bit quantization")
    if lora_adapter:
        print(f"  With LoRA adapter: {lora_adapter}")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Build model loading kwargs
    model_kwargs = {
        "device_map": device,
        "torch_dtype": dtype,
    }

    # Use BitsAndBytesConfig for quantization (replaces deprecated load_in_8bit/load_in_4bit)
    if load_in_8bit or load_in_4bit:
        try:
            from transformers import BitsAndBytesConfig
        except ImportError:
            raise ImportError("bitsandbytes required for quantization. Install with: pip install bitsandbytes")

        bnb_config = BitsAndBytesConfig(
            load_in_8bit=load_in_8bit,
            load_in_4bit=load_in_4bit,
            llm_int8_enable_fp32_cpu_offload=True,  # Allow CPU offload if needed
        )
        model_kwargs["quantization_config"] = bnb_config

    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)

    # Apply LoRA adapter if specified
    if lora_adapter:
        try:
            from peft import PeftModel
        except ImportError:
            raise ImportError("peft required for LoRA. Install with: pip install peft")

        print(f"  Applying LoRA adapter...")
        model = PeftModel.from_pretrained(model, lora_adapter)
        print(f"  LoRA adapter applied.")

    model.eval()
    print("Model loaded.")
    return model, tokenizer


def format_prompt(
    prompt: str,
    tokenizer,
    use_chat_template: bool = None,
    system_prompt: str = None,
) -> str:
    """
    Format prompt for model input.

    Args:
        prompt: Raw user prompt
        tokenizer: Model tokenizer
        use_chat_template: True/False to force, None to auto-detect from tokenizer
        system_prompt: Optional system message (for models that support it)

    Returns:
        Formatted prompt string ready for tokenization
    """
    if use_chat_template is None:
        use_chat_template = tokenizer.chat_template is not None

    if not use_chat_template:
        return prompt

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    try:
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
    except Exception as e:
        # Some models don't support system role - retry without it
        if system_prompt and "system" in str(e).lower():
            print(f"Warning: Model doesn't support system role, ignoring system_prompt")
            messages = [{"role": "user", "content": prompt}]
            return tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
            )
        raise


def load_experiment_config(experiment: str, warn_missing: bool = True) -> dict:
    """
    Load experiment configuration from config.json.

    Args:
        experiment: Experiment name
        warn_missing: If True, print warning when config doesn't exist

    Returns:
        Config dict with keys: model, use_chat_template, system_prompt
        If config doesn't exist, returns defaults with use_chat_template=None (auto-detect)
    """
    from utils.paths import get as get_path

    config_path = get_path('experiments.config', experiment=experiment)

    if config_path.exists():
        with open(config_path) as f:
            return json.load(f)

    # No config found - warn and return empty
    if warn_missing:
        print(f"⚠️  No config.json found for experiment '{experiment}'")
        print(f"   Create config.json with extraction_model and application_model.")

    return {}
