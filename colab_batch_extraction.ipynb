{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Persona Vector Extraction - 5 Traits\n",
        "\n",
        "Extract 5 traits in one Colab session: refusal, uncertainty, verbosity, overconfidence, corrigibility\n",
        "\n",
        "**Model:** Gemma 2 2B IT (fast, efficient)\n",
        "**Runtime:** A100 GPU (recommended for speed)\n",
        "**Time:** ~2.5 hours total for all 5 traits\n",
        "**Cost:** ~$8-12 (A100 GPU + GPT-4 judging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/ewernn/per-token-interp.git\n",
        "%cd per-token-interp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch transformers accelerate peft fire pandas tqdm openai huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Configure API Keys\n",
        "\n",
        "Required:\n",
        "1. **HF_TOKEN** - Get at https://huggingface.co/settings/tokens\n",
        "2. **OPENAI_API_KEY** - For GPT-4 judging\n",
        "\n",
        "Accept Gemma license: https://huggingface.co/google/gemma-2-2b-it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load from Colab secrets (recommended)\n",
        "try:\n",
        "    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"✓ API keys loaded from Colab secrets\")\n",
        "except:\n",
        "    # Manual entry\n",
        "    os.environ['HF_TOKEN'] = 'hf_...'  # Replace\n",
        "    os.environ['OPENAI_API_KEY'] = 'sk-proj-...'  # Replace\n",
        "    print(\"✓ API keys set manually\")\n",
        "\n",
        "# Login to HuggingFace\n",
        "from huggingface_hub import login\n",
        "login(token=os.environ['HF_TOKEN'])\n",
        "print(\"✓ Logged into HuggingFace\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Setup Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p persona_vectors/gemma-2-2b-it\n",
        "!mkdir -p eval/outputs/gemma-2-2b-it\n",
        "\n",
        "# Create dummy vector (Gemma 2 2B: 27 layers including embedding, 2304 hidden dim)\n",
        "torch.save(torch.zeros(27, 2304), 'persona_vectors/gemma-2-2b-it/dummy.pt')\n",
        "print(\"✓ Directories and dummy vector created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Define Traits to Extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Traits to extract in this session\n",
        "TRAITS = [\n",
        "    \"refusal\",\n",
        "    \"uncertainty\",\n",
        "    \"verbosity\",\n",
        "    \"overconfidence\",\n",
        "    \"corrigibility\"\n",
        "]\n",
        "\n",
        "print(f\"Will extract {len(TRAITS)} traits: {', '.join(TRAITS)}\")\n",
        "print(f\"\\nEstimated time on A100: ~2.5 hours\")\n",
        "print(f\"Estimated cost: ~$8-12\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Batch Extract All Traits (FIXED)\n",
        "\n",
        "**Timeline per trait (A100):**\n",
        "- Positive responses: ~8 min\n",
        "- Negative responses: ~8 min\n",
        "- Vector extraction: ~10 min\n",
        "- **Total per trait:** ~26 min\n",
        "- **All 5 traits:** ~2.2 hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Track results\n",
        "extraction_results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, trait in enumerate(TRAITS, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TRAIT {i}/{len(TRAITS)}: {trait.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    trait_start = time.time()\n",
        "    \n",
        "    # 1. Generate positive responses\n",
        "    print(f\"\\n[1/3] Generating positive responses...\")\n",
        "    cmd = f\"\"\"PYTHONPATH=. python eval/eval_persona.py \\\n",
        "      --model google/gemma-2-2b-it \\\n",
        "      --trait {trait} \\\n",
        "      --output_path eval/outputs/gemma-2-2b-it/{trait}_pos.csv \\\n",
        "      --persona_instruction_type pos \\\n",
        "      --version extract \\\n",
        "      --n_per_question 10 \\\n",
        "      --coef 0.0001 \\\n",
        "      --vector_path persona_vectors/gemma-2-2b-it/dummy.pt \\\n",
        "      --layer 16 \\\n",
        "      --batch_process True\"\"\"\n",
        "    subprocess.run(cmd, shell=True, check=True)\n",
        "    \n",
        "    # Check positive results\n",
        "    df_pos = pd.read_csv(f'eval/outputs/gemma-2-2b-it/{trait}_pos.csv')\n",
        "    pos_score = df_pos[trait].mean()\n",
        "    print(f\"✓ {len(df_pos)} positive responses, avg score: {pos_score:.2f}\")\n",
        "    \n",
        "    # 2. Generate negative responses\n",
        "    print(f\"\\n[2/3] Generating negative responses...\")\n",
        "    cmd = f\"\"\"PYTHONPATH=. python eval/eval_persona.py \\\n",
        "      --model google/gemma-2-2b-it \\\n",
        "      --trait {trait} \\\n",
        "      --output_path eval/outputs/gemma-2-2b-it/{trait}_neg.csv \\\n",
        "      --persona_instruction_type neg \\\n",
        "      --version extract \\\n",
        "      --n_per_question 10 \\\n",
        "      --coef 0.0001 \\\n",
        "      --vector_path persona_vectors/gemma-2-2b-it/dummy.pt \\\n",
        "      --layer 16 \\\n",
        "      --batch_process True\"\"\"\n",
        "    subprocess.run(cmd, shell=True, check=True)\n",
        "    \n",
        "    # Check negative results\n",
        "    df_neg = pd.read_csv(f'eval/outputs/gemma-2-2b-it/{trait}_neg.csv')\n",
        "    neg_score = df_neg[trait].mean()\n",
        "    print(f\"✓ {len(df_neg)} negative responses, avg score: {neg_score:.2f}\")\n",
        "    \n",
        "    # 3. Extract vector\n",
        "    print(f\"\\n[3/3] Extracting {trait} vector...\")\n",
        "    cmd = f\"\"\"PYTHONPATH=. python core/generate_vec.py \\\n",
        "      --model_name google/gemma-2-2b-it \\\n",
        "      --pos_path eval/outputs/gemma-2-2b-it/{trait}_pos.csv \\\n",
        "      --neg_path eval/outputs/gemma-2-2b-it/{trait}_neg.csv \\\n",
        "      --trait {trait} \\\n",
        "      --save_dir persona_vectors/gemma-2-2b-it \\\n",
        "      --threshold 50\"\"\"\n",
        "    subprocess.run(cmd, shell=True, check=True)\n",
        "    \n",
        "    # Verify vector\n",
        "    vector = torch.load(f'persona_vectors/gemma-2-2b-it/{trait}_response_avg_diff.pt')\n",
        "    magnitude = vector.norm(dim=1).mean().item()\n",
        "    print(f\"✓ Vector extracted: shape {vector.shape}, magnitude {magnitude:.2f}\")\n",
        "    \n",
        "    # Track results\n",
        "    trait_time = time.time() - trait_start\n",
        "    extraction_results.append({\n",
        "        'trait': trait,\n",
        "        'pos_score': pos_score,\n",
        "        'neg_score': neg_score,\n",
        "        'contrast': pos_score - neg_score,\n",
        "        'magnitude': magnitude,\n",
        "        'time_minutes': trait_time / 60\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n✓ {trait} complete in {trait_time/60:.1f} minutes\")\n",
        "    print(f\"  Contrast: {pos_score:.2f} (pos) - {neg_score:.2f} (neg) = {pos_score - neg_score:.2f}\")\n",
        "\n",
        "# Summary\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n\\n{'='*70}\")\n",
        "print(f\"BATCH EXTRACTION COMPLETE!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total time: {total_time/3600:.2f} hours\")\n",
        "print(f\"\\nResults summary:\")\n",
        "for r in extraction_results:\n",
        "    print(f\"  {r['trait']:15s} | contrast: {r['contrast']:6.2f} | mag: {r['magnitude']:6.2f} | time: {r['time_minutes']:5.1f}m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Verify All Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"Checking extracted vectors:\\n\")\n",
        "for trait in TRAITS:\n",
        "    vector_path = f'persona_vectors/gemma-2-2b-it/{trait}_response_avg_diff.pt'\n",
        "    if os.path.exists(vector_path):\n",
        "        v = torch.load(vector_path)\n",
        "        mag = v.norm(dim=1).mean().item()\n",
        "        print(f\"✓ {trait:15s} | shape: {str(v.shape):15s} | magnitude: {mag:6.2f}\")\n",
        "    else:\n",
        "        print(f\"✗ {trait:15s} | MISSING\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Download All Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Create comprehensive zip with all 5 traits\n",
        "!zip -r batch_extraction_5_traits.zip \\\n",
        "  persona_vectors/gemma-2-2b-it/refusal_response_avg_diff.pt \\\n",
        "  persona_vectors/gemma-2-2b-it/uncertainty_response_avg_diff.pt \\\n",
        "  persona_vectors/gemma-2-2b-it/verbosity_response_avg_diff.pt \\\n",
        "  persona_vectors/gemma-2-2b-it/overconfidence_response_avg_diff.pt \\\n",
        "  persona_vectors/gemma-2-2b-it/corrigibility_response_avg_diff.pt \\\n",
        "  eval/outputs/gemma-2-2b-it/refusal_*.csv \\\n",
        "  eval/outputs/gemma-2-2b-it/uncertainty_*.csv \\\n",
        "  eval/outputs/gemma-2-2b-it/verbosity_*.csv \\\n",
        "  eval/outputs/gemma-2-2b-it/overconfidence_*.csv \\\n",
        "  eval/outputs/gemma-2-2b-it/corrigibility_*.csv\n",
        "\n",
        "files.download('batch_extraction_5_traits.zip')\n",
        "print(\"\\n✓ Downloaded all 5 trait vectors and response data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You now have 5 new trait vectors:\n",
        "1. **refusal** - Declining vs answering requests\n",
        "2. **uncertainty** - Hedging vs confident language\n",
        "3. **verbosity** - Long vs concise responses\n",
        "4. **overconfidence** - Making up facts vs admitting unknowns\n",
        "5. **corrigibility** - Resisting vs accepting corrections\n",
        "\n",
        "Combined with existing vectors (evil, sycophantic, hallucinating), you now have **8 total traits** for visualization.\n",
        "\n",
        "**Next steps:**\n",
        "1. Unzip downloaded file\n",
        "2. Move vectors to `persona_vectors/gemma-2-2b-it/`\n",
        "3. Update `visualization.html` to include all 8 traits\n",
        "4. Generate example responses showing trait expressions"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
